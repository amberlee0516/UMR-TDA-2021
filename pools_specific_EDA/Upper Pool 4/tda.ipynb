{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# TDA Analysis\n"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Import libraries"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "source": [
    "import kmapper as km\n",
    "from kmapper.plotlyviz import *\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import sklearn\n",
    "#from sklearn import ensemble\n",
    "# DBSCAN from sklearn for clustering algorithms\n",
    "from sklearn.cluster import DBSCAN\n",
    "# PCA from sklearn for projection/lens creation\n",
    "from sklearn.decomposition import PCA\n",
    "# from sklearn.manifold import MDS\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.preprocessing import RobustScaler\n",
    "\n",
    "# scipy for interpolation\n",
    "# import scipy \n",
    "# from scipy.interpolate import *\n",
    "import hdbscan\n",
    "import pickle as pk\n",
    "import json as js\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.compose import ColumnTransformer\n",
    "\n",
    "# Plotly and Dash\n",
    "import plotly.graph_objs as go\n",
    "from kmapper.plotlyviz import plotlyviz\n",
    "from kmapper.plotlyviz import *\n",
    "from ipywidgets import (HBox, VBox)\n",
    "import dash_html_components as html\n",
    "import dash_core_components as dcc\n",
    "import dash\n",
    "from ipywidgets import interactive, HBox, VBox, widgets, interact\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Upload data"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "source": [
    "water20 = pd.read_csv(\"../../LTRM data/RF interpolation/water_full.csv\")\n",
    "water20 = water20[[\"SHEETBAR\", \"DATE\", \"LATITUDE\",\"LONGITUDE\",\"FLDNUM\", \"STRATUM\",\"LOCATCD\",\n",
    "\"TN\",\"TP\",\"TEMP\", \"DO\", \"TURB\", \"VEL\", \"SS\", \"WDP\", \"CHLcal\",  \"SECCHI\", \"YEAR\", \"SEASON\"]]"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Scale data"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "source": [
    "continuous_variables = [\"WDP\",\"SECCHI\",\"TEMP\", \"DO\", \"TURB\", \"VEL\", \"TP\", \"TN\", \"SS\", \"CHLcal\"]\n",
    "\n",
    "ct = ColumnTransformer([('somename', RobustScaler(), continuous_variables)], remainder = 'passthrough')\n",
    "\n",
    "water_df = ct.fit_transform(water20)\n",
    "\n",
    "water_df = pd.DataFrame(water_df, columns = [\"WDP\",\"SECCHI\",\"TEMP\", \"DO\", \"TURB\", \"VEL\", \"TP\", \"TN\", \"SS\", \"CHLcal\",\n",
    "                                             \"SHEETBAR\", \"DATE\", \"LATTITUDE\",\"LONGITUDE\",\"FLDNUM\", \"STRATUM\",\"LOCATCD\",\n",
    "                                             \"YEAR\", \"SEASON\"])  "
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Make decade columns\n"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "source": [
    "eco_time_periods = ['1993-2000', '1898-2004', '2001-2013', '2010-2016', '2014-2020']\n",
    "\n",
    "for time_period in eco_time_periods:\n",
    "    begin_year = int(time_period[0:4])\n",
    "    end_year = int(time_period[5:9])\n",
    "    \n",
    "    water_df[time_period] = np.where((water_df['YEAR'] >= begin_year) & (water_df['YEAR'] <= end_year), 1, 0)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Make dummy columns"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "source": [
    "to_dummy_vars = ['STRATUM', 'FLDNUM']\n",
    "\n",
    "for var in to_dummy_vars:\n",
    "    temp = pd.get_dummies(water_df[var])\n",
    "    water_df = pd.concat([water_df, temp], axis = 1)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Make coloring columns for continuous variables"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "source": [
    "for variable in continuous_variables:\n",
    "    column_name =  variable + \"_color\" \n",
    "\n",
    "    q25, q75 = np.percentile(water_df[variable], [25, 75])\n",
    "    iqr = q75 - q25\n",
    "\n",
    "    ceiling = water_df[variable].quantile(.75) + (3 * iqr)\n",
    "\n",
    "    water_df[column_name] = np.where(water_df[variable] >= ceiling, ceiling, water_df[variable])"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Run kmapper"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Make the kmapper scomplex"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "source": [
    "continuous_variables = [\"WDP\",\"SECCHI\",\"TEMP\", \"DO\", \"TURB\", \"VEL\", \"TP\", \"TN\", \"SS\", \"CHLcal\"]\n",
    "X = water_df[continuous_variables]\n",
    "\n",
    "var_to_index = {continuous_variables[i] : i for i in range(len(continuous_variables))}\n",
    "projected_var_indices = [var_to_index[var] for var in continuous_variables]\n",
    "\n",
    "cluster_alg = sklearn.cluster.DBSCAN(eps = 1.2, min_samples = 20, metric = 'euclidean')\n",
    "#cluster_alg = hdbscan.HDBSCAN(min_cluster_size = 10, min_samples = 10)\n",
    "pca = PCA(n_components = 2)\n",
    "lens = pca.fit_transform(X)\n",
    "\n",
    "mapper = km.KeplerMapper(verbose = 0)\n",
    "scomplex = mapper.map(lens, X, cover = km.Cover(n_cubes = [125, 125], perc_overlap = [.4, .4]), \n",
    "                                            clusterer = cluster_alg, remove_duplicate_nodes = True)\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Calculate the node densitites for each node in the scomplex"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Functions"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "source": [
    "from sklearn.neighbors import NearestNeighbors\n",
    "import queue"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "source": [
    "def getSubdf(scomplex, shape, df):\n",
    "    \"\"\"\n",
    "    Returns the part of the data frame from the particular shape in the simplicial complex.\n",
    "    params:\n",
    "    scomplex: the entire simplicial complex\n",
    "    shape: the particular shape being inspected (within the simplicial complex)\n",
    "    df: the entire data frame\n",
    "    \n",
    "    Description:\n",
    "    1. Get all the nodes from the particular simplicial complex. \n",
    "    2. Generate the indices we care about from the particular shape. To do this, we read each node and append it's \n",
    "    indices to a list. Then, we convert the list to a set and then back to a list to eliminate duplicates.\n",
    "    3. Return the dataframe with only those indices.\n",
    "    \"\"\"\n",
    "    \n",
    "    nodes = scomplex.get('nodes')\n",
    "    indices = []\n",
    "    npShape = np.array(shape).flatten()\n",
    "    for node in npShape:\n",
    "        indices.append(nodes.get(node))\n",
    "    indices = list(set([item for sublist in indices for item in sublist]))\n",
    "    subdf = df.loc[indices]\n",
    "    return subdf\n",
    "\n",
    "def adjacent(v, scomplex):\n",
    "    \"\"\"\n",
    "    Determines the nodes adjacent to a given vertex\n",
    "    \n",
    "    params:\n",
    "    v: vertex\n",
    "    scomlex: the entire simplicial complex\n",
    "    \n",
    "    Description:\n",
    "    Determines the nodes that are adjacent to a given vertex.\n",
    "    \"\"\"\n",
    "    \n",
    "    simplices = scomplex.get('simplices')\n",
    "    edges = [item for item in simplices if len(item) == 2]\n",
    "    result = []\n",
    "    for edge in edges:\n",
    "        if v in edge:\n",
    "            for item in edge:\n",
    "                if item != v:\n",
    "                    result.append(item)\n",
    "    return result\n",
    "\n",
    "def bfs(node, scomplex):\n",
    "    \"\"\"\n",
    "    Conducts a breadth first search to obtain the entire shape from a given node\n",
    "    params:\n",
    "    node: the start node\n",
    "    scomplex: the entire simplicial complex\n",
    "    \n",
    "    Description:\n",
    "    Preforms a breadth first search to obtain the entire shape for a given start node.\n",
    "    \"\"\"\n",
    "    Q = queue.Queue()\n",
    "    result = []\n",
    "    result.append(node)\n",
    "    Q.put(node)\n",
    "    while not Q.empty():\n",
    "        v = Q.get()\n",
    "        adjacentEdges = adjacent(v, scomplex)\n",
    "        for edge in adjacentEdges:\n",
    "            if edge not in result:\n",
    "                result.append(edge)\n",
    "                Q.put(edge)\n",
    "    return result\n",
    "\n",
    "def getShapes(scomplex):\n",
    "    \"\"\"\n",
    "    Gets all of the shapes from a given simplicial complex.\n",
    "    \n",
    "    params:\n",
    "    scomplex: the entire simplicial complex\n",
    "    \n",
    "    Description:\n",
    "    1. Obtain all the nodes for the entire complex\n",
    "    2. For each node, preform a breadth first search to obtain everything in that particular shape. \n",
    "    If this entire shape has not already been discovered, add it to the set of results. \n",
    "    The result item is a set as the order of the shapes does not matter. The resulting shape is a frozenset\n",
    "    which means items cannot be added or removed once created, and is needed to allow the set object to have other sets within it.\n",
    "    3. Convert each shape to a list and the result to a list for easier navigation outside of the function.\n",
    "    4. Return the result\n",
    "    \n",
    "    \"\"\"\n",
    "    \n",
    "    nodes = list(scomplex.get('nodes').keys())\n",
    "    result = set()\n",
    "    for node in nodes: # currently does more computations than necessary due to going through every node without considering it is already in a shape\n",
    "        bfsResult = frozenset(bfs(node, scomplex))\n",
    "        result.add(bfsResult)\n",
    "    result = [list(x) for x in result]\n",
    "    # Sort the list depending on what is decided: nodes or indices. Currently doing it by number of nodes\n",
    "    result.sort(key = len, reverse = True)\n",
    "    \n",
    "    \n",
    "    \n",
    "    return result\n",
    "\n",
    "def condenseShape(shape, scomplex):\n",
    "    \"\"\"\n",
    "    \n",
    "    params:\n",
    "    shape: a shape of two nodes. must be 2\n",
    "    scomplex: the entire simplicial complex\n",
    "    \n",
    "    description:\n",
    "    gets the two nodes a and b\n",
    "    gets the indices for a and b (what is inside the nodes)\n",
    "    if a \\subseteq b, return b\n",
    "    elif b \\subseteq a, return a \n",
    "    else return shape \n",
    "    \n",
    "    \"\"\"\n",
    "    nodes = scomplex.get('nodes')\n",
    "    a = shape[0]\n",
    "    b = shape[1]\n",
    "    aIndices = set(nodes.get(a))\n",
    "    bIndices = set(nodes.get(b))\n",
    "    \n",
    "    if aIndices.issubset(bIndices):\n",
    "        return b\n",
    "    elif bIndices.issubset(aIndices):\n",
    "        return a\n",
    "    else:\n",
    "        return shape\n",
    "\n",
    "def clean_getShapes(scomplex):\n",
    "    \"\"\"\n",
    "    Condenses 1-simplices down to 0-simplices when each node \n",
    "    is a subset of the other \n",
    "    \n",
    "    params:\n",
    "    scomplex: the entire simplicial complex\n",
    "    \n",
    "    Description:\n",
    "    1. Get all the shapes from the original getShapes function\n",
    "    2. For shapes that of length 2, if one is a subset of the other, return the larger of the two\n",
    "        Otherwise, do nothing\n",
    "    3. return the clean Shapes list \n",
    "    \n",
    "    \"\"\"\n",
    "    shapes = getShapes(scomplex)\n",
    "    cleanShapes = []\n",
    "    for shape in shapes:\n",
    "        if len(shape) == 2:\n",
    "            shape = condenseShape(shape, scomplex)\n",
    "            cleanShapes.append([shape])\n",
    "        else:\n",
    "            cleanShapes.append(shape)\n",
    "    return cleanShapes\n",
    "\n",
    "def k_nearest_neighbors(df, neigh, point, k):\n",
    "    return neigh.kneighbors([list(df.loc[point])], k)[0].flatten()\n",
    "    \n",
    "def calculate_density(scomplex, node, df, k):\n",
    "    neigh = NearestNeighbors(n_neighbors=k)\n",
    "    neigh.fit(df)\n",
    "    knn = 0\n",
    "    n = len(scomplex['nodes'][node])\n",
    "    for point in scomplex['nodes'][node]:\n",
    "        distances = k_nearest_neighbors(df, neigh, point, k)\n",
    "        knn += (sum(distances) / k)\n",
    "    density = knn / (n*n)\n",
    "    return (1.0 / density)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "source": [
    "#scomplex = scomplex_dict.get(key)\n",
    "scomplex['density'] = {}\n",
    "largestShape = clean_getShapes(scomplex)[0]\n",
    "#print(\"Largest shape is: \", largestShape, \"\\n\")\n",
    "largestShape_df = getSubdf(scomplex, largestShape, X)\n",
    "k = int((X.shape[0] / 10) + 1)\n",
    "for node_name in largestShape:\n",
    "    #scomplex['density'][node_name] = calculate_density(scomplex, node_name, largestShape_df, k)\n",
    "    scomplex['density'][node_name] = calculate_density(scomplex, node_name, X, k)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "allComplices = list(scomplex_dict.keys())\n",
    "df = pd.read_csv(\"../../LTRM data/RF interpolation/water_full.csv\")\n",
    "df = df[[\"WDP\", \"SECCHI\", \"TEMP\", \"DO\", \"TURB\", \"VEL\", \"TP\", \"TN\", \"SS\", \"CHLcal\"]]\n",
    "\n",
    "for key in allComplices: # remove the indices here to get all the strata for all the time periods\n",
    "    print(\"Current Simplical Complex: \", key)\n",
    "    scomplex = scomplex_dict.get(key)\n",
    "    scomplex['density'] = {}\n",
    "    largestShape = clean_getShapes(scomplex)[0]\n",
    "    #print(\"Largest shape is: \", largestShape, \"\\n\")\n",
    "    largestShape_df = getSubdf(scomplex, largestShape, df_dict.get(key))\n",
    "    k = int((df_dict.get(key).shape[0] / 10) + 1)\n",
    "    for node_name in largestShape:\n",
    "        #scomplex['density'][node_name] = calculate_density(scomplex, node_name, largestShape_df, k)\n",
    "        scomplex['density'][node_name] = calculate_density(scomplex, node_name, df_dict.get(key), k)"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Make digraph"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Functions"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "import networkx as nx"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "def generate_graph(scomplex, shape):\n",
    "    dg = generate_nodes(scomplex, shape)\n",
    "    dg = generate_edges(scomplex, shape, dg)\n",
    "    return dg\n",
    "\n",
    "def generate_nodes(scomplex, shape):\n",
    "    dg = nx.DiGraph()\n",
    "    dg.add_nodes_from(shape)\n",
    "    return dg\n",
    "\n",
    "def generate_edges(scomplex, shape, dg):\n",
    "    for node in shape:\n",
    "        if node in scomplex['links']:\n",
    "            for adjacent_node in scomplex['links'][node]:\n",
    "                if scomplex['density'][node] < scomplex['density'][adjacent_node]:\n",
    "                    dg.add_edge(node, adjacent_node)\n",
    "                else:\n",
    "                    dg.add_edge(adjacent_node, node)\n",
    "    return dg\n",
    "\n",
    "def get_local_maxima(dg):\n",
    "    maxima = []\n",
    "    for node in list(dg.nodes):\n",
    "        succ = dict(nx.bfs_successors(dg, source=node))\n",
    "        if not succ[node]:\n",
    "            maxima.append(node)\n",
    "    return maxima\n",
    "\n",
    "def draw_graph(scomplex, filepath, with_labels = False):\n",
    "    colors = ['#00A08A', '#e80909', '#F2AD00', '#a6d96a', '#d9ef8b', '#d0e1f0', '#74a7d2',\n",
    "             '#295981', '#17334a', '#a2a0f7', '#e68a0d', '#a68a52', '#50704a', '#458B74', '#817f85']\n",
    "    colors_dict = {scomplex['maxima'][i] : colors[i] for i in range(len(scomplex['maxima']))}\n",
    "    transition_color = '#ffffbf'\n",
    "    color_map = []\n",
    "    scomplex['states'] = {scomplex['maxima'][i] : [] for i in range(len(scomplex['maxima']))}\n",
    "    \n",
    "    fig = plt.figure(figsize = (12,12))\n",
    "    ax = plt.subplot(111)\n",
    "\n",
    "    title = filepath[:-4]\n",
    "    ax.set_title(title, fontsize = 60)\n",
    "    \n",
    "    for node in scomplex['graph']:\n",
    "        distDict = {scomplex['maxima'][i] : graph_distance(scomplex['graph'], node, scomplex['maxima'][i])\n",
    "                    for i in range(len(scomplex['maxima']))}\n",
    "        minDist = min(distDict.values())\n",
    "        states = [maxima if distDict[maxima] == minDist else None for maxima in scomplex['maxima']]\n",
    "        states = list(filter(None, states))\n",
    "        \n",
    "        for state in states:\n",
    "            scomplex['states'][state].append(node)\n",
    "        \n",
    "        if len(states) > 1:\n",
    "            color_map.append(transition_color)\n",
    "        else:\n",
    "            color_map.append(colors_dict[states[0]])\n",
    "\n",
    "    \n",
    "    nx.draw_kamada_kawai(scomplex['graph'], with_labels = with_labels, node_color = color_map)\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(filepath, format = \"PNG\")\n",
    "    plt.show()\n",
    "\n",
    "def graph_distance(dg, source, target):\n",
    "    if nx.has_path(dg, source, target):\n",
    "        return len(nx.shortest_path(dg, source, target))\n",
    "    return float('inf')"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "#for key in allComplices: # remove the indices here to get all the strata for all the time periods\n",
    "#scomplex = scomplex_dict.get(key)\n",
    "#print(key)\n",
    "largestShape = clean_getShapes(scomplex)[0]\n",
    "scomplex['graph'] = {}\n",
    "scomplex['graph'] = generate_graph(scomplex, largestShape)\n",
    "scomplex['maxima'] = get_local_maxima(scomplex['graph'])\n",
    "#print(key + \": \" + str(len(scomplex['maxima'])) )"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Print out digraph with states coloring"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "filepath = \"all_data_digraph.png\"\n",
    "draw_graph(scomplex, filepath, with_labels = False)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Get coloring names lists"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "continuous_variables = [\"WDP\",\"SECCHI\",\"TEMP\", \"DO\", \"TURB\", \"VEL\", \"TP\", \"TN\", \"SS\", \"CHLcal\"]\n",
    "\n",
    "continuous_variable_colors =  [\"WDP_color\", \"SECCHI_color\", \"TEMP_color\", \"DO_color\", \n",
    "                                \"TURB_color\", \"VEL_color\", \"TP_color\", \"TN_color\", \n",
    "                                \"SS_color\", \"CHLcal_color\"]\n",
    "\n",
    "pool_names = ['Unexploded Ordinance Area - Pool 13',\n",
    "                                        'Bellevue, IA',\n",
    "                                        'Brighton, IL',\n",
    "                                          'Havana, IL',\n",
    "                                         'Jackson, MO',\n",
    "                                       'Lake City, MN',\n",
    "                                        'Onalaska, WI']\n",
    "\n",
    "decade_names = ['1993-2000', '1898-2004', '2001-2013', '2010-2016', '2014-2020']\n",
    "\n",
    "stratum_names = ['Backwater area contiguous to the main channel',\n",
    "                                           'Impounded',\n",
    "                                            'Isolated',\n",
    "                             'Lake Pepin or Swan Lake',\n",
    "                                        'Main channel',\n",
    "                                        'Side channel']"
   ],
   "outputs": [],
   "metadata": {}
  }
 ],
 "metadata": {
  "orig_nbformat": 4,
  "language_info": {
   "name": "python",
   "version": "3.9.5",
   "mimetype": "text/x-python",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "pygments_lexer": "ipython3",
   "nbconvert_exporter": "python",
   "file_extension": ".py"
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.9.5 64-bit ('3.9')"
  },
  "interpreter": {
   "hash": "7812ea015bdcee6f23a998adcdd2ef97c151c0c241b7b7070987d9313e41299d"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}