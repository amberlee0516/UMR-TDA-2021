{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# Epsilon Selection\n",
    "\n",
    "In this script, we will be testing several different epsilons as a kmapper parameter to determine how many states persist when as this value changes. We will first run the kmapper algorithm on the whole interpolated and cleaned LTRM water quality data set. We will then use code written by Killian to determine the density of each node, find local maximum, and determine local maxima states. We will count how many states are in each scomplex with the different choices of epsilon. Since the states, as of now, do not hold qualitative meaning, we will only be comparing state persistence as a count of how many there are and generally where they exist in the simplicial complex. Also note that the state analysis is run only on the largest shape in the simplicial complex."
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Import libraries"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "source": [
    "import kmapper as km\n",
    "from kmapper.plotlyviz import *\n",
    "import pandas as pd\n",
    "\n",
    "import sklearn\n",
    "#from sklearn import ensemble\n",
    "# DBSCAN from sklearn for clustering algorithms\n",
    "from sklearn.cluster import DBSCAN\n",
    "# PCA from sklearn for projection/lens creation\n",
    "from sklearn.decomposition import PCA\n",
    "# from sklearn.manifold import MDS\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.preprocessing import RobustScaler\n",
    "\n",
    "# scipy for interpolation\n",
    "# import scipy \n",
    "# from scipy.interpolate import *\n",
    "import hdbscan\n",
    "import pickle as pk\n",
    "import json as js\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.compose import ColumnTransformer\n",
    "\n",
    "# Plotly and Dash\n",
    "import plotly.graph_objs as go\n",
    "from kmapper.plotlyviz import plotlyviz\n",
    "from kmapper.plotlyviz import *\n",
    "from ipywidgets import (HBox, VBox)\n",
    "import dash_html_components as html\n",
    "import dash_core_components as dcc\n",
    "import dash\n",
    "from ipywidgets import interactive, HBox, VBox, widgets, interact\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Read data"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "source": [
    "water20 = pd.read_csv(\"../../LTRM data/RF interpolation/water_full.csv\")\n",
    "#water20 = water20[[\"SHEETBAR\", \"DATE\", \"LATITUDE\",\"LONGITUDE\",\"FLDNUM\", \"STRATUM\",\"LOCATCD\",\n",
    "#\"TN\",\"TP\",\"TEMP\", \"DO\", \"TURB\", \"VEL\", \"SS\", \"WDP\", \"CHLcal\",  \"SECCHI\", \"YEAR\", \"SEASON\"]]"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Robust Scaling"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "source": [
    "water_df = water20[[\"WDP\", \"SECCHI\", \"TEMP\", \"DO\", \"TURB\",\n",
    "             \"VEL\", \"TP\", \"TN\", \"SS\", \"CHLcal\", \n",
    "             \"YEAR\", \"SEASON\", \"FLDNUM\", \"STRATUM\"]]\n",
    "\n",
    "ct = ColumnTransformer([\n",
    "        ('somename', RobustScaler(), [\"WDP\", \"SECCHI\", \"TEMP\", \"DO\", \"TURB\",\n",
    "                                        \"VEL\", \"TP\", \"TN\", \"SS\", \"CHLcal\"])\n",
    "    ], remainder='passthrough')\n",
    "\n",
    "water_df = pd.DataFrame(ct.fit_transform(water_df), columns = [\"WDP\", \"SECCHI\", \"TEMP\", \"DO\", \"TURB\",\n",
    "                                                 \"VEL\", \"TP\", \"TN\", \"SS\", \"CHLcal\", \n",
    "                                                 \"YEAR\", \"SEASON\", \"FLDNUM\", \"STRATUM\"])\n",
    "\n",
    "water_df = pd.DataFrame(water_df, columns = [\"WDP\", \"SECCHI\", \"TEMP\", \"DO\", \"TURB\",\n",
    "                             \"VEL\", \"TP\", \"TN\", \"SS\", \"CHLcal\"])"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Kmapper function"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "source": [
    "def mapper_func(df, DBSCAN_EPSILON = 20, DBSCAN_MIN_SAMPLES = 20, N_CUBES = [10, 10], PERC_OVERLAP = [.45, .45]):\n",
    "    \n",
    "    X = df[[\"WDP\", \"SECCHI\", \"TEMP\", \"DO\", \"TURB\", \"VEL\", \"TP\", \"TN\", \"SS\", \"CHLcal\"]]\n",
    "    continuous_variables =  [\"WDP\", \"SECCHI\", \"TEMP\", \"DO\", \"TURB\", \"VEL\", \"TP\", \"TN\", \"SS\", \"CHLcal\"]\n",
    "\n",
    "    var_to_index = {continuous_variables[i] : i for i in range(len(continuous_variables))}\n",
    "    #projected_vars = continuous_variables\n",
    "    projected_var_indices = [var_to_index[var] for var in continuous_variables]\n",
    "\n",
    "    # defining clustering and kmapper parameters\n",
    "    # create instance of clustering alg\n",
    "    cluster_alg = sklearn.cluster.DBSCAN(eps = DBSCAN_EPSILON, min_samples = DBSCAN_MIN_SAMPLES, metric = 'euclidean')\n",
    "\n",
    "    # Instantiate kepler mapper object\n",
    "    mapper = km.KeplerMapper(verbose = 0)\n",
    "    \n",
    "    # defining filter function as projection on to the first 2 component axis\n",
    "    pca = PCA(n_components = 2)\n",
    "    lens = pca.fit_transform(X)\n",
    "\n",
    "    #pca.fit_transform(X)\n",
    "    principle_component = max(abs(pca.components_[0].min()), abs(pca.components_[0].max()))\n",
    "    max_index = 0\n",
    "\n",
    "    for i in range(len(pca.components_[0])):\n",
    "        if abs(pca.components_[0][i]) == principle_component:\n",
    "            max_index = i\n",
    "\n",
    "    #print(\"Primary variable: \", continuous_variables[max_index])\n",
    "    #print(\"Corresponding component: \", pca.components_[0][max_index])\n",
    "    #print('Explained Variance: ', pca.explained_variance_ratio_)\n",
    "    \n",
    "    #lens = np.array(X[continuous_variables[max_index]])\n",
    "    #lens = np.array(X[['PredictedTN', 'PredictedSS']])\n",
    "    \n",
    "    # Generate the simplicial complex\n",
    "    scomplex = mapper.map(lens, X, cover = km.Cover(n_cubes = N_CUBES, perc_overlap = PERC_OVERLAP), \n",
    "                                                    clusterer = cluster_alg, remove_duplicate_nodes = True)  \n",
    "\n",
    "    return scomplex, X"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Parameters\n",
    "\n",
    "Some of the parameters for the clustering algorithm and the kmapper have already been selected. For kmapper, we will be using a cube size of 125 and a 40% overlap. The cube size is much larger than previously because robust scaling has made the point cluster much more dense. In order to gain high resolution of the data, we needed to divide each bin up by a larger number of cubes. A 40% overlap was selected because it provides a more continuous view of the data. Though we it contributes to more redundancy of nodes, we are okay with this because it adds resolution to the data. As for the DBSCAN parameters, we know that we want minsamples to be 10 because it ensures that each cluster has enough points while ignoring some clusteres that do not have enough data to be considered significant. All that is left is to determine the epsilon parameter. These parameters were also selected so that that largest node represents no more than 10% of the original data set and so that only about 5% of the data is lost due to cluster noise.\n",
    "\n",
    "The epsilon parameter is important because it indirectly determines how much noise we will allow in our simplicial complex as well as determining how many nodes will be in the resulting simplicial complex. A smaller epsilon makes it less likely that a cluster will achieve the minimum number of samples, thus resulting in a larger amount of data lost as \"noise\". A larger epsilon will group more data together than may be necessary and will as a result, lose a bit of resolution and detail of the data set. \n",
    "\n",
    "We will be using the number of persistent states as a measure of how good our epsilon because it will indirectly decide how much noise we are okay with losing with our parameter selection. The LTRM data is very noisy meaning that there is a lot of variable in many of the different variables. Some of this noise may be exactly that while other noise may indicate a rare but important state. With that said, we may no want to lose some of the data's noise as clustering noise. Looking at state persistence will help us to choose this parameter without directly choosing what we will be calling noise."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "source": [
    "scomplex_dict = {}\n",
    "df_dict = {}\n",
    "\n",
    "epsilon_list = [.5, .6, .7, .8, .9, 1, 1.1]\n",
    "\n",
    "for i in epsilon_list:\n",
    "    title = 'Epsilon_' + str(i)\n",
    "    scomplex, X = mapper_func(water_df, i, 10, [125, 125], [.4, .4])\n",
    "    scomplex_dict[title] = scomplex\n",
    "    df_dict[title] = X"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Get the number of maxima\n"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "source": [
    "variables = [\"WDP\", \"SECCHI\", \"TEMP\", \"DO\", \"TURB\", \"VEL\", \"TP\",\"TN\", \"SS\", \"CHLcal\"]"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Calculating Density for Nodes in the Largest Structure - Killian"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "source": [
    "from sklearn.neighbors import NearestNeighbors\n",
    "import queue"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Functions"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "source": [
    "def getSubdf(scomplex, shape, df):\n",
    "    \"\"\"\n",
    "    Returns the part of the data frame from the particular shape in the simplicial complex.\n",
    "    params:\n",
    "    scomplex: the entire simplicial complex\n",
    "    shape: the particular shape being inspected (within the simplicial complex)\n",
    "    df: the entire data frame\n",
    "    \n",
    "    Description:\n",
    "    1. Get all the nodes from the particular simplicial complex. \n",
    "    2. Generate the indices we care about from the particular shape. To do this, we read each node and append it's \n",
    "    indices to a list. Then, we convert the list to a set and then back to a list to eliminate duplicates.\n",
    "    3. Return the dataframe with only those indices.\n",
    "    \"\"\"\n",
    "    nodes = scomplex.get('nodes')\n",
    "    indices = []\n",
    "    npShape = np.array(shape).flatten()\n",
    "    for node in npShape:\n",
    "        indices.append(nodes.get(node))\n",
    "    indices = list(set([item for sublist in indices for item in sublist]))\n",
    "    subdf = df.loc[indices]\n",
    "    return subdf\n",
    "\n",
    "def adjacent(v, scomplex):\n",
    "    \"\"\"\n",
    "    Determines the nodes adjacent to a given vertex\n",
    "    \n",
    "    params:\n",
    "    v: vertex\n",
    "    scomlex: the entire simplicial complex\n",
    "    \n",
    "    Description:\n",
    "    Determines the nodes that are adjacent to a given vertex.\n",
    "    \"\"\"\n",
    "    \n",
    "    simplices = scomplex.get('simplices')\n",
    "    edges = [item for item in simplices if len(item) == 2]\n",
    "    result = []\n",
    "    for edge in edges:\n",
    "        if v in edge:\n",
    "            for item in edge:\n",
    "                if item != v:\n",
    "                    result.append(item)\n",
    "    return result\n",
    "\n",
    "def bfs(node, scomplex):\n",
    "    \"\"\"\n",
    "    Conducts a breadth first search to obtain the entire shape from a given node\n",
    "    params:\n",
    "    node: the start node\n",
    "    scomplex: the entire simplicial complex\n",
    "    \n",
    "    Description:\n",
    "    Preforms a breadth first search to obtain the entire shape for a given start node.\n",
    "    \"\"\"\n",
    "    Q = queue.Queue()\n",
    "    result = []\n",
    "    result.append(node)\n",
    "    Q.put(node)\n",
    "    while not Q.empty():\n",
    "        v = Q.get()\n",
    "        adjacentEdges = adjacent(v, scomplex)\n",
    "        for edge in adjacentEdges:\n",
    "            if edge not in result:\n",
    "                result.append(edge)\n",
    "                Q.put(edge)\n",
    "    return result\n",
    "\n",
    "def getShapes(scomplex):\n",
    "    \"\"\"\n",
    "    Gets all of the shapes from a given simplicial complex.\n",
    "    \n",
    "    params:\n",
    "    scomplex: the entire simplicial complex\n",
    "    \n",
    "    Description:\n",
    "    1. Obtain all the nodes for the entire complex\n",
    "    2. For each node, preform a breadth first search to obtain everything in that particular shape. \n",
    "    If this entire shape has not already been discovered, add it to the set of results. \n",
    "    The result item is a set as the order of the shapes does not matter. The resulting shape is a frozenset\n",
    "    which means items cannot be added or removed once created, and is needed to allow the set object to have other sets within it.\n",
    "    3. Convert each shape to a list and the result to a list for easier navigation outside of the function.\n",
    "    4. Return the result\n",
    "    \n",
    "    \"\"\"\n",
    "    \n",
    "    nodes = list(scomplex.get('nodes').keys())\n",
    "    result = set()\n",
    "    for node in nodes: # currently does more computations than necessary due to going through every node without considering it is already in a shape\n",
    "        bfsResult = frozenset(bfs(node, scomplex))\n",
    "        result.add(bfsResult)\n",
    "    result = [list(x) for x in result]\n",
    "    # Sort the list depending on what is decided: nodes or indices. Currently doing it by number of nodes\n",
    "    result.sort(key = len, reverse = True)\n",
    "    \n",
    "    \n",
    "    \n",
    "    return result\n",
    "\n",
    "def condenseShape(shape, scomplex):\n",
    "    \"\"\"\n",
    "    \n",
    "    params:\n",
    "    shape: a shape of two nodes. must be 2\n",
    "    scomplex: the entire simplicial complex\n",
    "    \n",
    "    description:\n",
    "    gets the two nodes a and b\n",
    "    gets the indices for a and b (what is inside the nodes)\n",
    "    if a \\subseteq b, return b\n",
    "    elif b \\subseteq a, return a \n",
    "    else return shape \n",
    "    \n",
    "    \"\"\"\n",
    "    nodes = scomplex.get('nodes')\n",
    "    a = shape[0]\n",
    "    b = shape[1]\n",
    "    aIndices = set(nodes.get(a))\n",
    "    bIndices = set(nodes.get(b))\n",
    "    \n",
    "    if aIndices.issubset(bIndices):\n",
    "        return b\n",
    "    elif bIndices.issubset(aIndices):\n",
    "        return a\n",
    "    else:\n",
    "        return shape\n",
    "\n",
    "def clean_getShapes(scomplex):\n",
    "    \"\"\"\n",
    "    Condenses 1-simplices down to 0-simplices when each node \n",
    "    is a subset of the other \n",
    "    \n",
    "    params:\n",
    "    scomplex: the entire simplicial complex\n",
    "    \n",
    "    Description:\n",
    "    1. Get all the shapes from the original getShapes function\n",
    "    2. For shapes that of length 2, if one is a subset of the other, return the larger of the two\n",
    "        Otherwise, do nothing\n",
    "    3. return the clean Shapes list \n",
    "    \n",
    "    \"\"\"\n",
    "    shapes = getShapes(scomplex)\n",
    "    cleanShapes = []\n",
    "    for shape in shapes:\n",
    "        if len(shape) == 2:\n",
    "            shape = condenseShape(shape, scomplex)\n",
    "            cleanShapes.append([shape])\n",
    "        else:\n",
    "            cleanShapes.append(shape)\n",
    "    return cleanShapes\n",
    "\n",
    "def k_nearest_neighbors(df, neigh, point, k):\n",
    "    return neigh.kneighbors([list(df.loc[point])], k)[0].flatten()\n",
    "    \n",
    "def calculate_density(scomplex, node, df, k):\n",
    "    neigh = NearestNeighbors(n_neighbors=k)\n",
    "    neigh.fit(df)\n",
    "    knn = 0\n",
    "    n = len(scomplex['nodes'][node])\n",
    "    for point in scomplex['nodes'][node]:\n",
    "        distances = k_nearest_neighbors(df, neigh, point, k)\n",
    "        knn += (sum(distances) / k)\n",
    "    density = knn / (n*n)\n",
    "    return (1.0 / density)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Get the density for each node in each scomplex"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "source": [
    "allComplices = list(scomplex_dict.keys())\n",
    "df = pd.read_csv(\"../../LTRM data/RF interpolation/water_full.csv\")\n",
    "df = df[[\"WDP\", \"SECCHI\", \"TEMP\", \"DO\", \"TURB\", \"VEL\", \"TP\", \"TN\", \"SS\", \"CHLcal\"]]\n",
    "\n",
    "for key in allComplices: # remove the indices here to get all the strata for all the time periods\n",
    "    print(\"Current Simplical Complex: \", key)\n",
    "    scomplex = scomplex_dict.get(key)\n",
    "    scomplex['density'] = {}\n",
    "    largestShape = clean_getShapes(scomplex)[0]\n",
    "    #print(\"Largest shape is: \", largestShape, \"\\n\")\n",
    "    largestShape_df = getSubdf(scomplex, largestShape, df_dict.get(key))\n",
    "    k = int((df_dict.get(key).shape[0] / 10) + 1)\n",
    "    for node_name in largestShape:\n",
    "        #scomplex['density'][node_name] = calculate_density(scomplex, node_name, largestShape_df, k)\n",
    "        scomplex['density'][node_name] = calculate_density(scomplex, node_name, df_dict.get(key), k)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Current Simplical Complex:  Epsilon_0.5\n",
      "Current Simplical Complex:  Epsilon_0.6\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Get the maxima for each scomplex and create a directed graph\n",
    "\n",
    "\n",
    "\n"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Functions"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "import networkx as nx"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "def generate_graph(scomplex, shape):\n",
    "    dg = generate_nodes(scomplex, shape)\n",
    "    dg = generate_edges(scomplex, shape, dg)\n",
    "    return dg\n",
    "\n",
    "def generate_nodes(scomplex, shape):\n",
    "    dg = nx.DiGraph()\n",
    "    dg.add_nodes_from(shape)\n",
    "    return dg\n",
    "\n",
    "def generate_edges(scomplex, shape, dg):\n",
    "    for node in shape:\n",
    "        if node in scomplex['links']:\n",
    "            for adjacent_node in scomplex['links'][node]:\n",
    "                if scomplex['density'][node] < scomplex['density'][adjacent_node]:\n",
    "                    dg.add_edge(node, adjacent_node)\n",
    "                else:\n",
    "                    dg.add_edge(adjacent_node, node)\n",
    "    return dg\n",
    "\n",
    "def get_local_maxima(dg):\n",
    "    maxima = []\n",
    "    for node in list(dg.nodes):\n",
    "        succ = dict(nx.bfs_successors(dg, source=node))\n",
    "        if not succ[node]:\n",
    "            maxima.append(node)\n",
    "    return maxima\n",
    "\n",
    "def draw_graph(scomplex, filepath, with_labels = False):\n",
    "    colors = ['#00A08A', '#e80909', '#F2AD00', '#a6d96a', '#d9ef8b']\n",
    "    colors_dict = {scomplex['maxima'][i] : colors[i] for i in range(len(scomplex['maxima']))}\n",
    "    transition_color = '#ffffbf'\n",
    "    color_map = []\n",
    "    scomplex['states'] = {scomplex['maxima'][i] : [] for i in range(len(scomplex['maxima']))}\n",
    "    \n",
    "    fig = plt.figure(figsize = (12,12))\n",
    "    ax = plt.subplot(111)\n",
    "\n",
    "    title = filepath[:-4]\n",
    "    ax.set_title(title, fontsize = 60)\n",
    "    \n",
    "    for node in scomplex['graph']:\n",
    "        distDict = {scomplex['maxima'][i] : graph_distance(scomplex['graph'], node, scomplex['maxima'][i])\n",
    "                    for i in range(len(scomplex['maxima']))}\n",
    "        minDist = min(distDict.values())\n",
    "        states = [maxima if distDict[maxima] == minDist else None for maxima in scomplex['maxima']]\n",
    "        states = list(filter(None, states))\n",
    "        \n",
    "        for state in states:\n",
    "            scomplex['states'][state].append(node)\n",
    "        \n",
    "        if len(states) > 1:\n",
    "            color_map.append(transition_color)\n",
    "        else:\n",
    "            color_map.append(colors_dict[states[0]])\n",
    "\n",
    "    \n",
    "    nx.draw_kamada_kawai(scomplex['graph'], with_labels = with_labels, node_color = color_map)\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(filepath, format = \"PNG\")\n",
    "    plt.show()\n",
    "\n",
    "def graph_distance(dg, source, target):\n",
    "    if nx.has_path(dg, source, target):\n",
    "        return len(nx.shortest_path(dg, source, target))\n",
    "    return float('inf')"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "for key in allComplices: # remove the indices here to get all the strata for all the time periods\n",
    "    scomplex = scomplex_dict.get(key)\n",
    "    print(key)\n",
    "    largestShape = clean_getShapes(scomplex)[0]\n",
    "    scomplex['graph'] = {}\n",
    "    scomplex['graph'] = generate_graph(scomplex, largestShape)\n",
    "    scomplex['maxima'] = get_local_maxima(scomplex['graph'])\n",
    "    print(key + \": \" + str(len(scomplex['maxima'])) )"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "#filepath =  \"Epsilon_0.5%.png\"\n",
    "\n",
    "for key in allComplices:\n",
    "    filepath = str(key) + \".png\"\n",
    "    draw_graph(scomplex[key], filepath, with_labels = False)"
   ],
   "outputs": [],
   "metadata": {}
  }
 ],
 "metadata": {
  "orig_nbformat": 4,
  "language_info": {
   "name": "python",
   "version": "3.9.5",
   "mimetype": "text/x-python",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "pygments_lexer": "ipython3",
   "nbconvert_exporter": "python",
   "file_extension": ".py"
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.9.5 64-bit"
  },
  "interpreter": {
   "hash": "aee8b7b246df8f9039afb4144a1f6fd8d2ca17a180786b69acc140d282b71a49"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}