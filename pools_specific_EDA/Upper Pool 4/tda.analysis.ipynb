{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# Simplicial Complex Analysis\n",
    "\n",
    "<span style='color:Red'>Stretch goal: Get a system to determine ecological state of a shape</span> <br>\n",
    "<span style='color:Red'> Currently using the largest number of nodes in each shape for ordering. <br> TODO: allow for user to change ordering style in the results of the get shapes function.</span> <br> \n",
    "Written by Frederick Miller, Casey McKean, and Wako Bungula. <br> \n",
    "The kepler mapper object gives an output that is not easily navigatible. To resolve this, we wish to create shapes that are easier to navigate and understand, and reveal the data inside of them. <br>\n",
    "We generate all the shapes in the simplicial complex, condense 1-simplices where possible, and obtain summary statistics on the shapes and the nodes within the shapes.\n",
    "\n",
    "Here, what is mostly contained is demonstrations of what the functions do. For specific analysis, wrappers that are very similar to these applications will likely be used."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "source": [
    "import numpy as np\n",
    "import pandas as pd \n",
    "import queue\n",
    "import animation\n",
    "import json\n",
    "import networkx as nx\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.neighbors import NearestNeighbors\n",
    "pd.set_option('display.max_rows', None)\n",
    "import pickle\n",
    "print(\"Imports Done\")"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Imports Done\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# File paths, `.p`, and `.json` import\n",
    "\n",
    "From the `kmapper_demo` file, I added one extra code block to place the resulting simplicial complices in a `.json` file, which is a way to store dictionaries in long term storage. Additionally, it stores the dictionary of dataframes in a `.p` file, which is similar. The code below only needs to have the file paths changed, and then it will read the simplicial complices generated from kepler mapper. <br>\n",
    "Here, we also import the actual data set, with data interpolated for the specific pool. <br>\n",
    "Lastly, a list of the 11 continuous variables (the interpolated versions) is created."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "source": [
    "jsonFilePath = \"DB_graphs.json\"\n",
    "jsonFile = open(jsonFilePath, \"r\")\n",
    "jsonData = json.load(jsonFile) \n",
    "jsonFile.close()\n",
    "\n",
    "dataFilePath = \"DB_dfs.p\"\n",
    "df_dict = pickle.load(open(dataFilePath, \"rb\"))\n",
    "\n",
    "variables = [\"PredictedWDP\", \"PredictedSECCHI\", \"PredictedTEMP\", \"PredictedDO\", \n",
    "           \"PredictedTURB\", \"PredictedVEL\", \"PredictedTP\", #\"PREDICTED_COND\",\n",
    "           \"PredictedTN\", \"PredictedSS\", \"PredictedCHLcal\"]\n",
    "\n",
    "variables = [\"PREDICTED_WDP\", \"PREDICTED_SECCHI\", \"PREDICTED_TEMP\", \"PREDICTED_DO\", \n",
    "           \"PREDICTED_TURB\", \"PREDICTED_VEL\", \"PREDICTED_TP\", #\"PREDICTED_COND\",\n",
    "           \"PREDICTED_TN\", \"PREDICTED_SS\", \"PREDICTED_CHLcal\"]\n",
    "\n",
    "variables = [\"WDP\", \"SECCHI\", \"TEMP\", \"DO\", \n",
    "           \"TURB\", \"VEL\", \"TP\", #\"COND\",\n",
    "           \"TN\", \"SS\", \"CHLcal\"]\n",
    "\n",
    "\n",
    "\n",
    "print(\"Json file imported\")\n",
    "\n",
    "print(len(jsonData.keys()))"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Json file imported\n",
      "3\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Functions\n",
    "See the `docstring`'s for what each function does and how it works."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "source": [
    "def getSubdf(scomplex, shape, df):\n",
    "    \"\"\"\n",
    "    Returns the part of the data frame from the particular shape in the simplicial complex.\n",
    "    params:\n",
    "    scomplex: the entire simplicial complex\n",
    "    shape: the particular shape being inspected (within the simplicial complex)\n",
    "    df: the entire data frame\n",
    "    \n",
    "    Description:\n",
    "    1. Get all the nodes from the particular simplicial complex. \n",
    "    2. Generate the indices we care about from the particular shape. To do this, we read each node and append it's \n",
    "    indices to a list. Then, we convert the list to a set and then back to a list to eliminate duplicates.\n",
    "    3. Return the dataframe with only those indices.\n",
    "    \"\"\"\n",
    "    nodes = scomplex.get('nodes')\n",
    "    indices = []\n",
    "    npShape = np.array(shape).flatten()\n",
    "    for node in npShape:\n",
    "        indices.append(nodes.get(node))\n",
    "    indices = list(set([item for sublist in indices for item in sublist]))\n",
    "    subdf = df.loc[indices]\n",
    "    return subdf\n",
    "\n",
    "def shapeDataSummary(scomplex, shape, df, variables, verbose = False):\n",
    "    \"\"\"\n",
    "    Generates summary statistics of the given variables for a given shape in the simplicial.\n",
    "    params:\n",
    "    scomplex: the entire simplicial complex\n",
    "    shape: the particular shape being inspected (within the simplicial complex) at this function call.\n",
    "    df: the entire dataframe\n",
    "    variables: the variables of interest\n",
    "    verbose: Determines if the function will print out extra information. False by default\n",
    "    \n",
    "    Description:\n",
    "    1. Create an empty result dataframe to store the summary statistics.\n",
    "    2. Get the sub dataframe (see getSubdf) for the particular shape\n",
    "    3. For each variable we are analzying, generate summary statistics from the sub dataframe and place them\n",
    "    inside the result dataframe.\n",
    "    4. Return the result dataframe\n",
    "    \n",
    "    NOTE: this only creates summaries for one particular shape. In executing this method, it is done for each shape \n",
    "    outside of the function.\n",
    "    \n",
    "    \"\"\"\n",
    "    result = pd.DataFrame()\n",
    "    if verbose == True:\n",
    "        print(\"Obtaining sub dataframe for: \", shape)\n",
    "        print(\"The number of nodes in this shape is: \", len(shape))\n",
    "    subdf = getSubdf(scomplex, shape, df)\n",
    "    if verbose == True:\n",
    "        print(\"The number of datapoints in this shape is: \", subdf.shape[0])\n",
    "    for var in variables:\n",
    "        result[var] = subdf[var].describe()\n",
    "    return result\n",
    "    \n",
    "    \n",
    "\n",
    "def adjacent(v, scomplex):\n",
    "    \"\"\"\n",
    "    Determines the nodes adjacent to a given vertex\n",
    "    \n",
    "    params:\n",
    "    v: vertex\n",
    "    scomlex: the entire simplicial complex\n",
    "    \n",
    "    Description:\n",
    "    Determines the nodes that are adjacent to a given vertex.\n",
    "    \"\"\"\n",
    "    \n",
    "    simplices = scomplex.get('simplices')\n",
    "    edges = [item for item in simplices if len(item) == 2]\n",
    "    result = []\n",
    "    for edge in edges:\n",
    "        if v in edge:\n",
    "            for item in edge:\n",
    "                if item != v:\n",
    "                    result.append(item)\n",
    "    return result\n",
    "\n",
    "def bfs(node, scomplex):\n",
    "    \"\"\"\n",
    "    Conducts a breadth first search to obtain the entire shape from a given node\n",
    "    params:\n",
    "    node: the start node\n",
    "    scomplex: the entire simplicial complex\n",
    "    \n",
    "    Description:\n",
    "    Preforms a breadth first search to obtain the entire shape for a given start node.\n",
    "    \"\"\"\n",
    "    Q = queue.Queue()\n",
    "    result = []\n",
    "    result.append(node)\n",
    "    Q.put(node)\n",
    "    while not Q.empty():\n",
    "        v = Q.get()\n",
    "        adjacentEdges = adjacent(v, scomplex)\n",
    "        for edge in adjacentEdges:\n",
    "            if edge not in result:\n",
    "                result.append(edge)\n",
    "                Q.put(edge)\n",
    "    return result\n",
    "\n",
    "\n",
    "        \n",
    "    \n",
    "def getShapes(scomplex):\n",
    "    \"\"\"\n",
    "    Gets all of the shapes from a given simplicial complex.\n",
    "    \n",
    "    params:\n",
    "    scomplex: the entire simplicial complex\n",
    "    \n",
    "    Description:\n",
    "    1. Obtain all the nodes for the entire complex\n",
    "    2. For each node, preform a breadth first search to obtain everything in that particular shape. \n",
    "    If this entire shape has not already been discovered, add it to the set of results. \n",
    "    The result item is a set as the order of the shapes does not matter. The resulting shape is a frozenset\n",
    "    which means items cannot be added or removed once created, and is needed to allow the set object to have other sets within it.\n",
    "    3. Convert each shape to a list and the result to a list for easier navigation outside of the function.\n",
    "    4. Return the result\n",
    "    \n",
    "    \"\"\"\n",
    "    \n",
    "    nodes = list(scomplex.get('nodes').keys())\n",
    "    result = set()\n",
    "    for node in nodes: # currently does more computations than necessary due to going through every node without considering it is already in a shape\n",
    "        bfsResult = frozenset(bfs(node, scomplex))\n",
    "        result.add(bfsResult)\n",
    "    result = [list(x) for x in result]\n",
    "    # Sort the list depending on what is decided: nodes or indices. Currently doing it by number of nodes\n",
    "    result.sort(key = len, reverse = True)\n",
    "    \n",
    "    \n",
    "    \n",
    "    return result\n",
    "\n",
    "def nodeDataSummary(node, scomplex, variables,df):\n",
    "    \"\"\"\n",
    "    Returns a data summary of a particular node\n",
    "    params:\n",
    "    node: node in question\n",
    "    scomplex: The entire simplicial complex\n",
    "    variables: The variables to obtain summaries\n",
    "    df: the entire dataframe \n",
    "    \n",
    "    description:\n",
    "    1. Creates a result dataframe\n",
    "    2. Get all the indices from the node from the simplicial complex\n",
    "    3. Generate summaries for each variable\n",
    "    4. Return the result\n",
    "    \"\"\"\n",
    "    result = pd.DataFrame()\n",
    "    if isinstance(node, list):\n",
    "        print(\"Node is a list and thus cannot get summary information: \", node)\n",
    "        indices = scomplex.get('nodes').get(node[0])\n",
    "        subdf = df.loc[indices]\n",
    "        for var in variables:\n",
    "            result[var] = subdf[var].describe()\n",
    "        return result\n",
    "    \n",
    "        \n",
    "    indices = scomplex.get('nodes').get(node)\n",
    "    subdf = df.loc[indices]\n",
    "    for var in variables:\n",
    "        result[var] = subdf[var].describe()\n",
    "    return result\n",
    "    \n",
    "    \n",
    "def condenseShape(shape, scomplex):\n",
    "    \"\"\"\n",
    "    \n",
    "    params:\n",
    "    shape: a shape of two nodes. must be 2\n",
    "    scomplex: the entire simplicial complex\n",
    "    \n",
    "    description:\n",
    "    gets the two nodes a and b\n",
    "    gets the indices for a and b (what is inside the nodes)\n",
    "    if a \\subseteq b, return b\n",
    "    elif b \\subseteq a, return a \n",
    "    else return shape \n",
    "    \n",
    "    \"\"\"\n",
    "    nodes = scomplex.get('nodes')\n",
    "    a = shape[0]\n",
    "    b = shape[1]\n",
    "    aIndices = set(nodes.get(a))\n",
    "    bIndices = set(nodes.get(b))\n",
    "    \n",
    "    if aIndices.issubset(bIndices):\n",
    "        return b\n",
    "    elif bIndices.issubset(aIndices):\n",
    "        return a\n",
    "    else:\n",
    "        return shape\n",
    "\n",
    "def clean_getShapes(scomplex):\n",
    "    \"\"\"\n",
    "    Condenses 1-simplices down to 0-simplices when each node \n",
    "    is a subset of the other \n",
    "    \n",
    "    params:\n",
    "    scomplex: the entire simplicial complex\n",
    "    \n",
    "    Description:\n",
    "    1. Get all the shapes from the original getShapes function\n",
    "    2. For shapes that of length 2, if one is a subset of the other, return the larger of the two\n",
    "        Otherwise, do nothing\n",
    "    3. return the clean Shapes list \n",
    "    \n",
    "    \"\"\"\n",
    "    shapes = getShapes(scomplex)\n",
    "    cleanShapes = []\n",
    "    for shape in shapes:\n",
    "        if len(shape) == 2:\n",
    "            shape = condenseShape(shape, scomplex)\n",
    "            cleanShapes.append([shape])\n",
    "        else:\n",
    "            cleanShapes.append(shape)\n",
    "    return cleanShapes\n",
    "\n",
    "\n",
    "def getBoxplots(subdf, shape, key,filePath):\n",
    "    \"\"\"\n",
    "    Generates box plots for 10 of the 11 continuous variables\n",
    "    NOTE: CONDUCTIVITY IS NOT INCLUDED\n",
    "    \n",
    "    params:\n",
    "    subdf: the sub dataframe of the particular shape\n",
    "    shape: the shape in question\n",
    "    key: what strata year season combo we are looking at \n",
    "    filePath: the output file path for all the box plots \n",
    "    \n",
    "    description:\n",
    "    clears the current plot \n",
    "    generates the sub dataframes for the respective variables.\n",
    "    the reason they are grouped is based upon the numerical outputs for making the boxplots readable\n",
    "    create a box plot, and then save it based upon the file path\n",
    "    clear the plot\n",
    "    repeat for the second set of variables\n",
    "    \"\"\"\n",
    "    plt.clf()\n",
    "    varDf1 = subdf[[\"PREDICTED_SS\",\"PREDICTED_TURB\",\"PREDICTED_TEMP\",\"PREDICTED_CHLcal\",\"PREDICTED_SECCHI\"]]\n",
    "    varDf2 = subdf[[\"PREDICTED_TP\",\"PREDICTED_TN\",\"PREDICTED_DO\",\"PREDICTED_VEL\",\"PREDICTED_WDP\"]]\n",
    "    plot1 = varDf1.boxplot(rot = 45)\n",
    "    plt.savefig(filePath + \"\\\\\" + key +\"_\" + str(shape)  + \"_SS_TURB_CHLcal\"  + \".png\")\n",
    "    plt.clf()\n",
    "    plot2 = varDf2.boxplot(rot = 45)\n",
    "    plt.savefig(filePath + \"\\\\\" + key +\"_\" + str(shape)  + \"_\" + \"TP_TN_VEL_etc\" + \".png\")\n",
    "    return plot1, plot2\n",
    "\n",
    "def determineOverlap(scomplex, shape, verbose = True):\n",
    "    \"\"\"\n",
    "    Determines the overlap within a shape. \n",
    "    \n",
    "    For each node, find it's neighbors, and generate the intersection, and saving the result without\n",
    "    duplicates through utlizing the set functionality of python.\n",
    "    \n",
    "    params:\n",
    "    scomplex: the entire simplicial complex in question\n",
    "    shapes: all the shapes\n",
    "    verbose: see prinout as the code works\n",
    "    \"\"\"\n",
    "    \n",
    "    result = set()\n",
    "    if verbose:\n",
    "        print(\"Shape: \", shape)\n",
    "    nodes = scomplex.get('nodes')\n",
    "    for node in shape:\n",
    "        # currently, this displays a lot of repeats. \n",
    "        A = set(nodes.get(node))\n",
    "        if verbose:\n",
    "            print(\"Node: \" , node  , \" | Indices: \",A)\n",
    "        B = adjacent(node, scomplex)\n",
    "        if verbose:\n",
    "            print(\"Adjacent nodes: \",B)\n",
    "        for b in B:\n",
    "            bSet = set(nodes.get(b))\n",
    "            name = str(node) + \" -> \" + str(b) +\": \"\n",
    "            intersection = set(A.intersection(bSet))\n",
    "            intersection.add(name)\n",
    "            intersection = frozenset(intersection)\n",
    "            result.add(intersection)\n",
    "            if verbose:\n",
    "                print(\"Node: \", b, \" | Indices: \", bSet)\n",
    "                print(\"Overlap is: \", A.intersection(bSet))\n",
    "    return result\n",
    "print(\"Functions loaded\")"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Functions loaded\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Generating Summary Statistics on the entire simplicial complex\n",
    "For each `mapper` output from `kepler-mapper`, we can generate the summary statistics for each of the continuous variables. This is done by first obtaining a list of the keys from the `.json` file, and then iterating through each complex, generating the shape and obtaining data summaries on each shape."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "allComplices = list(jsonData.keys())\n",
    "for key in allComplices[0:3]: # remove indices here to get all the strata for all the time periods\n",
    "    print(\"Current Simplical Complex: \", key)\n",
    "    scomplex = jsonData.get(key)\n",
    "    shapes = clean_getShapes(scomplex)\n",
    "    for shape in shapes:\n",
    "        summaries = shapeDataSummary(scomplex, shape, df_dict.get(key), variables, verbose = False)\n",
    "        if summaries.loc['count'][0] > 5 and len(shape)  > 2: # at least 6 datapoints and 3 nodes to see info\n",
    "            print(\"The shape is: \",shape)\n",
    "            print(\"The number of nodes in the shape is: \", len(shape))\n",
    "            display(summaries) # Uncomment to see summaries"
   ],
   "outputs": [],
   "metadata": {
    "scrolled": true,
    "tags": []
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Analyzing the largest structure per simplicial complex\n",
    "Largest = Node count of the shape. The largest structure is likely to be the dominant feature of the stratum during this particular time period. As such, it is important to analyze the nodes within it. To do this, we generate all the shapes, and since the shapes are returned in descending order of the number of nodes per shape, we pull the first shape. From here, we can preform an analysis on each one."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "allComplices = list(jsonData.keys())\n",
    "\n",
    "for key in allComplices[0:3]: # remove the indices here to get all the strata for all the time periods\n",
    "    print(\"Current Simplical Complex: \", key)\n",
    "    scomplex = jsonData.get(key)\n",
    "    largestShape = clean_getShapes(scomplex)[0]\n",
    "    nodes = scomplex.get('nodes')\n",
    "    print(\"Largest shape is: \", largestShape)\n",
    "    print(\"Number of nodes is: \", len(largestShape))\n",
    "    for node in largestShape:\n",
    "        summary = nodeDataSummary(node, scomplex,variables,df_dict.get(key))\n",
    "        if summary.loc['count'][0] > 5: # 5 is chosen arbitraily\n",
    "            print(\"Information for: \", node)\n",
    "            display(summary)"
   ],
   "outputs": [],
   "metadata": {
    "scrolled": true,
    "tags": []
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Condensing 1-simplices\n",
    "Currently, many one simplices that we have contain information that means one of them is a subset of the other. To resolve this, we replace them with one cluster with all the indices in one node.\n",
    "\n",
    "This is stored in the function `clean_getShapes(scomplex)` function. Below is a comparison of running the two functions\n",
    "\n",
    "Below is a demonstration of what this function does to show why it is preferred over the original `getShapes` function."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "allComplices = list(jsonData.keys())\n",
    "print(\"Standard shape version\")\n",
    "for key in allComplices[0:1]:\n",
    "    print(\"Current Simplical Complex: \", key)\n",
    "    scomplex = jsonData.get(key)\n",
    "    nodes = scomplex.get('nodes')\n",
    "    shapes = getShapes(scomplex)\n",
    "    for shape in shapes:\n",
    "        indices = []\n",
    "        for node in shape:\n",
    "            indices.append(nodes.get(node))\n",
    "        indices = list(set([item for sublist in indices for item in sublist]))\n",
    "        #print(str(shape) + \" : \" + str(indices))\n",
    "\n",
    "print(\"Clean shape version\")\n",
    "for key in allComplices[0:1]:\n",
    "    print(\"Current Simplical Complex: \", key)\n",
    "    scomplex = jsonData.get(key)\n",
    "    nodes = scomplex.get('nodes')\n",
    "    cleanShapes = clean_getShapes(scomplex)\n",
    "    for shape in cleanShapes:\n",
    "        indices = []\n",
    "        npShape = np.array(shape).flatten()\n",
    "        for node in npShape:\n",
    "            indices.append(nodes.get(node))\n",
    "        indices = list(set([item for sublist in indices for item in sublist]))\n",
    "        #print(str(shape) + \" : \" + str(indices))"
   ],
   "outputs": [],
   "metadata": {
    "scrolled": true,
    "tags": []
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Box plot per shape\n",
    "Here, we generate box plots for the variables of interest. for each shape in the simplicial complex"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "allComplices = list(jsonData.keys())\n",
    "for key in allComplices[0:1]: # remove the indices here to get all the strata for all the time periods\n",
    "    print(\"Current Simplical Complex: \", key)\n",
    "    scomplex = jsonData.get(key)\n",
    "    shapes = clean_getShapes(scomplex)\n",
    "    print(\"number of shapes: \", len(shapes))\n",
    "    for shape in shapes:\n",
    "        \"\"\"\n",
    "        Hello whoever is using this function\n",
    "        getBoxplots takes a couple arguments. \n",
    "        the big thing that matters here is the \n",
    "        strataYear variable. Essentially, the key that allows us to access each simplicial complex is weird.\n",
    "        Depending on your file system, using the str(key) conversion may cause errors. to resolve this, \n",
    "        below is a potential example. Feel free to change it as you go for your use.\n",
    "        \"\"\"\n",
    "        strataYear = str(key).replace(\" \",\"-\").replace(\":\",\"\").replace(\"[\",\"\").replace(\"]\",\"\").replace(\"'\",\"\")\n",
    "        print(strataYear)\n",
    "        \"\"\"\n",
    "        before: \n",
    "        ['Stratum 1 SUMMER 93-00: ']\n",
    "        after:\n",
    "        Stratum-1-SUMMER-93-00-\n",
    "        \"\"\"\n",
    "        subdf = getSubdf(scomplex, shape, df_dict.get(key))\n",
    "        plots = getBoxplots(subdf, shape, strataYear,  \n",
    "                            filePath = r\"C:\\Users\\forre\\Desktop\\REU\\TDA\\Data\\TDAOutputs\\Boxplots\")\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Discovering which indices within the nodes overlap\n",
    "This code will determine what points in the nodes are overlapping within two nodes. This can have applications to see what is similar between the nodes, and potentially determine the \"width\" of the edges between two nodes. "
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "allComplices = list(jsonData.keys())\n",
    "for key in allComplices[0:1]: # remove the indices here to get all the strata for all the time periods\n",
    "    print(\"Current Simplical Complex: \", key)\n",
    "    scomplex = jsonData.get(key)\n",
    "    shapes = clean_getShapes(scomplex)\n",
    "    print(\"number of shapes: \", len(shapes))\n",
    "    result = list(determineOverlap(scomplex, shapes[0], verbose = False))\n",
    "    print(\"Shape: \", shapes[0])\n",
    "    # todo: Currently, result double counts. prevent that.\n",
    "    for item in result:\n",
    "        itemList = list(item)\n",
    "        for a in itemList:\n",
    "            if isinstance(a, str):\n",
    "                print(a)\n",
    "        s = []\n",
    "        for b in itemList:\n",
    "            if isinstance(b, int):\n",
    "                s.append(b)\n",
    "        print(s)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# See compare shapes over the years\n",
    "NOTE: Comparing the largest shape in each.\n",
    "\n",
    "Modifying what complices can be seen and normalizing the variables, we can generate many box plots to see what variables are changing over time in the largest shape."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "allComplices = list(jsonData.keys())\n",
    "allComplices = [\"['Stratum 1 SUMMER 93-00: ']\",\n",
    "               \"['Stratum 1 SUMMER 98-04: ']\",\n",
    "               \"['Stratum 1 SUMMER 01-13: ']\",\n",
    "                \"['Stratum 1 SUMMER 10-16: ']\",\n",
    "                \"['Stratum 1 SUMMER 14-20: ']\"\n",
    "               ] # just looking at one year\n",
    "\n",
    "for key in allComplices:\n",
    "    print(key)\n",
    "    temp = pd.DataFrame()\n",
    "    scomplex = jsonData.get(key)\n",
    "    data = df_dict.get(key)\n",
    "    shape = clean_getShapes(scomplex)[0]\n",
    "    print(shape)\n",
    "    subdf = getSubdf(scomplex, shape, data)\n",
    "    print(data.shape[0])\n",
    "    subdfCopy = subdf.copy()\n",
    "    for var in variables:\n",
    "        subdfCopy[var] =  subdfCopy[var]/subdfCopy[var].max()\n",
    "    for var in variables:\n",
    "        \n",
    "        temp[var] = subdfCopy[var].describe()\n",
    "    display(temp)\n",
    "    strataYear = str(key).replace(\" \",\"-\").replace(\":\",\"\").replace(\"[\",\"\").replace(\"]\",\"\").replace(\"'\",\"\")\n",
    "    print(strataYear)\n",
    "    getBoxplots(subdfCopy, shape, strataYear,filePath = r\"C:\\Users\\forre\\Desktop\\REU\\TDA\\Data\\TDAOutputs\\Stratum1Box\")"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Calculating Density for Nodes in the Largest Structure\n",
    "Note that density is jsonData"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "source": [
    "def k_nearest_neighbors(df, neigh, point, k):\n",
    "    return neigh.kneighbors([list(df.loc[point])], k)[0].flatten()\n",
    "    \n",
    "def calculate_density(scomplex, node, df, k):\n",
    "    neigh = NearestNeighbors(n_neighbors=k)\n",
    "    neigh.fit(df)\n",
    "    knn = 0\n",
    "    n = len(scomplex['nodes'][node])\n",
    "    for point in scomplex['nodes'][node]:\n",
    "        distances = k_nearest_neighbors(df, neigh, point, k)\n",
    "        knn += (sum(distances) / k)\n",
    "    density = knn / (n*n)\n",
    "    return (1.0 / density)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "source": [
    "allComplices = list(jsonData.keys())\n",
    "df = pd.read_csv(\"../../LTRM data/RF interpolation/water_full.csv\")\n",
    "#df = df[df['FLDNUM']=='Havana, IL']\n",
    "df = df[[\"WDP\", \"SECCHI\", \"TEMP\", \"DO\", \n",
    "           \"TURB\", \"VEL\", \"TP\", \n",
    "           \"TN\", \"SS\", \"CHLcal\"]]\n",
    "\n",
    "for key in allComplices: # remove the indices here to get all the strata for all the time periods\n",
    "    print(\"Current Simplical Complex: \", key)\n",
    "    scomplex = jsonData.get(key)\n",
    "    scomplex['density'] = {}\n",
    "    largestShape = clean_getShapes(scomplex)[0]\n",
    "    #print(\"Largest shape is: \", largestShape, \"\\n\")\n",
    "    largestShape_df = getSubdf(scomplex, largestShape, df_dict.get(key))\n",
    "    k = int((df_dict.get(key).shape[0] / 10) + 1)\n",
    "    for node_name in largestShape:\n",
    "        #scomplex['density'][node_name] = calculate_density(scomplex, node_name, largestShape_df, k)\n",
    "        scomplex['density'][node_name] = calculate_density(scomplex, node_name, df_dict.get(key), k)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Current Simplical Complex:  25% SUBSET: MinClust_10_MinSamp_10\n",
      "Current Simplical Complex:  50% SUBSET: MinClust_10_MinSamp_10\n",
      "Current Simplical Complex:  100% SUBSET: MinClust_10_MinSamp_10\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Creating the Directed Graph"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "source": [
    "def generate_graph(scomplex, shape):\n",
    "    dg = generate_nodes(scomplex, shape)\n",
    "    dg = generate_edges(scomplex, shape, dg)\n",
    "    return dg\n",
    "\n",
    "def generate_nodes(scomplex, shape):\n",
    "    dg = nx.DiGraph()\n",
    "    dg.add_nodes_from(shape)\n",
    "    return dg\n",
    "\n",
    "def generate_edges(scomplex, shape, dg):\n",
    "    for node in shape:\n",
    "        if node in scomplex['links']:\n",
    "            for adjacent_node in scomplex['links'][node]:\n",
    "                if scomplex['density'][node] < scomplex['density'][adjacent_node]:\n",
    "                    dg.add_edge(node, adjacent_node)\n",
    "                else:\n",
    "                    dg.add_edge(adjacent_node, node)\n",
    "    return dg\n",
    "\n",
    "def get_local_maxima(dg):\n",
    "    maxima = []\n",
    "    for node in list(dg.nodes):\n",
    "        succ = dict(nx.bfs_successors(dg, source=node))\n",
    "        if not succ[node]:\n",
    "            maxima.append(node)\n",
    "    return maxima\n",
    "\n",
    "def draw_graph(scomplex, filepath, with_labels=False):\n",
    "    colors = ['#00A08A', '#e80909', '#F2AD00', '#a6d96a', '#d9ef8b']\n",
    "    colors_dict = {scomplex['maxima'][i] : colors[i] for i in range(len(scomplex['maxima']))}\n",
    "    transition_color = '#ffffbf'\n",
    "    color_map = []\n",
    "    scomplex['states'] = {scomplex['maxima'][i] : [] for i in range(len(scomplex['maxima']))}\n",
    "    \n",
    "    fig = plt.figure(figsize = (12,12))\n",
    "    ax = plt.subplot(111)\n",
    "    #ax.set_title('Nurmom_CUBES = 100, PERC_OVERLAP = .3', fontsize=72)\n",
    "    \n",
    "    for node in scomplex['graph']:\n",
    "        distDict = {scomplex['maxima'][i] : graph_distance(scomplex['graph'], node, scomplex['maxima'][i])\n",
    "                    for i in range(len(scomplex['maxima']))}\n",
    "        minDist = min(distDict.values())\n",
    "        states = [maxima if distDict[maxima] == minDist else None for maxima in scomplex['maxima']]\n",
    "        states = list(filter(None, states))\n",
    "        \n",
    "        for state in states:\n",
    "            scomplex['states'][state].append(node)\n",
    "        \n",
    "        if len(states) > 1:\n",
    "            color_map.append(transition_color)\n",
    "        else:\n",
    "            color_map.append(colors_dict[states[0]])\n",
    "\n",
    "    \n",
    "    nx.draw_kamada_kawai(scomplex['graph'], with_labels = with_labels, node_color = color_map)\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(filepath, format = \"PNG\")\n",
    "    plt.show()\n",
    "\n",
    "def graph_distance(dg, source, target):\n",
    "    if nx.has_path(dg, source, target):\n",
    "        return len(nx.shortest_path(dg, source, target))\n",
    "    return float('inf')"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "source": [
    "for key in allComplices: # remove the indices here to get all the strata for all the time periods\n",
    "    scomplex = jsonData.get(key)\n",
    "    print(key)\n",
    "    largestShape = clean_getShapes(scomplex)[0]\n",
    "    scomplex['graph'] = {}\n",
    "    scomplex['graph'] = generate_graph(scomplex, largestShape)\n",
    "    scomplex['maxima'] = get_local_maxima(scomplex['graph'])"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "25% SUBSET: MinClust_10_MinSamp_10\n",
      "50% SUBSET: MinClust_10_MinSamp_10\n",
      "100% SUBSET: MinClust_10_MinSamp_10\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "source": [
    "# With HDBSCAN\n",
    "print(jsonData['100% SUBSET: MinClust_10_MinSamp_10']['maxima']) #12\n",
    "print(jsonData['50% SUBSET: MinClust_10_MinSamp_10']['maxima']) #9\n",
    "print(jsonData['25% SUBSET: MinClust_10_MinSamp_10']['maxima']) #20"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "['cube308_cluster0', 'cube31_cluster2', 'cube98_cluster2', 'cube443_cluster0', 'cube308_cluster1', 'cube445_cluster1', 'cube446_cluster0', 'cube154_cluster1', 'cube196_cluster1']\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "source": [
    "#key name is not related. This is all of LaGrange\n",
    "filepath =  \"HDB_25%.png\"\n",
    "draw_graph(jsonData[\"25% SUBSET: MinClust_10_MinSamp_10\"], filepath, with_labels=False)"
   ],
   "outputs": [
    {
     "output_type": "error",
     "ename": "IndexError",
     "evalue": "list index out of range",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-21-8c9c20b9e94a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m#key name is not related. This is all of LaGrange\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mfilepath\u001b[0m \u001b[0;34m=\u001b[0m  \u001b[0;34m\"HDB_25%.png\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mdraw_graph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mjsonData\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"25% SUBSET: MinClust_10_MinSamp_10\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfilepath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mwith_labels\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-14-88088884584e>\u001b[0m in \u001b[0;36mdraw_graph\u001b[0;34m(scomplex, filepath, with_labels)\u001b[0m\n\u001b[1;32m     29\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mdraw_graph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mscomplex\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfilepath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mwith_labels\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     30\u001b[0m     \u001b[0mcolors\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m'#00A08A'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'#e80909'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'#F2AD00'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'#a6d96a'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'#d9ef8b'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 31\u001b[0;31m     \u001b[0mcolors_dict\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0mscomplex\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'maxima'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m:\u001b[0m \u001b[0mcolors\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mscomplex\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'maxima'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     32\u001b[0m     \u001b[0mtransition_color\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'#ffffbf'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     33\u001b[0m     \u001b[0mcolor_map\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-14-88088884584e>\u001b[0m in \u001b[0;36m<dictcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     29\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mdraw_graph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mscomplex\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfilepath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mwith_labels\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     30\u001b[0m     \u001b[0mcolors\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m'#00A08A'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'#e80909'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'#F2AD00'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'#a6d96a'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'#d9ef8b'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 31\u001b[0;31m     \u001b[0mcolors_dict\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0mscomplex\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'maxima'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m:\u001b[0m \u001b[0mcolors\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mscomplex\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'maxima'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     32\u001b[0m     \u001b[0mtransition_color\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'#ffffbf'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     33\u001b[0m     \u001b[0mcolor_map\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mIndexError\u001b[0m: list index out of range"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "source": [
    "keys = list(jsonData[\"['Stratum 3 WINTER 14-20: ']\"]['states'].keys())\n",
    "for key in keys:\n",
    "    print(len(jsonData[\"['Stratum 3 WINTER 14-20: ']\"]['states'][key]))"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "88\n",
      "6\n",
      "10\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [],
   "outputs": [],
   "metadata": {}
  }
 ],
 "metadata": {
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.9.5 64-bit ('3.9')"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  },
  "interpreter": {
   "hash": "7812ea015bdcee6f23a998adcdd2ef97c151c0c241b7b7070987d9313e41299d"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}