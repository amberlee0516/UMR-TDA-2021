{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "pediatric-mileage",
   "metadata": {},
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "other-german",
   "metadata": {},
   "outputs": [],
   "source": [
    "import kmapper as km\n",
    "from kmapper.plotlyviz import *\n",
    "import sklearn\n",
    "# DBSCAN from sklearn for clustering algorithms\n",
    "from sklearn.cluster import DBSCAN\n",
    "# PCA from sklearn for projection/lens creation\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "# Plotly and Dash\n",
    "import plotly.graph_objs as go\n",
    "import dash_html_components as html\n",
    "import dash_core_components as dcc\n",
    "import dash\n",
    "from ipywidgets import interactive, HBox, VBox, widgets, interact\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "import pandas as pd\n",
    "import pickle,json"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "advance-trail",
   "metadata": {},
   "source": [
    "### Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "individual-touch",
   "metadata": {},
   "outputs": [],
   "source": [
    "def cluster_fun(X, DBSCAN_EPSILON = 20, DBSCAN_MIN_SAMPLES = 1, N_CUBES = [7,7], PERC_OVERLAP = [.5,.5]):\n",
    "    \"\"\"\n",
    "\n",
    "    \"\"\"\n",
    "    \n",
    "    \n",
    "#     keys = list(dict_df.keys())\n",
    "#     print(keys)\n",
    "#     X = dict_df.get(keys[0])\n",
    "    # Killians interpolation naming for important columns\n",
    "    kill = [\"PREDICTED_WDP\", \"PREDICTED_SECCHI\", \"PREDICTED_TEMP\", \"PREDICTED_DO\", \n",
    "           \"PREDICTED_TURB\",\"PREDICTED_COND\", \"PREDICTED_VEL\", \"PREDICTED_TP\", \n",
    "           \"PREDICTED_TN\", \"PREDICTED_SS\", \"PREDICTED_CHLcal\"]\n",
    "    \n",
    "    # My interpolation naming for important columns\n",
    "    my = ['PredictedTN','PredictedTP','TEMP','DO','TURB','COND','PredictedVEL','SS','WDP','CHLcal','SECCHI']\n",
    "    nocond = ['PredictedTN','PredictedTP','TEMP','DO','TURB','PredictedVEL','SS','WDP','CHLcal','SECCHI']\n",
    "\n",
    "    X = X[nocond]\n",
    "    \n",
    "    \n",
    "    if X.shape[0]<DBSCAN_MIN_SAMPLES:\n",
    "        #print(X)\n",
    "        print(\"Not enough data to cluster in \", keys, \"_size = \", X.shape[0])\n",
    "        print(\"DBSCAN_MIN_SAMPLES\", DBSCAN_MIN_SAMPLES)\n",
    "        return([DBSCAN_MIN_SAMPLES, X.shape[0]])\n",
    "    \n",
    "    \n",
    "    db = DBSCAN(eps=20, min_samples=2).fit(X)\n",
    "    # Number of clusters in labels, ignoring noise if present.\n",
    "    labels = db.labels_\n",
    "    n_clusters_ = len(set(labels)) - (1 if -1 in labels else 0)\n",
    "    n_noise_ = list(labels).count(-1)\n",
    "\n",
    "    print('Estimated number of clusters: %d' % n_clusters_)\n",
    "    print('Estimated number of noise points: %d' % n_noise_)\n",
    "    \n",
    "    return(db)\n",
    "\n",
    "    \n",
    "    \n",
    "def mapper_pca_func(X,title, DBSCAN_EPSILON = 20, DBSCAN_MIN_SAMPLES = 1, N_CUBES = 7, PERC_OVERLAP = .5):\n",
    "    \"\"\"\n",
    "    str(df_stratum_season_time_dict_list[1].keys())\n",
    "    \"\"\"\n",
    "    \n",
    "    # \n",
    "#     keys = list(dict_df.keys())\n",
    "#     print(keys)\n",
    "#     X = dict_df.get(keys[0])\n",
    "    kill = [\"PREDICTED_WDP\", \"PREDICTED_SECCHI\", \"PREDICTED_TEMP\", \"PREDICTED_DO\", \n",
    "           \"PREDICTED_TURB\",\"PREDICTED_COND\", \"PREDICTED_VEL\", \"PREDICTED_TP\", \n",
    "           \"PREDICTED_TN\", \"PREDICTED_SS\", \"PREDICTED_CHLcal\"]\n",
    "    \n",
    "    # My interpolation naming for important columns\n",
    "    my = ['PredictedTN','PredictedTP','TEMP','DO','TURB','COND','PredictedVEL','SS','WDP','CHLcal','SECCHI']\n",
    "    nocond = ['PredictedTN','PredictedTP','TEMP','DO','TURB','PredictedVEL','SS','WDP','CHLcal','SECCHI']\n",
    "\n",
    "    X = X[nocond]\n",
    "    \n",
    "    X.reset_index(drop=True,inplace = True)\n",
    "    \n",
    "    var_to_index = {nocond[i] : i for i in range(len(nocond))}\n",
    "    projected_vars = nocond\n",
    "    projected_var_indices = [var_to_index[var] for var in projected_vars]\n",
    "    \n",
    "    print(\"X.shape: \",X.shape[0])\n",
    "    \n",
    "    if X.shape[0]<10:\n",
    "        #print(X)\n",
    "        print(\"Not enough data in \", keys, \"_size = \", X.shape[0])\n",
    "        return(X.shape[0])\n",
    "\n",
    "    # defining clustering and kmapper parameters\n",
    "    \n",
    "    # create instance of clustering alg\n",
    "    cluster_alg = sklearn.cluster.DBSCAN(eps=DBSCAN_EPSILON, min_samples=DBSCAN_MIN_SAMPLES, metric='euclidean')\n",
    "\n",
    "    # Instantiate kepler mapper object\n",
    "    mapper = km.KeplerMapper(verbose=0)\n",
    "    \n",
    "    # defining filter function as projection on to the first 2 component axis\n",
    "    pca = PCA(n_components=1)\n",
    "    lens = pca.fit_transform(X)\n",
    "    print(\"Variance explained: \",pca.explained_variance_ratio_)\n",
    "    \n",
    "    # Project data onto the ones we want to display\n",
    "    summary_variable = mapper.project(np.array(X),projection = projected_var_indices, scaler=None)\n",
    "    \n",
    "    # Generate the simplicial complex\n",
    "    scomplex = mapper.map(lens, X, cover=km.Cover(n_cubes=N_CUBES, perc_overlap=PERC_OVERLAP), clusterer=cluster_alg,remove_duplicate_nodes=True)  \n",
    "\n",
    "\n",
    "    pl_brewer = [[0.0, '#006837'],\n",
    "             [0.1, '#1a9850'],\n",
    "             [0.2, '#66bd63'],\n",
    "             [0.3, '#a6d96a'],\n",
    "             [0.4, '#d9ef8b'],\n",
    "             [0.5, '#ffffbf'],\n",
    "             [0.6, '#fee08b'],\n",
    "             [0.7, '#fdae61'],\n",
    "             [0.8, '#f46d43'],\n",
    "             [0.9, '#d73027'],\n",
    "             [1.0, '#a50026']]\n",
    "\n",
    "    color_values = lens [:,0] - lens[:,0].min()\n",
    "    my_colorscale = pl_brewer\n",
    "    color_function_name = \"Distance to x-min\"\n",
    "    kmgraph,  mapper_summary, colorf_distribution = get_mapper_graph(scomplex, \n",
    "                                                                     color_values,  \n",
    "                                                                     color_function_name=color_function_name, \n",
    "                                                                     colorscale=my_colorscale)\n",
    "\n",
    "    bgcolor = 'rgba(10,10,10, 0.9)'\n",
    "    # y_gridcolor = 'rgb(150,150,150)'# on a black background the gridlines are set on  grey\n",
    "\n",
    "    plotly_graph_data = plotly_graph(kmgraph, graph_layout='fr', colorscale=my_colorscale, \n",
    "                                     factor_size=2.5, edge_linewidth=0.5)\n",
    "    plot_title = title + str(DBSCAN_EPSILON) + str(DBSCAN_EPSILON) + ', MIN_SAMPLES ' + str(DBSCAN_MIN_SAMPLES) \n",
    "    # plot_title = 'Pool 13, Summer 1993-1999; Epsilon ' + str(DBSCAN_EPSILON) + ', MIN_SAMPLES ' + str(DBSCAN_MIN_SAMPLES) \n",
    "    layout = plot_layout(title=plot_title,  \n",
    "                         width=620, height=570,\n",
    "                         annotation_text=get_kmgraph_meta(mapper_summary),  \n",
    "                         bgcolor=bgcolor)\n",
    "    \n",
    "    # FigureWidget is responsible for event listeners\n",
    "\n",
    "    fw_graph = go.FigureWidget(data=plotly_graph_data, layout=layout)\n",
    "    fw_hist = node_hist_fig(colorf_distribution, bgcolor=bgcolor)\n",
    "    fw_summary = summary_fig(mapper_summary, height=300)\n",
    "\n",
    "    dashboard = hovering_widgets(kmgraph, \n",
    "                                 fw_graph, \n",
    "                                 bgcolor=bgcolor, \n",
    "                                 member_textbox_width=600)\n",
    "\n",
    "    # DESIRED FILE PATH, CHANGE TO FIT YOUR LOCAL MACHINE\n",
    "    directory_path = r\"Mapper outputs\\Conductivity Removed PCA 1\"\n",
    "    \n",
    "    #Update the fw_graph colorbar, setting its title:\n",
    "    fw_graph.data[1].marker.colorbar.title = 'dist to<br>x-min'\n",
    "    html_output_path = directory_path + \"\\\\\" + title + 'PCA_1' + 'all_var_' + 'Eps_' + str(DBSCAN_EPSILON) +'MinS_' + str(DBSCAN_MIN_SAMPLES) + 'NCUBES_' + str(N_CUBES) + 'PEROvLp_' + str(PERC_OVERLAP) + '.html'\n",
    "    html_output_path = html_output_path.replace(\":\",\"_\")\n",
    "    mapper.visualize(scomplex, path_html=html_output_path, color_values=color_values, color_function_name=color_function_name,lens=summary_variable,lens_names=projected_vars)\n",
    "    return scomplex, X"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "middle-devon",
   "metadata": {},
   "source": [
    "### Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "democratic-logistics",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initial interpolated size:  12381\n",
      "Getting data TDA ready\n",
      "TDA ready data size:  11314\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>SHEETBAR</th>\n",
       "      <th>DATE</th>\n",
       "      <th>LATITUDE</th>\n",
       "      <th>LONGITUDE</th>\n",
       "      <th>FLDNUM</th>\n",
       "      <th>STRATUM</th>\n",
       "      <th>LOCATCD</th>\n",
       "      <th>TN</th>\n",
       "      <th>TP</th>\n",
       "      <th>TEMP</th>\n",
       "      <th>...</th>\n",
       "      <th>SS</th>\n",
       "      <th>WDP</th>\n",
       "      <th>CHLcal</th>\n",
       "      <th>SECCHI</th>\n",
       "      <th>YEAR</th>\n",
       "      <th>MONTH</th>\n",
       "      <th>SEASON</th>\n",
       "      <th>PredictedTN</th>\n",
       "      <th>PredictedTP</th>\n",
       "      <th>PredictedVEL</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>45001693</td>\n",
       "      <td>10/10/1995</td>\n",
       "      <td>37.288269</td>\n",
       "      <td>-89.514765</td>\n",
       "      <td>5</td>\n",
       "      <td>2</td>\n",
       "      <td>9553072</td>\n",
       "      <td>1.273</td>\n",
       "      <td>0.084</td>\n",
       "      <td>12.24140</td>\n",
       "      <td>...</td>\n",
       "      <td>25.6</td>\n",
       "      <td>5.0</td>\n",
       "      <td>12.24140</td>\n",
       "      <td>41.0</td>\n",
       "      <td>1995</td>\n",
       "      <td>10</td>\n",
       "      <td>FALL</td>\n",
       "      <td>1.273000</td>\n",
       "      <td>0.084000</td>\n",
       "      <td>0.01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>45001698</td>\n",
       "      <td>10/10/1995</td>\n",
       "      <td>37.282954</td>\n",
       "      <td>-89.517829</td>\n",
       "      <td>5</td>\n",
       "      <td>2</td>\n",
       "      <td>9553149</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>11.99852</td>\n",
       "      <td>...</td>\n",
       "      <td>31.6</td>\n",
       "      <td>3.4</td>\n",
       "      <td>11.99852</td>\n",
       "      <td>40.0</td>\n",
       "      <td>1995</td>\n",
       "      <td>10</td>\n",
       "      <td>FALL</td>\n",
       "      <td>1.229465</td>\n",
       "      <td>0.103828</td>\n",
       "      <td>0.04</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>45001699</td>\n",
       "      <td>10/10/1995</td>\n",
       "      <td>37.276890</td>\n",
       "      <td>-89.510781</td>\n",
       "      <td>5</td>\n",
       "      <td>2</td>\n",
       "      <td>9553073</td>\n",
       "      <td>1.332</td>\n",
       "      <td>0.126</td>\n",
       "      <td>13.77964</td>\n",
       "      <td>...</td>\n",
       "      <td>35.1</td>\n",
       "      <td>1.3</td>\n",
       "      <td>13.77964</td>\n",
       "      <td>41.0</td>\n",
       "      <td>1995</td>\n",
       "      <td>10</td>\n",
       "      <td>FALL</td>\n",
       "      <td>1.332000</td>\n",
       "      <td>0.126000</td>\n",
       "      <td>0.31</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>45001702</td>\n",
       "      <td>10/10/1995</td>\n",
       "      <td>37.266244</td>\n",
       "      <td>-89.501127</td>\n",
       "      <td>5</td>\n",
       "      <td>2</td>\n",
       "      <td>9553074</td>\n",
       "      <td>1.302</td>\n",
       "      <td>0.085</td>\n",
       "      <td>7.70764</td>\n",
       "      <td>...</td>\n",
       "      <td>14.1</td>\n",
       "      <td>2.0</td>\n",
       "      <td>7.70764</td>\n",
       "      <td>50.0</td>\n",
       "      <td>1995</td>\n",
       "      <td>10</td>\n",
       "      <td>FALL</td>\n",
       "      <td>1.302000</td>\n",
       "      <td>0.085000</td>\n",
       "      <td>0.10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>45001704</td>\n",
       "      <td>10/10/1995</td>\n",
       "      <td>37.245900</td>\n",
       "      <td>-89.498688</td>\n",
       "      <td>5</td>\n",
       "      <td>2</td>\n",
       "      <td>9553046</td>\n",
       "      <td>2.007</td>\n",
       "      <td>0.561</td>\n",
       "      <td>10.05548</td>\n",
       "      <td>...</td>\n",
       "      <td>49.3</td>\n",
       "      <td>2.8</td>\n",
       "      <td>10.05548</td>\n",
       "      <td>35.0</td>\n",
       "      <td>1995</td>\n",
       "      <td>10</td>\n",
       "      <td>FALL</td>\n",
       "      <td>2.007000</td>\n",
       "      <td>0.561000</td>\n",
       "      <td>0.02</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 23 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "    SHEETBAR        DATE   LATITUDE  LONGITUDE  FLDNUM  STRATUM  LOCATCD  \\\n",
       "2   45001693  10/10/1995  37.288269 -89.514765       5        2  9553072   \n",
       "4   45001698  10/10/1995  37.282954 -89.517829       5        2  9553149   \n",
       "5   45001699  10/10/1995  37.276890 -89.510781       5        2  9553073   \n",
       "8   45001702  10/10/1995  37.266244 -89.501127       5        2  9553074   \n",
       "10  45001704  10/10/1995  37.245900 -89.498688       5        2  9553046   \n",
       "\n",
       "       TN     TP      TEMP  ...    SS  WDP    CHLcal  SECCHI  YEAR  MONTH  \\\n",
       "2   1.273  0.084  12.24140  ...  25.6  5.0  12.24140    41.0  1995     10   \n",
       "4     NaN    NaN  11.99852  ...  31.6  3.4  11.99852    40.0  1995     10   \n",
       "5   1.332  0.126  13.77964  ...  35.1  1.3  13.77964    41.0  1995     10   \n",
       "8   1.302  0.085   7.70764  ...  14.1  2.0   7.70764    50.0  1995     10   \n",
       "10  2.007  0.561  10.05548  ...  49.3  2.8  10.05548    35.0  1995     10   \n",
       "\n",
       "    SEASON  PredictedTN  PredictedTP PredictedVEL  \n",
       "2     FALL     1.273000     0.084000         0.01  \n",
       "4     FALL     1.229465     0.103828         0.04  \n",
       "5     FALL     1.332000     0.126000         0.31  \n",
       "8     FALL     1.302000     0.085000         0.10  \n",
       "10    FALL     2.007000     0.561000         0.02  \n",
       "\n",
       "[5 rows x 23 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predicted_df = pd.read_csv(r\"Interpolated Data\\Open_river_interpolatedTN_TP_VEL.csv\")\n",
    "predicted_df.drop(\"Unnamed: 0\",axis = 1,inplace = True)\n",
    "continuous = ['PredictedTN','PredictedTP','TEMP','DO','TURB','COND','PredictedVEL','SS','WDP','CHLcal','SECCHI']\n",
    "nocond = ['PredictedTN','PredictedTP','TEMP','DO','TURB','PredictedVEL','SS','WDP','CHLcal','SECCHI']\n",
    "print(\"Initial interpolated size: \",predicted_df.shape[0])\n",
    "print(\"Getting data TDA ready\")\n",
    "predicted_df.drop(\"COND\",axis=1,inplace=True)\n",
    "predicted_df.dropna(axis=0, how='any', thresh=None, subset=nocond, inplace=True)\n",
    "print(\"TDA ready data size: \",predicted_df.shape[0])\n",
    "predicted_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "toxic-dryer",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10\n"
     ]
    }
   ],
   "source": [
    "### Creating three main time spans and two overlapping time spans,\n",
    "### a total of five time spans\n",
    "\n",
    "# defining different time periods\n",
    "# first decade\n",
    "time_dec1 = [1993, 1994, 1995, 1997, 1998, 1999, 2000]\n",
    "# second decade\n",
    "time_dec2 = [2001, 2002,2003, 2004, 2005, 2006, 2007, 2008, 2009, 2010, 2011, 2012, 2013]\n",
    "# third decade\n",
    "time_dec3 = [2014, 2015, 2016, 2017, 2018, 2019, 2020]\n",
    "# overlap time periods for continuity\n",
    "time_overlap1 = [1998, 1999, 2000, 2001, 2002,2003, 2004]\n",
    "time_overlap2 = [2010, 2011, 2012, 2013, 2014, 2015, 2016]\n",
    "time_list = [time_dec1, time_overlap1, time_dec2, time_overlap2, time_dec3]\n",
    "\n",
    "time_list_names = ['93-00', '98-04', '01-13', '10-16', '14-20']\n",
    "# The stratums in my pool\n",
    "stratum_list = [1, 2] #, 4, 5, 6, 7, 9]\n",
    "# The seasons we are currently looking at\n",
    "Season_names = [\"SUMMER\"] #, \"SPRING\", \"FALL\", \"WINTER\"]\n",
    "\n",
    "\n",
    "df_stratum_season_time_dict = {}\n",
    "s=\"\"\n",
    "for i in range(len(time_list)):\n",
    "    for j in stratum_list:\n",
    "        for k in Season_names:\n",
    "            s= \"Stratum \" + str(j)+ \" \" + k + \" \" + time_list_names[i] + \": \"\n",
    "            df_stratum_season_time_dict[s] = predicted_df[(predicted_df['YEAR'].isin(time_list[i])) &\n",
    "                                                             (predicted_df['STRATUM'].isin([j])) & \n",
    "                                                             (predicted_df['SEASON'].isin([k]))]\n",
    "            s=\"\"\n",
    "print(len(df_stratum_season_time_dict))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "professional-virus",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stratum 1 SUMMER 93-00: \n",
      "Stratum 2 SUMMER 93-00: \n",
      "Stratum 1 SUMMER 98-04: \n",
      "Stratum 2 SUMMER 98-04: \n",
      "Stratum 1 SUMMER 01-13: \n",
      "Stratum 2 SUMMER 01-13: \n",
      "Stratum 1 SUMMER 10-16: \n",
      "Stratum 2 SUMMER 10-16: \n",
      "Stratum 1 SUMMER 14-20: \n",
      "Stratum 2 SUMMER 14-20: \n"
     ]
    }
   ],
   "source": [
    "for key in df_stratum_season_time_dict:\n",
    "    print(key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "failing-country",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Estimated number of clusters: 6\n",
      "Estimated number of noise points: 12\n",
      "Estimated number of clusters: 6\n",
      "Estimated number of noise points: 15\n",
      "Estimated number of clusters: 5\n",
      "Estimated number of noise points: 13\n",
      "Estimated number of clusters: 5\n",
      "Estimated number of noise points: 16\n",
      "Estimated number of clusters: 5\n",
      "Estimated number of noise points: 12\n",
      "Estimated number of clusters: 3\n",
      "Estimated number of noise points: 10\n",
      "Estimated number of clusters: 10\n",
      "Estimated number of noise points: 25\n",
      "Estimated number of clusters: 11\n",
      "Estimated number of noise points: 17\n",
      "Estimated number of clusters: 12\n",
      "Estimated number of noise points: 32\n",
      "Estimated number of clusters: 16\n",
      "Estimated number of noise points: 26\n"
     ]
    }
   ],
   "source": [
    "dbclus_dict = {}\n",
    "for key in df_stratum_season_time_dict:\n",
    "    dbclus_dict[key] = cluster_fun(df_stratum_season_time_dict[key])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "brazilian-expression",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X.shape:  236\n",
      "Variance explained:  [0.97162739]\n",
      "X.shape:  236\n",
      "Variance explained:  [0.97162739]\n",
      "X.shape:  236\n",
      "Variance explained:  [0.97162739]\n",
      "X.shape:  236\n",
      "Variance explained:  [0.97162739]\n",
      "X.shape:  236\n",
      "Variance explained:  [0.97162739]\n",
      "X.shape:  236\n",
      "Variance explained:  [0.97162739]\n",
      "X.shape:  236\n",
      "Variance explained:  [0.97162739]\n",
      "X.shape:  236\n",
      "Variance explained:  [0.97162739]\n",
      "X.shape:  236\n",
      "Variance explained:  [0.97162739]\n",
      "X.shape:  255\n",
      "Variance explained:  [0.96113053]\n",
      "X.shape:  255\n",
      "Variance explained:  [0.96113053]\n",
      "X.shape:  255\n",
      "Variance explained:  [0.96113053]\n",
      "X.shape:  255\n",
      "Variance explained:  [0.96113053]\n",
      "X.shape:  255\n",
      "Variance explained:  [0.96113053]\n",
      "X.shape:  255\n",
      "Variance explained:  [0.96113053]\n",
      "X.shape:  255\n",
      "Variance explained:  [0.96113053]\n",
      "X.shape:  255\n",
      "Variance explained:  [0.96113053]\n",
      "X.shape:  255\n",
      "Variance explained:  [0.96113053]\n",
      "X.shape:  408\n",
      "Variance explained:  [0.95382488]\n",
      "X.shape:  408\n",
      "Variance explained:  [0.95382488]\n",
      "X.shape:  408\n",
      "Variance explained:  [0.95382488]\n",
      "X.shape:  408\n",
      "Variance explained:  [0.95382488]\n",
      "X.shape:  408\n",
      "Variance explained:  [0.95382488]\n",
      "X.shape:  408\n",
      "Variance explained:  [0.95382488]\n",
      "X.shape:  408\n",
      "Variance explained:  [0.95382488]\n",
      "X.shape:  408\n",
      "Variance explained:  [0.95382488]\n",
      "X.shape:  408\n",
      "Variance explained:  [0.95382488]\n",
      "X.shape:  416\n",
      "Variance explained:  [0.9547623]\n",
      "X.shape:  416\n",
      "Variance explained:  [0.9547623]\n",
      "X.shape:  416\n",
      "Variance explained:  [0.9547623]\n",
      "X.shape:  416\n",
      "Variance explained:  [0.9547623]\n",
      "X.shape:  416\n",
      "Variance explained:  [0.9547623]\n",
      "X.shape:  416\n",
      "Variance explained:  [0.9547623]\n",
      "X.shape:  416\n",
      "Variance explained:  [0.9547623]\n",
      "X.shape:  416\n",
      "Variance explained:  [0.9547623]\n",
      "X.shape:  416\n",
      "Variance explained:  [0.9547623]\n",
      "X.shape:  780\n",
      "Variance explained:  [0.94906032]\n",
      "X.shape:  780\n",
      "Variance explained:  [0.94906032]\n",
      "X.shape:  780\n",
      "Variance explained:  [0.94906032]\n",
      "X.shape:  780\n",
      "Variance explained:  [0.94906032]\n",
      "X.shape:  780\n",
      "Variance explained:  [0.94906032]\n",
      "X.shape:  780\n",
      "Variance explained:  [0.94906032]\n",
      "X.shape:  780\n",
      "Variance explained:  [0.94906032]\n",
      "X.shape:  780\n",
      "Variance explained:  [0.94906032]\n",
      "X.shape:  780\n",
      "Variance explained:  [0.94906032]\n",
      "X.shape:  676\n",
      "Variance explained:  [0.95643783]\n",
      "X.shape:  676\n",
      "Variance explained:  [0.95643783]\n",
      "X.shape:  676\n",
      "Variance explained:  [0.95643783]\n",
      "X.shape:  676\n",
      "Variance explained:  [0.95643783]\n",
      "X.shape:  676\n",
      "Variance explained:  [0.95643783]\n",
      "X.shape:  676\n",
      "Variance explained:  [0.95643783]\n",
      "X.shape:  676\n",
      "Variance explained:  [0.95643783]\n",
      "X.shape:  676\n",
      "Variance explained:  [0.95643783]\n",
      "X.shape:  676\n",
      "Variance explained:  [0.95643783]\n",
      "X.shape:  501\n",
      "Variance explained:  [0.97078002]\n",
      "X.shape:  501\n",
      "Variance explained:  [0.97078002]\n",
      "X.shape:  501\n",
      "Variance explained:  [0.97078002]\n",
      "X.shape:  501\n",
      "Variance explained:  [0.97078002]\n",
      "X.shape:  501\n",
      "Variance explained:  [0.97078002]\n",
      "X.shape:  501\n",
      "Variance explained:  [0.97078002]\n",
      "X.shape:  501\n",
      "Variance explained:  [0.97078002]\n",
      "X.shape:  501\n",
      "Variance explained:  [0.97078002]\n",
      "X.shape:  501\n",
      "Variance explained:  [0.97078002]\n",
      "X.shape:  452\n",
      "Variance explained:  [0.98250266]\n",
      "X.shape:  452\n",
      "Variance explained:  [0.98250266]\n",
      "X.shape:  452\n",
      "Variance explained:  [0.98250266]\n",
      "X.shape:  452\n",
      "Variance explained:  [0.98250266]\n",
      "X.shape:  452\n",
      "Variance explained:  [0.98250266]\n",
      "X.shape:  452\n",
      "Variance explained:  [0.98250266]\n",
      "X.shape:  452\n",
      "Variance explained:  [0.98250266]\n",
      "X.shape:  452\n",
      "Variance explained:  [0.98250266]\n",
      "X.shape:  452\n",
      "Variance explained:  [0.98250266]\n",
      "X.shape:  520\n",
      "Variance explained:  [0.96515659]\n",
      "X.shape:  520\n",
      "Variance explained:  [0.96515659]\n",
      "X.shape:  520\n",
      "Variance explained:  [0.96515659]\n",
      "X.shape:  520\n",
      "Variance explained:  [0.96515659]\n",
      "X.shape:  520\n",
      "Variance explained:  [0.96515659]\n",
      "X.shape:  520\n",
      "Variance explained:  [0.96515659]\n",
      "X.shape:  520\n",
      "Variance explained:  [0.96515659]\n",
      "X.shape:  520\n",
      "Variance explained:  [0.96515659]\n",
      "X.shape:  520\n",
      "Variance explained:  [0.96515659]\n",
      "X.shape:  521\n",
      "Variance explained:  [0.98064078]\n",
      "X.shape:  521\n",
      "Variance explained:  [0.98064078]\n",
      "X.shape:  521\n",
      "Variance explained:  [0.98064078]\n",
      "X.shape:  521\n",
      "Variance explained:  [0.98064078]\n",
      "X.shape:  521\n",
      "Variance explained:  [0.98064078]\n",
      "X.shape:  521\n",
      "Variance explained:  [0.98064078]\n",
      "X.shape:  521\n",
      "Variance explained:  [0.98064078]\n",
      "X.shape:  521\n",
      "Variance explained:  [0.98064078]\n",
      "X.shape:  521\n",
      "Variance explained:  [0.98064078]\n"
     ]
    }
   ],
   "source": [
    "n_cubes = [5, 10, 15]\n",
    "perc_overlap = [.35, .45, .55]\n",
    "\n",
    "mapper_output_dict = {}\n",
    "mapper_output_df = {}\n",
    "for key in df_stratum_season_time_dict:\n",
    "    for j in n_cubes:\n",
    "        for k in perc_overlap:\n",
    "            newkey = key+\"ncubes_\"+str(j)+\"_overlap_\"+str(k)\n",
    "            scomplex, X = mapper_pca_func(df_stratum_season_time_dict[key],key, N_CUBES = j, PERC_OVERLAP = k)\n",
    "            mapper_output_df[newkey] = X\n",
    "            mapper_output_dict[newkey] = scomplex"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "grateful-warner",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "90"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(mapper_pca_output_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "saving-introduction",
   "metadata": {},
   "source": [
    "Saving all outputs to json save scomplices and data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "confused-portal",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "jsonFile = json.dumps(mapper_output_dict)\n",
    "f = open(\"All_graphs.json\",\"w\")\n",
    "pickle = pickle.dump(mapper_output_df,path)\n",
    "f.write(jsonFile)\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "increasing-coffee",
   "metadata": {},
   "source": [
    "Load mapper data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "public-indonesian",
   "metadata": {},
   "outputs": [],
   "source": [
    "jsonFilePath = r\"All_graphs.json\"\n",
    "jsonFile = open(jsonFilePath,\"r\")\n",
    "data = json.load(jsonFile)\n",
    "jsonFile.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "handmade-sullivan",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "90"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(data.keys())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "recorded-interval",
   "metadata": {},
   "source": [
    "Pick one output to analyze"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "narrow-delhi",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stratum 2 SUMMER 01-13: ncubes_10_overlap_0.45\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "dict_keys(['nodes', 'links', 'simplices', 'meta_data', 'meta_nodes'])"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#key = list(data.keys())[0]\n",
    "key = \"Stratum 2 SUMMER 01-13: ncubes_10_overlap_0.45\"\n",
    "print(key)\n",
    "output = data[key]\n",
    "output.keys()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "hawaiian-karen",
   "metadata": {},
   "source": [
    "Function to return a nested list of connected components"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "id": "specialized-observer",
   "metadata": {},
   "outputs": [],
   "source": [
    "def connected_components(output):\n",
    "    simplices = output.get(\"simplices\")\n",
    "    # Pick out the edges\n",
    "    pairs = [item for item in simplices if len(item)==2]\n",
    "    # Pick out all nodes\n",
    "    nodes = output.get(\"nodes\").keys()\n",
    "    # Unpacks the list of lists to get every node that is connected to at least one other node with an edge\n",
    "    connected_nodes = [node for sublist in edges for node in sublist]\n",
    "    # Pick out the nodes that are its own connected component (not a connected node)\n",
    "    singles = [[node] for node in nodes if node not in connected_nodes]\n",
    "    \n",
    "    # Build connected components\n",
    "    components = []\n",
    "    for a, b in pairs:\n",
    "        for component in components:\n",
    "            if a in component:\n",
    "                for i, other_component in enumerate(components):\n",
    "                    if b in other_component and other_component != component: # a, and b are already in different components: merge\n",
    "                        component.extend(other_component)\n",
    "                        components[i:i+1] = []\n",
    "                        break # we don't have to look for other components for b\n",
    "                else: # b wasn't found in any other component\n",
    "                    if b not in component:\n",
    "                        component.append(b)\n",
    "                break # we don't have to look for other components for a\n",
    "            if b in component: # a wasn't in in the component \n",
    "                component.append(a)\n",
    "                break # we don't have to look further\n",
    "        else: # neither a nor b were found\n",
    "            components.append([a, b])\n",
    "            \n",
    "    # Add the singles into the components\n",
    "    print(\"Number of singles:\",len(singles))\n",
    "    components.extend(singles)\n",
    "    \n",
    "    # Sort the components by size\n",
    "    components.sort(reverse = True, key = len)\n",
    "    \n",
    "    return components"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "id": "orange-hebrew",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of singles: 15\n"
     ]
    }
   ],
   "source": [
    "components = connected_components(output)\n",
    "len(components)\n",
    "for component in components:\n",
    "    print(len(component))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "id": "brown-carry",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['cube0_cluster0',\n",
       "  'cube1_cluster0',\n",
       "  'cube2_cluster0',\n",
       "  'cube2_cluster9',\n",
       "  'cube3_cluster0',\n",
       "  'cube3_cluster5',\n",
       "  'cube3_cluster16',\n",
       "  'cube4_cluster4',\n",
       "  'cube5_cluster2'],\n",
       " ['cube3_cluster19',\n",
       "  'cube4_cluster14',\n",
       "  'cube3_cluster20',\n",
       "  'cube5_cluster7',\n",
       "  'cube6_cluster3'],\n",
       " ['cube5_cluster5',\n",
       "  'cube6_cluster2',\n",
       "  'cube7_cluster0',\n",
       "  'cube8_cluster0',\n",
       "  'cube9_cluster7'],\n",
       " ['cube4_cluster0', 'cube5_cluster0', 'cube6_cluster0', 'cube6_cluster1'],\n",
       " ['cube2_cluster17', 'cube3_cluster14', 'cube4_cluster12'],\n",
       " ['cube3_cluster11', 'cube4_cluster7', 'cube3_cluster12'],\n",
       " ['cube0_cluster1', 'cube1_cluster6'],\n",
       " ['cube0_cluster2', 'cube1_cluster11'],\n",
       " ['cube1_cluster1', 'cube2_cluster6'],\n",
       " ['cube1_cluster2', 'cube2_cluster7'],\n",
       " ['cube1_cluster3', 'cube2_cluster8'],\n",
       " ['cube1_cluster4', 'cube2_cluster11'],\n",
       " ['cube1_cluster5', 'cube2_cluster12'],\n",
       " ['cube1_cluster7', 'cube2_cluster13'],\n",
       " ['cube1_cluster8', 'cube2_cluster14'],\n",
       " ['cube1_cluster9', 'cube2_cluster15'],\n",
       " ['cube1_cluster10', 'cube2_cluster19'],\n",
       " ['cube2_cluster1', 'cube3_cluster1'],\n",
       " ['cube2_cluster3', 'cube3_cluster2'],\n",
       " ['cube2_cluster4', 'cube3_cluster6'],\n",
       " ['cube2_cluster5', 'cube3_cluster8'],\n",
       " ['cube2_cluster10', 'cube3_cluster13'],\n",
       " ['cube2_cluster16', 'cube3_cluster15'],\n",
       " ['cube3_cluster3', 'cube4_cluster1'],\n",
       " ['cube3_cluster4', 'cube4_cluster2'],\n",
       " ['cube3_cluster7', 'cube4_cluster3'],\n",
       " ['cube3_cluster9', 'cube4_cluster5'],\n",
       " ['cube3_cluster17', 'cube4_cluster16'],\n",
       " ['cube3_cluster18', 'cube4_cluster17'],\n",
       " ['cube4_cluster8', 'cube5_cluster1'],\n",
       " ['cube4_cluster10', 'cube5_cluster3'],\n",
       " ['cube4_cluster11', 'cube5_cluster4'],\n",
       " ['cube4_cluster13', 'cube5_cluster6'],\n",
       " ['cube4_cluster15', 'cube5_cluster8'],\n",
       " ['cube4_cluster18', 'cube5_cluster9'],\n",
       " ['cube8_cluster1', 'cube9_cluster0'],\n",
       " ['cube8_cluster2', 'cube9_cluster5'],\n",
       " ['cube0_cluster3'],\n",
       " ['cube0_cluster4'],\n",
       " ['cube0_cluster5'],\n",
       " ['cube0_cluster6'],\n",
       " ['cube2_cluster2'],\n",
       " ['cube2_cluster18'],\n",
       " ['cube3_cluster10'],\n",
       " ['cube4_cluster6'],\n",
       " ['cube4_cluster9'],\n",
       " ['cube5_cluster10'],\n",
       " ['cube9_cluster1'],\n",
       " ['cube9_cluster2'],\n",
       " ['cube9_cluster3'],\n",
       " ['cube9_cluster4'],\n",
       " ['cube9_cluster6']]"
      ]
     },
     "execution_count": 130,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "components"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "protecting-shore",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
