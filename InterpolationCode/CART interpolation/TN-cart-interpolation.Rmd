---
title: "TN-cart-interpolation"
author: "Alaina Stockdill"
data: "6/24/21"
output: pdf_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE, message = FALSE, warning = FALSE)
```

```{r}
library(tidyverse)
library(lubridate)
library(stringr)
library(caret)
library(rattle)
library(rpart)
```
# Predicting TN values with CART regression trees

The purpose of this program is to interpolate missing TN values in our data set with CART regression trees. The benefit of the CART regression tree is that they will automatically choose the most important variables in predicting TN and are able to handle missing data very easily. With a data set that contains a large amount of incomplete rows, it seems that this method will be helpful in getting around this issue.

The data set that we are uploading has already been filtered for bad QF codes, negative TP and TN values, and for surface samples in the LTRM water quality data set. SHEETBAR codes that have multiple rows have already been combined so that each possible bar code only has on one row of data. 


```{r}
setwd("/Users/alainastockdill/UMR-TDA-2021/InterpolationCode/CART interpolation")
water <- read.csv(file = "../../LTRM data/water_data_qfneg.csv")

# Sets the maximum number of nested expressions to be evaluated
options(expressions = 5e5)
```


```{r}
# Add in the date, year, and season
water <- water %>% 
  mutate(nice_date = mdy(DATE),
         year = year(nice_date),
         season = quarter(nice_date, fiscal_start = 3)) %>%
  select(-SHEETBAR, -nice_date, -DATE, -LOCATCD)

# Make the FLDNUM and STRATUM categorical variables
water$FLDNUM <- as.character(water$FLDNUM)
water$STRATUM <- as.character(water$STRATUM)
water$season <- as.character(water$season)

# Set pred_col to the columns that we want to use as predictors
predictor_vars <- c("TP", "TN", "TURB", "COND", "VEL", "WDP", "season", "STRATUM", "FLDNUM", "CHLcal", "SS", "TEMP")

data <- water[predictor_vars]

```

Here are several different data sets that will serve when necessary
```{r}
###  COMPLETE DATA SET ###
# Create a data set that takes out the NA values
comp_data = data %>% 
  filter_at(vars(names(data)), all_vars(!is.na(.)))

### COMPLETE TN COLUMN ###
# Create a new data set that removes only the na TN values
comp_TN_data = data %>%
  filter_at(vars(TN), any_vars(!is.na(.)))

### REMOVE TN OUTLIERS ###
rm_out_TN_data = data %>%
  filter_at(vars(TN), any_vars(!is.na(.)))
rm_out_TN_data = rm_out_TN_data %>%
  filter(TN < 100)
```

## caret rpart2 model for all years and seasons
Caret cannot have any have any NA values in the data set that it creates the tree on.
This will be used just a reference since for our purposes, we will being using the rpart package. 
```{r}
### CARET MODEL ###
# Runs the caret rpart model with the complete data set
# Runs for all years and all seasons
set.seed(4321)
fit_control <- caret::trainControl(method = "cv", number = 10)
tr.TN_caret <- caret::train(TN ~ .,
                      data = (comp_data %>% sample_frac(0.5)),
                      method = "rpart2",
                      trControl = fit_control,
                      tuneGrid = data.frame(maxdepth = (1:20)))

fancyRpartPlot(tr.TN_caret$finalModel, main = "TN all years")
```

```{r}
plot(tr.TN_caret, main = "TN all years")
```

## rpart model with all years and seasons
Using the rpart library allows us to create a tree with the entire data set.
If a value is missing, a surrogate value will be used.
```{r}
### RPART MODEL ###
# Runs on the entire data set - even with na values in all columns
set.seed(4321)
tr.TN_rpart <- rpart(TN ~ .,
                    data = (data %>% sample_frac(0.5)),
                    method = "anova",
                    control = (maxdepth = 20))
fancyRpartPlot(tr.TN_rpart, main = "TN prediction for all years")
```


```{r}
# Tree evaluation summary 
plotcp(tr.TN_rpart)
printcp(tr.TN_rpart)
rsq.rpart(tr.TN_rpart)
summary(tr.TN_rpart)
```

#### GLM model for predicting TN
The purpose of the GLM model is to get a base performance level for the data set that we are creating a rpart regression tree on in the next step. This will help us to compare the accuracy of the each model.

```{r}
sample_size = 0.80 * nrow(comp_TN_data)
set.seed(571)
train_indices <- sample(seq_len(nrow(comp_TN_data)), size = sample_size)

data_train <- comp_TN_data[train_indices, ]
data_test <- comp_TN_data[-train_indices, ]

# Generalized linear model 
TN_glm <- glm(TN ~ .,
              data = data_train)


# Use the model to predict TN on the test data
data_test$PREDICTED <- predict(TN_glm, data_test)

# Plot actual v. predicted TN for the test data
ggplot(data_test, mapping = aes(TN, PREDICTED)) +
  geom_point(alpha = .1) +
  coord_cartesian(xlim = c(0, 20))

data_test <- data_test %>% filter(!is.na(PREDICTED))


RMSE(data_test$TN, data_test$PREDICTED)
mae(data_test$TN, data_test$PREDICTED)
cor(data_test$TN, data_test$PREDICTED)

```

### Run with test and training data
Create a tree model for all years using the data set that has empty TN values removed. This will be beneficial for testing the effectiveness of the model because we will be able to create a model on the training set, test it on the test data, and then compare the actual versus predicted TN values for 

#### Run using comp_tn_data: a complete TN row

```{r}
sample_size = 0.80 * nrow(comp_TN_data)
set.seed(571)
train_indices <- sample(seq_len(nrow(comp_TN_data)), size = sample_size)

data_train <- comp_TN_data[train_indices, ]
data_test <- comp_TN_data[-train_indices, ]

set.seed(4321)
tr.TN_rpart1 <- rpart(TN ~ .,
                    data = data_train,
                    method = "anova",
                    control = (maxdepth = 20))
fancyRpartPlot(tr.TN_rpart, main = "TN prediction for all years")


# Test the model on both the training and testing data
data_train$PREDICTED <- predict(tr.TN_rpart, data_train)
data_test$PREDICTED <- predict(tr.TN_rpart, data_test)

# Plots for actual v. predicted TN values in both the training and testing data
ggplot(data_train, mapping = aes(TN, PREDICTED)) +
  geom_point(alpha = 0.1)

train_lm <- lm(PREDICTED ~ TN, data = data_train)

ggplot(data_test, mapping = aes(TN, PREDICTED)) +
  geom_point(alpha = 0.1)

test_lm <- lm(PREDICTED ~ TN, data = data_test)

# Get the RMSE and correlation coefficient for the test data
MAE(data_test$TN, data_test$PREDICTED)
RMSE(data_test$TN, data_test$PREDICTED)
mean((data_test$TN - data_test$PREDICTED)^2)
cor(data_test$TN, data_test$PREDICTED)
summary(test_lm)$r.squared
```

#### Run with the original TN column in the data set
```{r}
sample_size = 0.80 * nrow(data)
set.seed(7362)
train_indices <- sample(seq_len(nrow(data)), size = sample_size)

data_train <- data[train_indices, ]
data_test <- data[-train_indices, ]

set.seed(2847)
tr.TN_rpart2 <- rpart(TN ~ .,
                    data = data_train,
                    method = "anova",
                    control = (maxdepth = 20))
fancyRpartPlot(tr.TN_rpart, main = "TN prediction for all years")


# Test the model on both the training and testing data
data_train$PREDICTED <- predict(tr.TN_rpart, data_train)
data_test$PREDICTED <- predict(tr.TN_rpart, data_test)

# Plots for actual v. predicted TN values in both the training and testing data
ggplot(data_train, mapping = aes(TN, PREDICTED)) +
  geom_point(alpha = 0.1)

train_lm <- lm(PREDICTED ~ TN, data = data_train)

ggplot(data_test, mapping = aes(TN, PREDICTED)) +
  geom_point(alpha = 0.1)

test_lm <- lm(PREDICTED ~ TN, data = data_test)

# Get the RMSE and correlation coefficient for the test data
MAE(data_test$TN, data_test$PREDICTED, na.rm = TRUE)
RMSE(data_test$TN, data_test$PREDICTED, na.rm = TRUE)
mean((data_test$TN - data_test$PREDICTED)^2, na.rm = TRUE)
summary(test_lm)$r.squared
```


#### Run without outliers and train and test
To check our model, we will look at what happens when we remove the outliers from the data set. This data is different from before in that there are na TN values
```{r}
sample_size = 0.80 * nrow(rm_out_TN_data)
set.seed(571)
train_indices <- sample(seq_len(nrow(rm_out_TN_data)), size = sample_size)

data_train <- rm_out_TN_data[train_indices, ]
data_test <- rm_out_TN_data[-train_indices, ]

set.seed(4321)
tr.TN_rpart <- rpart(TN ~.,
                    data = data_train,
                    method = "anova",
                    control = (maxdepth = 20))
fancyRpartPlot(tr.TN_rpart, main = "TN prediction for all years")

# test the model on the training data
data_train$PREDICTED <- predict(tr.TN_rpart, data_train)

# Plots for actual v. predicted TN values in the training data
ggplot(data_train, mapping = aes(TN, PREDICTED)) +
  geom_point(alpha = 0.1)

# Summary stats and plots
rsq.rpart(tr.TN_rpart) 
RMSE(data_train$TN, data_train$PREDICTED)

```

## Predict TN by splitting into different years and seasons using caret
```{r}
# Functions for splitting into years and seasons - uses caret::train
# Split by year and season
make_year_tuples <- function(index, year_partition) {
  return(c(year_partition[index], year_partition[index + 1]))
}

# Creates trees by groups of years 
tree_by_years <- function(year_tuple, water_data) {
  water_data <- water_data %>% filter(year >= year_tuple[1] &
                                      year <= year_tuple[2])
  
  fit_control <- caret::trainControl(method = "cv")
  
  tr.TN <- caret::train(TN ~ .,
                       data = water_data,
                       method = "rpart2",
                       trControl = fit_control,
                       tuneGrid = data.frame(maxdepth = (1:20)))
  
  return(tr.TN)
}

# Create trees by season
tree_by_season <- function(season, min_year, max_year, year_interval, water_data) {
  year_partition <- seq(min_year, max_year, year_interval)
  
  year_tuples <- lapply(1: (length(year_partition) - 1), make_year_tuples, year_partition)
  
  water_data <- water_data %>% filter(season == season)
  
  tree_models <- lapply(year_tuples, tree_by_years, water_data)
  
  return(tree_models)

}
```

## Predict TN with year and season split - RPART
```{r}
# Functions for splitting into years and seasons - using rplot: can have nas
# Split by year and season
make_year_tuples <- function(index, year_partition) {
  return(c(year_partition[index], year_partition[index + 1]))
}

# Creates trees by groups of years 
tree_by_years_rpart <- function(year_tuple, water_data) {
  water_data <- water_data %>% filter(year >= year_tuple[1] &
                                      year <= year_tuple[2])
  
  tr.TN_rpart <- rpart(TN ~.,
                    data = (water_data %>% sample_frac(0.5)),
                    method = "anova",
                    control = (maxdepth = 20))
  
  fancyRpartPlot(tr.TN_rpart)
  
  return(tr.TN_rpart)
}

# Create trees by season
tree_by_season_rpart<- function(season, min_year, max_year, year_interval, water_data) {
  year_partition <- seq(min_year, max_year, year_interval)
  
  year_tuples <- lapply(1: (length(year_partition) - 1), make_year_tuples, year_partition)
  
  water_data <- water_data %>% filter(season == season)
  
  #tree_models <- lapply(year_tuples, tree_by_years_rpart, water_data)
  
  return(water_data)
}

```

```{r}
# Test the accuracy of the model with a train and test set
train_test_rpart <- function(model, data, pred_col) {
  sample_size = 0.80 * nrow()
  set.seed(7362)
  train_indices <- sample(seq_len(nrow(data)), size = sample_size)
  
  data_train <- data_train[train_indices, ]
  data_test <- data[-train_indices, ]
  
  set.seed(2847)
  tr.TN_rpart2 <- rpart(TN ~ pred_col,
                      data = data_train,
                      method = "anova",
                      control = (maxdepth = 20))
  fancyRpartPlot(tr.TN_rpart, main = "TN prediction for all years")
  
  
  # Test the model on both the training and testing data
  data_train$PREDICTED <- predict(tr.TN_rpart, data_train)
  data_test$PREDICTED <- predict(tr.TN_rpart, data_test)
  
  # Plots for actual v. predicted TN values in both the training and testing data
  ggplot(data_train, mapping = aes(TN, PREDICTED)) +
    geom_point(alpha = 0.1)
  
  ggplot(data_test, mapping = aes(TN, PREDICTED)) +
    geom_point(alpha = 0.1)
  
  # Get the RMSE and correlation coefficient for the test data
  mae(data_test$TN, data_test$PREDICTED)
  RMSE(data_test$TN, data_test$PREDICTED)
  mse(data_test$TN, data_test$PREDICTED)
  cor(data_test$TN, data_test$PREDICTED)
}

```


### Run the trees by seasons and print the trees and the RMSE plots
```{r} 
#Why is this in here?
#comp_TN_data <- subset(comp_TN_data, select = -season)

partition <- seq(2000, 2020, 5)
tuples <- lapply(1: length(partition) -1, make_year_tuples, partition)

tuples[1]
tuples[2]
tuples[3]

```

#### Spring

```{r}
# Split into groups of 7 years
trees.sp.1992.2020 <- tree_by_season_rpart(1, 1992, 2020, 7, comp_TN_data)
```


```{r}
# Print the trees
lapply(1:4, function(x) fancyRpartPlot(trees.sp.1992.2020[[x]],
                                               main = paste(as.character(x)) ))
```

```{r}
# Print out summary plots and stats
lapply(1:4, function(x) rsq.rpart(trees.sp.1992.2020[[x]]))
lapply(1:4, function(x) plotcp(trees.sp.1992.2020[[x]]))
lapply(1:4, function(x) summary(trees.sp.1992.2020[[x]]))

# Need to check if this will work
lapply(1:4, function(x) return(train_test_rpart(trees.sp.1992.2020[[x]], comp_TN_data, pred_col)))
```


#### Summer
```{r}
trees.su.1992.2020 <- tree_by_season_rpart(2, 1992, 2020, 7, comp_TN_data)

```

```{r}
lapply(1:4, function(x) fancyRpartPlot(trees.su.1992.2020[[x]],
                                               main = paste(as.character(x)) ))
```

```{r}
# Print out summary plots and stats
lapply(1:4, function(x) rsq.rpart(trees.su.1992.2020[[x]]))
lapply(1:4, function(x) plotcp(trees.su.1992.2020[[x]]))
lapply(1:4, function(x) summary(trees.su.1992.2020[[x]]))
```


#### Fall

```{r}
trees.fa.1992.2020 <- tree_by_season_rpart(3, 1992, 2020, 7, data)

```

```{r}
lapply(1:4, function(x) fancyRpartPlot(trees.fa.1992.2020[[x]],
                                               main = paste(as.character(x)) ))
```

```{r}
# Print out summary plots and stats
lapply(1:4, function(x) rsq.rpart(trees.fa.1992.2020[[x]]))
lapply(1:4, function(x) plotcp(trees.fa.1992.2020[[x]]))
lapply(1:4, function(x) summary(trees.fa.1992.2020[[x]]))
```

#### Winter

```{r}
trees.wi.1992.2020 <- tree_by_season_rpart(4, 1992, 2020, 7, data)

```

```{r}
lapply(1:4, function(x) fancyRpartPlot(trees.wi.1992.2020[[x]],
                                               main = paste(as.character(x)) ))
```

```{r}
# Print out summary plots and stats
lapply(1:4, function(x) rsq.rpart(trees.wi.1992.2020[[x]]))
lapply(1:4, function(x) plotcp(trees.wi.1992.2020[[x]]))
lapply(1:4, function(x) summary(trees.wi.1992.2020[[x]]))
```

## Filtering by a FIELDNUM 

```{r}
### RPART MODEL ###
# Runs on the entire data set - even with na values in all columns
water4 <- data %>% filter(FLDNUM == "1")
set.seed(4321)
water4_rpart <- rpart(TN ~.,
                    data = (water4 %>% sample_frac(0.7)),
                    method = "anova",
                    control = (maxdepth = 20))
fancyRpartPlot(water4_rpart, main = "TN prediction for pool 4 - all years")
```
With training and testing
```{r}
sample_size = 0.80 * nrow(water4)
set.seed(571)
train_indices <- sample(seq_len(nrow(water4)), size = sample_size)

data_train <- water4[train_indices, ]
data_test <- water4[-train_indices, ]

set.seed(4321)
water4_rpart2 <- rpart(TN ~.,
                    data = data_train,
                    method = "anova",
                    control = (maxdepth = 20))
fancyRpartPlot(water4_rpart2, main = "TN prediction in pool 4 - all years")


# Test the model on both the training and testing data
data_train$PREDICTED <- predict(water4_rpart2, data_train)
data_test$PREDICTED <- predict(water4_rpart2, data_test)

# Plots for actual v. predicted TN values in both the training and testing data
ggplot(data_train, mapping = aes(TN, PREDICTED)) +
  geom_point(alpha = 0.1)

train_lm <- lm(PREDICTED ~ TN, data = data_train)

ggplot(data_test, mapping = aes(TN, PREDICTED)) +
  geom_point(alpha = 0.1)

test_lm <- lm(PREDICTED ~ TN, data = data_test)

# Get the RMSE and correlation coefficient for the test data
MAE(data_test$TN, data_test$PREDICTED, na.rm = TRUE)
RMSE(data_test$TN, data_test$PREDICTED, na.rm = TRUE)
mean((data_test$TN - data_test$PREDICTED)^2, na.rm = TRUE)
summary(test_lm)$r.squared

```



