---
title: "CART and RF Interpolation"
author: "Amber Lee"
date: "6/22/2021"
output: pdf_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE, message = FALSE)
```

# Introduction

Initial implementation of regression tree, random forest, and polynomial interpolation for the 11 continuous water quality variables in our dataset. 

### Load libraries 
```{r}
library(tidyverse)
library(stringr)
library(lubridate)

library(rpart) # for regression tree
library(rpart.plot)
library(caret) # for other models
library(rattle)
library(kableExtra)
library(broom)
```

### Read data

```{r}
water20 <- read.csv("../../LTRM data/water_data_qfneg.csv", header = TRUE)
```

### Scope of interpolation (exploring missingness)

```{r}
var_missing_rate <- sapply(water20, function(x) sum(is.na(x))/length(x))

var_missing_rate <- data.frame(var = names(var_missing_rate), 
                               missing_rate = unname(var_missing_rate))

var_missing_rate %>% 
  mutate(`missing rate` = round(missing_rate, digits = 3)) %>% 
  select(-missing_rate) %>%
  arrange(-`missing rate`) %>%
  kbl(booktabs = T)
```

```{r}

var_missing_rate %>% filter(missing_rate > 0) %>%
  ggplot(aes(x = var, y = missing_rate)) +
  geom_bar(stat = "identity") +
  coord_flip()

```

### Data cleaning

* Add year and season variable

* Change `FLDNUM` to be categorical (character).



```{r}

water20 <- water20 %>%
  mutate(nice_date = mdy(DATE),
         year = year(nice_date),
         quarter = quarter(nice_date, fiscal_start = 3),
         FLDNUM = case_when(FLDNUM == 1 ~ "Lake City, MN", 
                            FLDNUM == 2 ~ "Onalaska, WI",
                            FLDNUM == 3 ~ "Bellevue, IA",
                            FLDNUM == 4 ~ "Brighton, IL",
                            FLDNUM == 5 ~ "Jackson, MO",
                            FLDNUM == 6 ~ "Havana, IL"),
         FLDNUM = as.factor(FLDNUM),
         STRATUM = as.factor(STRATUM)) %>% # CART can split on categorical variable if
                                         # encoded as a factor variable
  select(-SHEETBAR, -nice_date, -DATE, -LOCATCD) 

```

# Regression trees

### About RT

## `TP`



http://st47s.com/Math154/Notes/class.html#r-cart-example

```{r}
set.seed(4747)

# train and test data created from rows with existing TP values
fullTP <- water20 %>% filter(!is.na(TP))

# this gives me a total of ten 80/20 splits
train_idx <- createDataPartition(fullTP$TP, p = .8,list = FALSE)

TPtrain1 <- fullTP[train_idx,]
TPtest1 <- fullTP[-train_idx,]


```

There are:

* 82,000 samples in the filtered water dataset `water20`

* 31,000 samples in the training data `fullTP`. This is because the testing and training data both need existing `TP` values

* 25,000 samples in the `TPtrain1`

* and 6,000 samples in the `TPtest1`

### using the `rpart` library

https://www.statmethods.net/advstats/cart.html 

https://rstudio-pubs-static.s3.amazonaws.com/27179_e64f0de316fc4f169d6ca300f18ee2aa.html



Use `rpart` because there is surrogates

```{r}

tr.TP <- rpart(TP ~ .,
               data = TPtrain1,
               control = rpart.control(xval = 10,
                                       minsplit = 100)) 

printcp(tr.TP)

png("tree_TP.png")
fancyRpartPlot(tr.TP)
dev.off()

png("tree_TP_CV.png")
plotcp(tr.TP)
dev.off()

fancyRpartPlot(tr.TP)
plotcp(tr.TP)


```

```{r}

printcp(tr.TP)

```

### Pruning a tree with the "1 SE rule"

https://stats.stackexchange.com/questions/13471/how-to-choose-the-number-of-splits-in-rpart

The convention is to use the best tree (lowest cross-validate relative error) or the smallest (simplest) tree within one standard error of the best tree.

https://www.mayo.edu/research/documents/rpartminipdf/doc-10027257

section 6.1.2 and 6.1.3

The tree with the lowest CV error has 10 splits with relative error `0.57869` and standard error `0.067620`. We prune to the smallest tree (least splits) within one standard error of the lowest CV tree's error, which is `0.57869 + 0.067620 = 0.646`. So we choose a tree with 6 splits because it has a relative error of `0.63 < 0.646`. To choose this tree, we set `cp = 0.02`. (We just have to choose a `cp` between 0.028508, which has 5 splits, and 0.018834, which has 6 splits.) 


```{r}
# tidy(tr.TP) TODO
```

```{r}
# https://rstudio-pubs-static.s3.amazonaws.com/27179_e64f0de316fc4f169d6ca300f18ee2aa.html
# this author uses the lowest CV error to choose the cp. we don't use this message

# use 1-SE rule 

tr.TP.prune <- prune(tr.TP, cp = 0.02)

fancyRpartPlot(tr.TP.prune)
```

??

https://datascience.stackexchange.com/questions/31346/caret-and-rpart-does-caret-automatically-prune-rpart-trees

http://www.rdatamining.com/docs/regression-and-classification-with-r

## Evalute `TP` prediction

```{r}

TPtrain1$TP.PREDICT <- predict(tr.TP, data = TPtrain1)

TPtrain1 <- TPtrain1 %>%
  mutate(TP.SQ.ERROR = (TP - TP.PREDICT)^2)

print("RSME is")
sqrt(sum(TPtrain1$TP.SQ.ERROR)/dim(TPtrain1)[1])

TPtrain1 %>% 
  ggplot(aes(x = TP, y = TP.PREDICT)) +
  geom_point(alpha = 0.2) +
  coord_equal() 

ggsave("tree_training_predictvsactual.png")

# plot distribution of residuals 
TPtrain1 %>% 
  ggplot(aes(x = TP.SQ.ERROR)) +
  geom_histogram(bins = 100)

```

```{r}

TPtest1$TP.PREDICT <- predict(tr.TP.prune, TPtest1)
  
TPtest1 <- TPtest1 %>%
  mutate(TP.SQ.ERROR = (TP - TP.PREDICT)^2)

print("RSME is")
sqrt(sum(TPtest1$TP.SQ.ERROR)/dim(TPtest1)[1])

TPtest1 %>% 
  ggplot(aes(x = TP, y = TP.PREDICT)) +
  geom_point(alpha = 0.2) +
  coord_equal() +
  theme(aspect.ratio = 1) + xlim(0, 2) + ylim(0, 2)

ggsave("tree_test_predictvsactual.png")

```

## `TN`

Predicting TN is a lot harder...

```{r}

fullTN <- water20 %>%
  filter(!is.na(TN))

train_idx <- createDataPartition(fullTN$TN, p = .8,list = FALSE)

TNtrain1 <- fullTN[train_idx,]
TNtest1 <- fullTN[-train_idx,]

tr.TN <- rpart(TN ~ .,
               data = TNtrain1,
               control = rpart.control(xval = 10,
                                       minsplit = 100,
                                       cp = 0.001)) 

printcp(tr.TN)

png("tree_TN.png")
fancyRpartPlot(tr.TN)
dev.off()

png("tree_TN_CV.png")
plotcp(tr.TN)
dev.off()

fancyRpartPlot(tr.TN)
plotcp(tr.TN)

```

```{r}

tr.TN.prune <- prune(tr.TN, cp = 0.01)

TNtrain1$TN.PREDICT <- predict(tr.TN.prune, data = TNtrain1)
  
TNtrain1 <- TNtrain1 %>%
  mutate(TN.SQ.ERROR = (TN - TN.PREDICT)^2)

print("RSME is")
sqrt(sum(TNtrain1$TN.SQ.ERROR)/dim(TNtrain1)[1])

TNtrain1 %>% 
  ggplot(aes(x = TN, y = TN.PREDICT)) +
  geom_point(alpha = 0.2)

ggsave("TN_tree_test_predictvsactual.png")

TNtest1$TN.PREDICT <- predict(tr.TN.prune, data = TNtest1)
  
TPtest1 <- TPtest1 %>%
  mutate(TP.SQ.ERROR = (TP - TP.PREDICT)^2)

print("RSME is")
sqrt(sum(TPtest1$TP.SQ.ERROR)/dim(TPtest1)[1])

TPtest1 %>% 
  ggplot(aes(x = TP, y = TP.PREDICT)) +
  geom_point(alpha = 0.2) +
  coord_equal() +
  theme(aspect.ratio = 1) + xlim(0, 2) + ylim(0, 2)

ggsave("tree_test_predictvsactual.png")

```



# Old stuff

#### multivariate linear regression

```{r}

watertrain2 <- fullTP[train_idx[,2],]
watertest2 <- fullTP[-train_idx[,2],]

# remove all na's because that's how GLM works
watertrain2 <- watertrain2 %>%
  filter_all(all_vars(!is.na(.)))
watertest2 <- watertest2 %>%
  filter_all(all_vars(!is.na(.)))

linreg <- glm(TP ~ ., data = watertrain2)

summary(linreg)

tidy(linreg) %>% 
  filter(p.value < 0.05) %>%
  mutate(estimate = round(estimate, digits = 3),
         p.value = round(p.value, digits = 3)) %>%
  select(term, estimate, p.value) %>%
  kbl(booktabs = T)

tidy(linreg) %>% 
  filter(p.value >= 0.05) %>%
  mutate(estimate = round(estimate, digits = 3),
         p.value = round(p.value, digits = 3)) %>%
  select(term, estimate, p.value) %>%
  kbl(booktabs = T)

# watertrain2.selected <- fullTP[train_idx[,2],] %>%
#   select(all_of(c("TEMP", "DO", "TURB", "COND", "FLDNUM", 
#                   "CHLcal", "quarter"))) %>%
#   filter_all(all_vars(!is.na(.))) 
# TODO remove the missing values here and try to run the regression again, see if it applies to more values

```

```{r}
sum(fitted(linreg) == predict(linreg))
```


```{r}
watertrain2$TP.PREDICT <- predict(linreg)

watertrain2 <- watertrain2 %>%
  mutate(TP.SQ.ERROR = (TP - TP.PREDICT)^2)

print("RSME is")
sqrt(sum(watertrain2$TP.SQ.ERROR)/dim(watertrain2)[1])

watertrain2 %>% 
  ggplot(aes(x = TP, y = TP.PREDICT)) +
  geom_point(alpha = 0.2) +
  coord_equal() +
  theme(aspect.ratio = 1) + xlim(0, 2) + ylim(0, 2)

ggsave("glm_train_predictvsactual.png")

```


```{r}

watertest2$TP.PREDICT <- predict(linreg, watertest2)
  
watertest2 <- watertest2 %>%
  mutate(TP.SQ.ERROR = (TP - TP.PREDICT)^2)

print("RSME is")
sqrt(sum(watertest2$TP.SQ.ERROR)/dim(watertest2)[1])

watertest2 %>% 
  ggplot(aes(x = TP, y = TP.PREDICT)) +
  geom_point(alpha = 0.2) +
  coord_equal() +
  theme(aspect.ratio = 1) + xlim(0, 2) + ylim(0, 2)

ggsave("glm_test_predictvsactual.png")

```



### TP by year and season

```{r, eval = F}

make_year_touples <- function(index, year_partition){
  # index goes from 1 to length(year_partition) - 1
  
  return(c(year_partition[index], year_partition[index+1]))
  
}

tree_by_years <- function(year_touple, water_data){
  
  # filter for specific group of years
  water_data <- water_data %>% filter(year >= year_touple[1] &
                                        year <= year_touple[2])
  
  fitControl <- caret::trainControl(method="cv")

  tr.TP <- caret::train(TP ~ ., 
                        data = water_data,
                        # what is method?
                        method = "rpart2", 
                        trControl = fitControl, 
                        # don't quite understand maxdepth
                        tuneGrid = data.frame(maxdepth=1:20))
  
  return(tr.TP)
  
}

tree_by_season <- function(season, min_year, max_year, year_interval, water_data){
  # season can be 1, 2, 3, 4 with 1 being spring
  ## this is already processed in line 70
  
  year_partition <- seq(min_year, max_year, year_interval)
  # make a list of each year interval
  year_touples <- lapply(1:(length(year_partition)-1), make_year_touples, year_partition)
  
  water_data <- water_data %>% filter(quarter == season)
  
  tree_models <- lapply(year_touples, tree_by_years, water_data)
  
  return(tree_models)
  
}

```

\newpage

#### TP spring

```{r, eval = F}
 

# model for spring, 2000-2005, ..., 2015-2020
trees.sp.2000.2020 <- tree_by_season(1, # 1 stands for spring
                                     2000, # minimum year
                                     2020, # maximum year
                                     5, # 5 year intervals
                                     narm_water20) # dataset

# the 4 comes from (2020 - 2000) / 5
lapply(1:4, function(x) return(fancyRpartPlot(trees.sp.2000.2020[[x]]$finalModel,
                                              main = paste(as.character(x)) )))

lapply(1:4, function(x) return(plot(trees.sp.2000.2020[[x]], 
                                    main = paste(as.character(x)) )))


```
\newpage

#### TP summer

```{r, eval = F}

# model for summer, 2000-2020
trees.su.2000.2020 <- tree_by_season(2, 2000, 2020, 5, narm_water20)

# the 4 comes from (2020 - 2000) / 5
lapply(1:4, function(x) return(fancyRpartPlot(trees.su.2000.2020[[x]]$finalModel,
                                              main = paste(as.character(x)) )))

lapply(1:4, function(x) return(plot(trees.su.2000.2020[[x]], 
                                    main = paste(as.character(x)) )))

```

\newpage

#### TP fall

```{r, eval = F}

# model for fall, 2000-2020
trees.fa.2000.2020 <- tree_by_season(3, 2000, 2020, 5, narm_water20)

# the *2 is to insert line breaks
lapply(1:4*2, function(x)
  if (x %% 2 == 1) { # if x is even
    asis_output("\\\\[10cm]")
    } else {
      return(fancyRpartPlot(trees.fa.2000.2020[[x/2]]$finalModel,
                            main = paste(as.character(x/2)) ))
    } )

lapply(1:4, function(x) return(plot(trees.fa.2000.2020[[x]],
                                    main = paste(as.character(x)) )))

```

\newpage 

#### TP winter

```{r, eval = F}

# model for winter, 2000-2020
trees.wi.2000.2020 <- tree_by_season(4, 2000, 2020, 5, narm_water20)

lapply(1:4, function(x) return(fancyRpartPlot(trees.wi.2000.2020[[x]]$finalModel,
                                              main = paste(as.character(x)) )))

lapply(1:4, function(x) return(plot(trees.wi.2000.2020[[x]], 
                                    main = paste(as.character(x)) )))

```

