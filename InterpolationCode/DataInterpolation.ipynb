{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "handed-heating",
   "metadata": {},
   "source": [
    "# Imports\n",
    "This the script for prepossing the data to create data ready for the TDA mapper algorithm."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "completed-encoding",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imports done\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import pandas as pd\n",
    "from geopy import distance\n",
    "import math\n",
    "pd.set_option('display.max_columns', None)\n",
    "import numpy as np\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.preprocessing import RobustScaler,StandardScaler\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "print(\"imports done\")\n",
    "pd.set_option(\"display.max_rows\", None)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "champion-nudist",
   "metadata": {},
   "source": [
    "# File\n",
    "From the resulting `R` script written by Amber and `python` script written by Alaina, grab the cleaned data file. The `R` file can be found in the github repository and is called `water cleaning data.Rmd` and is in the WaterCleaning folder. Secondly, run it through the `python` script called `Data_Collapse.ipynb`. From here, this will result in the `cleaned_data.csv` file. This can found in the github repo as well. Download it, and edit the file path if necessary to read in the dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "handed-photograph",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dataFrame Made\n"
     ]
    }
   ],
   "source": [
    "filePath = r\"..\\LTRM data\\water_data_qfneg.csv\"\n",
    "dataFrame = pd.read_csv(filePath, low_memory = False)\n",
    "print(\"dataFrame Made\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "thousand-letter",
   "metadata": {},
   "source": [
    "# Filter for your pool\n",
    "Resets the index as well. This is done through the `FLDNUM` parameter, and can be found in accompanying documentation for the correct pool number. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "national-cleanup",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(11449, 18)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>SHEETBAR</th>\n",
       "      <th>DATE</th>\n",
       "      <th>LATITUDE</th>\n",
       "      <th>LONGITUDE</th>\n",
       "      <th>FLDNUM</th>\n",
       "      <th>STRATUM</th>\n",
       "      <th>LOCATCD</th>\n",
       "      <th>TN</th>\n",
       "      <th>TP</th>\n",
       "      <th>TEMP</th>\n",
       "      <th>DO</th>\n",
       "      <th>TURB</th>\n",
       "      <th>COND</th>\n",
       "      <th>VEL</th>\n",
       "      <th>SS</th>\n",
       "      <th>WDP</th>\n",
       "      <th>CHLcal</th>\n",
       "      <th>SECCHI</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>44000159</td>\n",
       "      <td>10/19/1993</td>\n",
       "      <td>38.873994</td>\n",
       "      <td>-90.189663</td>\n",
       "      <td>4</td>\n",
       "      <td>5</td>\n",
       "      <td>9343058</td>\n",
       "      <td>2.769</td>\n",
       "      <td>0.145</td>\n",
       "      <td>15.7</td>\n",
       "      <td>8.5</td>\n",
       "      <td>37.0</td>\n",
       "      <td>424.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>40.3</td>\n",
       "      <td>0.5</td>\n",
       "      <td>24.29702</td>\n",
       "      <td>27.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>44000160</td>\n",
       "      <td>10/19/1993</td>\n",
       "      <td>38.873772</td>\n",
       "      <td>-90.180452</td>\n",
       "      <td>4</td>\n",
       "      <td>5</td>\n",
       "      <td>9343059</td>\n",
       "      <td>3.049</td>\n",
       "      <td>0.146</td>\n",
       "      <td>15.7</td>\n",
       "      <td>8.3</td>\n",
       "      <td>43.0</td>\n",
       "      <td>424.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>48.6</td>\n",
       "      <td>1.1</td>\n",
       "      <td>26.64710</td>\n",
       "      <td>30.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>44000161</td>\n",
       "      <td>10/19/1993</td>\n",
       "      <td>38.869837</td>\n",
       "      <td>-90.166778</td>\n",
       "      <td>4</td>\n",
       "      <td>5</td>\n",
       "      <td>9343063</td>\n",
       "      <td>3.267</td>\n",
       "      <td>0.158</td>\n",
       "      <td>15.2</td>\n",
       "      <td>8.2</td>\n",
       "      <td>39.0</td>\n",
       "      <td>444.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>36.5</td>\n",
       "      <td>0.7</td>\n",
       "      <td>32.82694</td>\n",
       "      <td>32.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>44000162</td>\n",
       "      <td>10/19/1993</td>\n",
       "      <td>38.864269</td>\n",
       "      <td>-90.160085</td>\n",
       "      <td>4</td>\n",
       "      <td>5</td>\n",
       "      <td>9343065</td>\n",
       "      <td>3.345</td>\n",
       "      <td>0.161</td>\n",
       "      <td>15.2</td>\n",
       "      <td>8.4</td>\n",
       "      <td>37.0</td>\n",
       "      <td>456.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>43.3</td>\n",
       "      <td>1.5</td>\n",
       "      <td>42.57542</td>\n",
       "      <td>28.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>44000163</td>\n",
       "      <td>10/19/1993</td>\n",
       "      <td>38.877205</td>\n",
       "      <td>-90.173401</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>9343010</td>\n",
       "      <td>3.661</td>\n",
       "      <td>0.183</td>\n",
       "      <td>14.9</td>\n",
       "      <td>9.1</td>\n",
       "      <td>54.0</td>\n",
       "      <td>457.0</td>\n",
       "      <td>0.55</td>\n",
       "      <td>79.3</td>\n",
       "      <td>10.4</td>\n",
       "      <td>68.25222</td>\n",
       "      <td>26.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   SHEETBAR        DATE   LATITUDE  LONGITUDE  FLDNUM  STRATUM  LOCATCD  \\\n",
       "0  44000159  10/19/1993  38.873994 -90.189663       4        5  9343058   \n",
       "1  44000160  10/19/1993  38.873772 -90.180452       4        5  9343059   \n",
       "2  44000161  10/19/1993  38.869837 -90.166778       4        5  9343063   \n",
       "3  44000162  10/19/1993  38.864269 -90.160085       4        5  9343065   \n",
       "4  44000163  10/19/1993  38.877205 -90.173401       4        1  9343010   \n",
       "\n",
       "      TN     TP  TEMP   DO  TURB   COND   VEL    SS   WDP    CHLcal  SECCHI  \n",
       "0  2.769  0.145  15.7  8.5  37.0  424.0  0.00  40.3   0.5  24.29702    27.0  \n",
       "1  3.049  0.146  15.7  8.3  43.0  424.0  0.00  48.6   1.1  26.64710    30.0  \n",
       "2  3.267  0.158  15.2  8.2  39.0  444.0  0.00  36.5   0.7  32.82694    32.0  \n",
       "3  3.345  0.161  15.2  8.4  37.0  456.0  0.00  43.3   1.5  42.57542    28.0  \n",
       "4  3.661  0.183  14.9  9.1  54.0  457.0  0.55  79.3  10.4  68.25222    26.0  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataFrame = dataFrame[dataFrame['FLDNUM'] == 4]\n",
    "print(dataFrame.shape)\n",
    "dataFrame  = dataFrame.reset_index(drop = True)\n",
    "dataFrame.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "accepted-overall",
   "metadata": {},
   "source": [
    "# Functions\n",
    "Here, we interpolate for missing data values. These occur when the data set has a missing value. The way it is computed utilizes a $k$-nearest neighbors algorithm. A weighted average using the $k$ nearest points is used to compute the missing value, and it appends it to a new column in the data set called `\"PREDICTED_\" + variable`, where `variable` is what we wish to interpolate (`TN` or `TP`) for example."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "wicked-roberts",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Functions have been loaded\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Params:\n",
    "df = the dataframe filtered for the pool\n",
    "hashtable = the hash table of distances of each point for the data frame. (Created from construct_hashtable)\n",
    "naVar = the variable we wish to interpolate\n",
    "year = the year we wish to predict for\n",
    "k = the number of terms in the weighted average for interpolation\n",
    "\n",
    "NOTE: for now, set k = 2 due to potential bug for larger k\n",
    "\n",
    "This is one of two predict functions. \n",
    "Here, multiple years worth of data can put in, and only the specified year will be predicted\n",
    "and added to the dataframe. Note that this function will find the k nearnest neighbors using df,\n",
    "regardless of year.\n",
    "\"\"\"\n",
    "def predict_years(df, hashtable, naVar, year, k):\n",
    "    df[\"PREDICTED_\" + naVar] = df[naVar]\n",
    "    df_year = df.copy()\n",
    "    df_year = df_year[df_year[\"YEAR\"] == year]\n",
    "    naIndices = df_year[(df_year[naVar].isnull())]\n",
    "    print(\"For \" + naVar + \" we will interpolate \" + str(len(naIndices)) + \" points.\")\n",
    "    for index, row in naIndices.iterrows():\n",
    "        distances, neighbors = k_nearest_neighbors(df, index, naVar, hashtable, k)\n",
    "        df.loc[index, \"PREDICTED_\" + naVar] = interpolate(df, distances, neighbors, naVar)\n",
    "    print(naVar + \" interpolation success\")\n",
    "\"\"\"\n",
    "Params:\n",
    "df = the dataframe filtered for the pool\n",
    "hashtable = the hash table of distances of each point for the data frame.\n",
    "naVar = the variable we wish to interpolate\n",
    "k = the number of terms in the weighted average for interpolation\n",
    "\n",
    "NOTE: for now, keep k = 2 due to potential bug for k > 2\n",
    "\n",
    "This predict function is more crude than predict_years. It will predict using missing values of naVar for \n",
    "the entire dataframe, using the entire dataframe to locate the k nearnest neighbors.\n",
    "\"\"\"\n",
    "def predict(df, hashtable, naVar, k):\n",
    "    df[\"PREDICTED_\" + naVar] = df[naVar]\n",
    "    naIndices = df[(df[naVar].isnull())]\n",
    "    print(\"For \" + naVar + \" we will interpolate \" + str(len(naIndices)) + \" points.\")\n",
    "    for index, row in naIndices.iterrows():\n",
    "        distances, neighbors = k_nearest_neighbors(df, index, naVar, hashtable, k)\n",
    "        df.loc[index, \"PREDICTED_\" + naVar] = interpolate(df, distances, neighbors, naVar)\n",
    "    print(naVar + \" interpolation success\")\n",
    "\n",
    "    \n",
    "\"\"\"\n",
    "params:\n",
    "\n",
    "minimums = lower bound of data, typically latitude or longitude\n",
    "maximums = upper bound of data, typically latitude or longitude\n",
    "x = data we wish to transfrom in to [0,1], typically a latitude or a longitude\n",
    "\n",
    "This is a helper function for construct_hashtable and k_nearest_neighbors. It takes in a latitude or\n",
    "a longitude and maps to in to [0,1] so the point can be plotted properly in the hashtable\n",
    "\"\"\"    \n",
    "    \n",
    "def transform(minimum, maximum, x):\n",
    "    return (1 / (maximum - minimum) ) * (x - minimum)\n",
    "\"\"\"\n",
    "Params:\n",
    "point 1 = First point (latitude and longitude)\n",
    "point 2 = Second point (latitude and longitude)\n",
    "\n",
    "Returns the distance in kilometers between two points in space, using\n",
    "scipy distance function.\n",
    "\"\"\"\n",
    "def dist(point1, point2):\n",
    "    return distance.distance(point1, point2).km\n",
    "\"\"\"\n",
    "params:\n",
    "df = the dataframe\n",
    "\n",
    "returns: hashtable (list of lists of lists of tuples(index, latitude, longitude))\n",
    "\n",
    "Constructs a hash table of locations of points (the position where data is recorded)\n",
    "This is used in the k nearest neighbors algorithm. Locations that are near each other in space\n",
    "are near each other in the hashtable\n",
    "\n",
    "\"\"\"    \n",
    "def construct_hashtable(df):\n",
    "    #get hashtable information\n",
    "    data_length = math.sqrt(df.shape[0])\n",
    "    #print(\"data_length: \" + str(data_length))\n",
    "    interval_length = 1 / data_length\n",
    "    lat_minimum = df[[\"LATITUDE\"]].min()[0] - 1\n",
    "    lat_maximum = df[[\"LATITUDE\"]].max()[0] + 1\n",
    "    long_minimum = df[[\"LONGITUDE\"]].min()[0] - 1\n",
    "    long_maximum = df[[\"LONGITUDE\"]].max()[0] + 1\n",
    "    \n",
    "    #construct hashtable\n",
    "    hashtable = [[[] for x in range(int(data_length)+1)] for y in range(int(data_length)+1)]\n",
    "    \n",
    "    #populate hashtable\n",
    "    for index, row in df.iterrows():\n",
    "        r_lat = row['LATITUDE']\n",
    "        r_long = row['LONGITUDE']\n",
    "        lat = math.floor(transform(lat_minimum, lat_maximum, r_lat) / interval_length)\n",
    "        long = math.floor(transform(long_minimum, long_maximum, r_long) / interval_length)\n",
    "        #print(\"lat: \" + str(lat))\n",
    "        #print(\"long: \" + str(long))\n",
    "        hashtable[lat][long].append((index, r_lat, r_long))\n",
    "\n",
    "    return hashtable\n",
    "\"\"\"\n",
    "Params:\n",
    "df = dataframe\n",
    "index = index of variable we wish to find k nearest neighbors of\n",
    "naVar = variable to predict\n",
    "hashtable = data structure created from construct_hashtable\n",
    "k = number of nearest neighbors\n",
    "\n",
    "Returns: (distances, indices) of k nearnest neighbors\n",
    "\n",
    "This algorithm will find the k nearest neighbors of a desired point using the hashtable, if possible.\n",
    "If there are no valid points near the given point, then the algorithm will use brute force\n",
    "\"\"\"\n",
    "def k_nearest_neighbors(df, index, naVar, hashtable, k):\n",
    "\n",
    "    distances = []\n",
    "    neighbor_indices = []\n",
    "    neighbors = {}\n",
    "    \n",
    "    data_length = math.sqrt(df.shape[0])\n",
    "    interval_length = 1 / data_length\n",
    "    lat_minimum = df[[\"LATITUDE\"]].min()[0] - 1\n",
    "    lat_maximum = df[[\"LATITUDE\"]].max()[0] + 1\n",
    "    long_minimum = df[[\"LONGITUDE\"]].min()[0] - 1\n",
    "    long_maximum = df[[\"LONGITUDE\"]].max()[0] + 1\n",
    "    \n",
    "    \n",
    "    row_na = df.loc[index]\n",
    "    point_na = (row_na['LATITUDE'], row_na['LONGITUDE'])\n",
    "    lat = math.floor(transform(lat_minimum, lat_maximum, point_na[0]) / interval_length)\n",
    "    long = math.floor(transform(long_minimum, long_maximum, point_na[1]) / interval_length)\n",
    "    season = row_na['SEASON']\n",
    "    \n",
    "    for inx, latitude, longitude in hashtable[lat][long]:\n",
    "        distance_km = dist(point_na, (latitude, longitude))\n",
    "        if not np.isnan(df.loc[inx][naVar]) and distance_km != 0:\n",
    "            distances.append(distance_km)\n",
    "            if distance_km in neighbors.keys():\n",
    "                neighbors[distance_km].append(inx)\n",
    "            else:\n",
    "                neighbors[distance_km] = [inx]\n",
    "    \n",
    "    if lat != 0:\n",
    "        \n",
    "        for inx, latitude, longitude in hashtable[lat - 1][long]:\n",
    "                distance_km = dist(point_na, (latitude, longitude))\n",
    "                if not np.isnan(df.loc[inx][naVar]) and distance_km != 0:\n",
    "                    distances.append(distance_km)\n",
    "                    if distance_km in neighbors.keys():\n",
    "                        neighbors[distance_km].append(inx)\n",
    "                    else:\n",
    "                        neighbors[distance_km] = [inx]\n",
    "                    \n",
    "        if long != 0:\n",
    "            for inx, latitude, longitude in hashtable[lat - 1][long - 1]:\n",
    "                distance_km = dist(point_na, (latitude, longitude))\n",
    "                if not np.isnan(df.loc[inx][naVar]) and distance_km != 0:\n",
    "                    distances.append(distance_km)\n",
    "                    if distance_km in neighbors.keys():\n",
    "                        neighbors[distance_km].append(inx)\n",
    "                    else:\n",
    "                        neighbors[distance_km] = [inx]\n",
    "        \n",
    "        if long + 1 != len(hashtable):\n",
    "            for inx, latitude, longitude in hashtable[lat - 1][long + 1]:\n",
    "                distance_km = dist(point_na, (latitude, longitude))\n",
    "                if not np.isnan(df.loc[inx][naVar]) and distance_km != 0:\n",
    "                    distances.append(distance_km)\n",
    "                    if distance_km in neighbors.keys():\n",
    "                        neighbors[distance_km].append(inx)\n",
    "                    else:\n",
    "                        neighbors[distance_km] = [inx]\n",
    "        \n",
    "    if lat + 1 != len(hashtable):\n",
    "        \n",
    "        for inx, latitude, longitude in hashtable[lat + 1][long]:\n",
    "                distance_km = dist(point_na, (latitude, longitude))\n",
    "                if not np.isnan(df.loc[inx][naVar]) and distance_km != 0:\n",
    "                    distances.append(distance_km)\n",
    "                    if distance_km in neighbors.keys():\n",
    "                        neighbors[distance_km].append(inx)\n",
    "                    else:\n",
    "                        neighbors[distance_km] = [inx]\n",
    "                    \n",
    "        if long != 0:\n",
    "            for inx, latitude, longitude in hashtable[lat + 1][long - 1]:\n",
    "                distance_km = dist(point_na, (latitude, longitude))\n",
    "                if not np.isnan(df.loc[inx][naVar]) and distance_km != 0:\n",
    "                    distances.append(distance_km)\n",
    "                    if distance_km in neighbors.keys():\n",
    "                        neighbors[distance_km].append(inx)\n",
    "                    else:\n",
    "                        neighbors[distance_km] = [inx]\n",
    "        \n",
    "        if long + 1 != len(hashtable):\n",
    "            for inx, latitude, longitude in hashtable[lat + 1][long + 1]:\n",
    "                distance_km = dist(point_na, (latitude, longitude))\n",
    "                if not np.isnan(df.loc[inx][naVar]) and distance_km != 0:\n",
    "                    distances.append(distance_km)\n",
    "                    if distance_km in neighbors.keys():\n",
    "                        neighbors[distance_km].append(inx)\n",
    "                    else:\n",
    "                        neighbors[distance_km] = [inx]\n",
    "        \n",
    "    if long != 0:\n",
    "        for inx, latitude, longitude in hashtable[lat][long - 1]:\n",
    "            distance_km = dist(point_na, (latitude, longitude))\n",
    "            if not np.isnan(df.loc[inx][naVar]) and distance_km != 0:\n",
    "                distances.append(distance_km)\n",
    "                if distance_km in neighbors.keys():\n",
    "                    neighbors[distance_km].append(inx)\n",
    "                else:\n",
    "                    neighbors[distance_km] = [inx]\n",
    "        \n",
    "    if long + 1 != len(hashtable):\n",
    "        for inx, latitude, longitude in hashtable[lat][long + 1]:\n",
    "            distance_km = dist(point_na, (latitude, longitude))\n",
    "            if not np.isnan(df.loc[inx][naVar]) and distance_km != 0:\n",
    "                distances.append(distance_km)\n",
    "                if distance_km in neighbors.keys():\n",
    "                    neighbors[distance_km].append(inx)\n",
    "                else:\n",
    "                    neighbors[distance_km] = [inx]\n",
    "                    \n",
    "    #Possible bug with neighbor dictionary for k > 2\n",
    "    distances.sort()\n",
    "    distances = distances[0:k]\n",
    "    for distance_km in distances:\n",
    "        for inx in neighbors[distance_km]:\n",
    "            neighbor_indices.append(inx)\n",
    "    neighbor_indices = neighbor_indices[0:k]\n",
    "    if len(neighbor_indices) < k and len(neighbor_indices) != 0:\n",
    "        print(\"INTERPOLATING WITH \" + str(len(neighbor_indices)) + \" POINTS INSTEAD OF \" + str(k) + \" POINTS\")\n",
    "    if len(neighbor_indices) >= 2:\n",
    "        return (distances, neighbor_indices)\n",
    "    \n",
    "    #Possible bug with neighbor dictionary for k > 2\n",
    "    distances = []\n",
    "    neighbors = {}\n",
    "    neighbor_indices = []\n",
    "    for inx, row in df.iterrows():\n",
    "        distance_km = dist(point_na, (row['LATITUDE'], row['LONGITUDE']))\n",
    "        if not np.isnan(df.loc[inx][naVar]) and distance_km != 0:\n",
    "            distances.append(distance_km)\n",
    "            if distance_km in neighbors.keys():\n",
    "                neighbors[distance_km].append(inx)\n",
    "            else:\n",
    "                neighbors[distance_km] = [inx]\n",
    "    distances.sort()\n",
    "    distances = distances[0:k]\n",
    "    for distance_km in distances:\n",
    "        for inx in neighbors[distance_km]:\n",
    "            neighbor_indices.append(inx)\n",
    "    neighbor_indices = neighbor_indices[0:k]\n",
    "    return (distances, neighbor_indices)            \n",
    "    \n",
    "\"\"\"\n",
    "Params:\n",
    "df = dataframe\n",
    "distances = distances of the points being used to interpolate for the missing value \n",
    "neighbors  = the points for prediction\n",
    "naVar = variable to predict\n",
    "\n",
    "Returns the interpolated data value for a missing datapoint.\n",
    "\"\"\"\n",
    "def interpolate(df, distances, neighbors, naVar):\n",
    "    result = 0\n",
    "    denominator = [1 / x for x in distances]\n",
    "    denominator = sum(denominator)\n",
    "    for i in range(len(distances)):\n",
    "        result += ((1/distances[i]) / denominator) * df.loc[neighbors[i]][naVar]\n",
    "    return result\n",
    "print(\"Functions have been loaded\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "strong-delaware",
   "metadata": {},
   "source": [
    "# Season by Season Interpolation\n",
    "Here, we begin the process of interpolating for missing data based upon the season. To do this, the dataframe we input needs a season column. To obtain this, we create a copy of the dataframe, and use this copy throughout the rest of the work. To obtain the season, we utilize the date recorded to get the season. We create a column for the particular month, and then use a dictionary to replace that value with the appropriate season."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "chemical-jamaica",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>SHEETBAR</th>\n",
       "      <th>DATE</th>\n",
       "      <th>LATITUDE</th>\n",
       "      <th>LONGITUDE</th>\n",
       "      <th>FLDNUM</th>\n",
       "      <th>STRATUM</th>\n",
       "      <th>LOCATCD</th>\n",
       "      <th>TN</th>\n",
       "      <th>TP</th>\n",
       "      <th>TEMP</th>\n",
       "      <th>DO</th>\n",
       "      <th>TURB</th>\n",
       "      <th>COND</th>\n",
       "      <th>VEL</th>\n",
       "      <th>SS</th>\n",
       "      <th>WDP</th>\n",
       "      <th>CHLcal</th>\n",
       "      <th>SECCHI</th>\n",
       "      <th>MONTH</th>\n",
       "      <th>YEAR</th>\n",
       "      <th>SEASON</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>44000159</td>\n",
       "      <td>10/19/1993</td>\n",
       "      <td>38.873994</td>\n",
       "      <td>-90.189663</td>\n",
       "      <td>4</td>\n",
       "      <td>5</td>\n",
       "      <td>9343058</td>\n",
       "      <td>2.769</td>\n",
       "      <td>0.145</td>\n",
       "      <td>15.7</td>\n",
       "      <td>8.5</td>\n",
       "      <td>37.0</td>\n",
       "      <td>424.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>40.3</td>\n",
       "      <td>0.5</td>\n",
       "      <td>24.29702</td>\n",
       "      <td>27.0</td>\n",
       "      <td>10</td>\n",
       "      <td>1993</td>\n",
       "      <td>FALL</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>44000160</td>\n",
       "      <td>10/19/1993</td>\n",
       "      <td>38.873772</td>\n",
       "      <td>-90.180452</td>\n",
       "      <td>4</td>\n",
       "      <td>5</td>\n",
       "      <td>9343059</td>\n",
       "      <td>3.049</td>\n",
       "      <td>0.146</td>\n",
       "      <td>15.7</td>\n",
       "      <td>8.3</td>\n",
       "      <td>43.0</td>\n",
       "      <td>424.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>48.6</td>\n",
       "      <td>1.1</td>\n",
       "      <td>26.64710</td>\n",
       "      <td>30.0</td>\n",
       "      <td>10</td>\n",
       "      <td>1993</td>\n",
       "      <td>FALL</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>44000161</td>\n",
       "      <td>10/19/1993</td>\n",
       "      <td>38.869837</td>\n",
       "      <td>-90.166778</td>\n",
       "      <td>4</td>\n",
       "      <td>5</td>\n",
       "      <td>9343063</td>\n",
       "      <td>3.267</td>\n",
       "      <td>0.158</td>\n",
       "      <td>15.2</td>\n",
       "      <td>8.2</td>\n",
       "      <td>39.0</td>\n",
       "      <td>444.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>36.5</td>\n",
       "      <td>0.7</td>\n",
       "      <td>32.82694</td>\n",
       "      <td>32.0</td>\n",
       "      <td>10</td>\n",
       "      <td>1993</td>\n",
       "      <td>FALL</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>44000162</td>\n",
       "      <td>10/19/1993</td>\n",
       "      <td>38.864269</td>\n",
       "      <td>-90.160085</td>\n",
       "      <td>4</td>\n",
       "      <td>5</td>\n",
       "      <td>9343065</td>\n",
       "      <td>3.345</td>\n",
       "      <td>0.161</td>\n",
       "      <td>15.2</td>\n",
       "      <td>8.4</td>\n",
       "      <td>37.0</td>\n",
       "      <td>456.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>43.3</td>\n",
       "      <td>1.5</td>\n",
       "      <td>42.57542</td>\n",
       "      <td>28.0</td>\n",
       "      <td>10</td>\n",
       "      <td>1993</td>\n",
       "      <td>FALL</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>44000163</td>\n",
       "      <td>10/19/1993</td>\n",
       "      <td>38.877205</td>\n",
       "      <td>-90.173401</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>9343010</td>\n",
       "      <td>3.661</td>\n",
       "      <td>0.183</td>\n",
       "      <td>14.9</td>\n",
       "      <td>9.1</td>\n",
       "      <td>54.0</td>\n",
       "      <td>457.0</td>\n",
       "      <td>0.55</td>\n",
       "      <td>79.3</td>\n",
       "      <td>10.4</td>\n",
       "      <td>68.25222</td>\n",
       "      <td>26.0</td>\n",
       "      <td>10</td>\n",
       "      <td>1993</td>\n",
       "      <td>FALL</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   SHEETBAR        DATE   LATITUDE  LONGITUDE  FLDNUM  STRATUM  LOCATCD  \\\n",
       "0  44000159  10/19/1993  38.873994 -90.189663       4        5  9343058   \n",
       "1  44000160  10/19/1993  38.873772 -90.180452       4        5  9343059   \n",
       "2  44000161  10/19/1993  38.869837 -90.166778       4        5  9343063   \n",
       "3  44000162  10/19/1993  38.864269 -90.160085       4        5  9343065   \n",
       "4  44000163  10/19/1993  38.877205 -90.173401       4        1  9343010   \n",
       "\n",
       "      TN     TP  TEMP   DO  TURB   COND   VEL    SS   WDP    CHLcal  SECCHI  \\\n",
       "0  2.769  0.145  15.7  8.5  37.0  424.0  0.00  40.3   0.5  24.29702    27.0   \n",
       "1  3.049  0.146  15.7  8.3  43.0  424.0  0.00  48.6   1.1  26.64710    30.0   \n",
       "2  3.267  0.158  15.2  8.2  39.0  444.0  0.00  36.5   0.7  32.82694    32.0   \n",
       "3  3.345  0.161  15.2  8.4  37.0  456.0  0.00  43.3   1.5  42.57542    28.0   \n",
       "4  3.661  0.183  14.9  9.1  54.0  457.0  0.55  79.3  10.4  68.25222    26.0   \n",
       "\n",
       "   MONTH  YEAR SEASON  \n",
       "0     10  1993   FALL  \n",
       "1     10  1993   FALL  \n",
       "2     10  1993   FALL  \n",
       "3     10  1993   FALL  \n",
       "4     10  1993   FALL  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = dataFrame.copy()\n",
    "df[\"MONTH\"] = pd.DatetimeIndex(dataFrame[\"DATE\"]).month\n",
    "df[\"YEAR\"] = pd.DatetimeIndex(dataFrame[\"DATE\"]).year\n",
    "df[\"SEASON\"] = df[\"MONTH\"]\n",
    "seasons = {3 : 'SPRING',\n",
    "           4 : 'SPRING',\n",
    "           5 : 'SPRING',\n",
    "           6 : 'SUMMER',\n",
    "           7 : 'SUMMER',\n",
    "           8 : 'SUMMER',\n",
    "           9 : 'FALL',\n",
    "           10 : 'FALL',\n",
    "           11: 'FALL',\n",
    "           12: 'WINTER',\n",
    "           1: 'WINTER',\n",
    "           2: 'WINTER'}\n",
    "df = df.replace({\"SEASON\" : seasons})\n",
    "# for index, row in newData.iterrows():\n",
    "#     newData.loc[index, 'SEASON'] = seasons\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "hourly-terrorist",
   "metadata": {},
   "source": [
    "# Prediction Group by Season and several years\n",
    "\n",
    "Here, we interpolate missing data values for a certain year, grouping data by season. For example, we can use spring data from 2001, 2002, and 2003 to predict spring data for 2002. In the list below, `x` represents the year we predict. Modify the lower bound to be the earliest year of data that you have.\n",
    "\n",
    "Then, we create a year column on the data frame to allow the predict function to get the correct year for prediction purposes. Then, we interpolate data values year by year, season by season. After it is done, it sends the output to a `.csv` file, so modify the path as necessary. The `if (seasonFrame.shape[0] > 1)` is a check to make sure there is enough data present to do any interpolating, since we use `2` points to predict a missing third here.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "adapted-timothy",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "continuous = ['TN','TP','TEMP','DO','TURB','COND','VEL','SS','WDP','CHLcal','SECCHI']\n",
    "years = [[x-1, x, x+1] for x in range(1995, 2021)]\n",
    "newData[\"YEAR\"] = pd.DatetimeIndex(dataFrame[\"DATE\"]).year\n",
    "result = pd.DataFrame()\n",
    "for setOfYears in years:\n",
    "    print(\"Set of years: \" + str(setOfYears))\n",
    "    print(\"Year to interpolate missing data: \" + str(setOfYears[1]))\n",
    "    threeYearFrame = newData[newData['YEAR'].isin(setOfYears)]\n",
    "    seasons = ['SPRING','SUMMER','FALL','WINTER']\n",
    "    for season in seasons:\n",
    "        print(season)\n",
    "        seasonalFrame = threeYearFrame[threeYearFrame['SEASON'] == season]\n",
    "        if (seasonalFrame.shape[0] > 1):\n",
    "            seasonalHash = construct_hashtable(seasonalFrame)\n",
    "            for var in continuous:\n",
    "                predict_years(seasonalFrame, seasonalHash, var,setOfYears[1],2)\n",
    "            \n",
    "            yearToAdd = seasonalFrame[seasonalFrame['YEAR'] == setOfYears[1]]\n",
    "            result = result.append(yearToAdd, ignore_index = True)\n",
    "            result = result.reset_index(drop = True)\n",
    "            \n",
    "            \n",
    "    print(\"Predicted for \" + str(setOfYears[1]))\n",
    "    \n",
    "result.to_csv(r\"..\\pools_specific_EDA\\Open River\\allvars_interpolated_3yearsxseason.csv\")\n",
    "print(\"Done\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "innocent-portfolio",
   "metadata": {},
   "source": [
    "# Prediction group year by year\n",
    "\n",
    "Here, we do predictions for missing data for a given year using the entire year's worth of data. As such, we use the `predict` function instead of the `predict_years` function. Here as well, we create a year column from the date in the data frame, which we then use to generate a list of years to predict data for. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "vocal-purse",
   "metadata": {},
   "outputs": [],
   "source": [
    "newData = dataFrame.copy()\n",
    "newData[\"YEAR\"] = pd.DatetimeIndex(dataFrame[\"DATE\"]).year\n",
    "years = newData[\"YEAR\"].unique()\n",
    "result = pd.DataFrame()\n",
    "for year in years:\n",
    "    currentSet = newData[(newData[\"YEAR\"] == year)]\n",
    "    currentSet = currentSet.reset_index(drop = True)\n",
    "    hashTable = construct_hashtable(currentSet)\n",
    "    predict(currentSet, hashTable, \"TN\",2)\n",
    "    predict(currentSet, hashTable, \"TP\",2)\n",
    "    print(\"Predicted for \" + str(year))\n",
    "    result = result.append(currentSet)\n",
    "\n",
    "result = result.reset_index(drop = True)\n",
    "result.to_csv(r\"C:\\Users\\forre\\Desktop\\REU\\TDA\\Data\\predicted_tn_tp_years.csv\")\n",
    "print(\"Done\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "graduate-somalia",
   "metadata": {},
   "source": [
    "# Spacial interpolation by year, by season (Casey)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "royal-mailman",
   "metadata": {},
   "source": [
    "Load functions - parameter definitons and correct documentation still needs to be completed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "heavy-approval",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "from geopy import distance\n",
    "import time\n",
    "import pickle\n",
    "from sklearn.model_selection import cross_val_score,train_test_split\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.preprocessing import RobustScaler,PolynomialFeatures\n",
    "import sklearn.metrics\n",
    "pd.set_option('display.max_columns', None)\n",
    "\n",
    "# useful functions and classes\n",
    "\n",
    "# This class stores the latitude and longitude of a sample, and indicates \n",
    "# if this location has the desired variable we are estimating\n",
    "class Location:\n",
    "    def __init__(self,latitude,longitude,hasv,ID,value,SHEET):\n",
    "        self.ID = ID\n",
    "        self.SHEET = SHEET\n",
    "        self.latitude = latitude\n",
    "        self.longitude = longitude\n",
    "        self.hasv = hasv\n",
    "        self.value = value\n",
    "        \n",
    "    def __str__(self):\n",
    "        return str(self.ID)\n",
    "\n",
    "# Calculates the distance between 2 samples in km\n",
    "def getdist(S1,S2):\n",
    "    # radius of earth in km\n",
    "    coords_1 = (S1.latitude, S1.longitude)\n",
    "    coords_2 = (S2.latitude, S2.longitude)\n",
    "    dist = distance.distance(coords_1, coords_2).km\n",
    "    return dist\n",
    "\n",
    "\n",
    "# PRE: all locations in the dataframe are\n",
    "# unique\n",
    "def DistanceMatrix(dataframe,variable):\n",
    "    # numunique = len(dataframe[\"LOCATCD\"].unique())\n",
    "    # numlocations = dataframe.shape[0]\n",
    "    # try:\n",
    "    #     assert(numunique == numlocations), f\"{numunique} unique locations but {numlocations} number of locations\"\n",
    "    \n",
    "    # except AssertionError as msg:\n",
    "    #     print(dataframe[dataframe[\"LOCATCD\"].duplicated(keep=False)])\n",
    "    #     print(msg)\n",
    "        \n",
    "    # the list of location objects\n",
    "    locations = []\n",
    "    # the list of indexes where the the row is located in the dataframe\n",
    "    #indexes = []\n",
    "    for index,row in dataframe.iterrows():\n",
    "        # make a location object on this row\n",
    "        if pd.isnull(row[variable]):\n",
    "            hasv = False\n",
    "        else:\n",
    "            hasv = True\n",
    "        locations.append(Location(row[\"LATITUDE\"],row[\"LONGITUDE\"],hasv,row[\"LOCATCD\"],row[variable],row[\"SHEETBAR\"]))\n",
    "        #indexes.append(index)\n",
    "        \n",
    "    matrix = pd.DataFrame(0,index=locations,columns=locations)\n",
    "    for ci,column in enumerate(locations):\n",
    "        for ri,row in enumerate(locations):\n",
    "            if ri>ci:\n",
    "                # compute distance between column and row\n",
    "                dist = getdist(row,column)\n",
    "            elif ci>ri:\n",
    "                dist = matrix.iloc[ci,ri]\n",
    "            # put this distance in the dataframe\n",
    "            else:\n",
    "                continue\n",
    "            matrix.iloc[ri,ci] = dist\n",
    "    return matrix\n",
    "\n",
    "def changeVar(DM,dataframe,variable):\n",
    "    locations = DM.index\n",
    "    # loop through each location\n",
    "    for i,loc in enumerate(locations):\n",
    "        SHEET = loc.SHEET\n",
    "        row = dataframe.loc[dataframe[\"SHEETBAR\"]==SHEET]\n",
    "        #print(row)\n",
    "        #print(row.shape)\n",
    "        #print(row.loc[row.index[0],variable])\n",
    "        #print(type(row[variable]))\n",
    "        try:\n",
    "            assert(row.shape[0]==1), \"Multiple rows with same SHEETBAR\"\n",
    "        except AssertionError as msg:\n",
    "            print(dataframe[dataframe[\"SHEETBAR\"].duplicated(keep=False)])\n",
    "            print(msg)\n",
    "            \n",
    "        # Pull value of desired variable\n",
    "        val = row.loc[row.index[0],variable]\n",
    "        if pd.isnull(val):\n",
    "            locations[i].hasv = False\n",
    "            locations[i].value = None\n",
    "        else:\n",
    "            locations[i].hasv = True\n",
    "            locations[i].value = val\n",
    "            \n",
    "    DM.index = locations\n",
    "    DM.columns = locations\n",
    "        \n",
    "def getclosest(numclosest,distancematrix,location):\n",
    "    column = distancematrix.loc[:,location].copy()\n",
    "    #print(type(distancematrix.index[0]))\n",
    "    # Filter the locations that dont have the desired variable\n",
    "    doesnthavev = []\n",
    "    for i in range(len(column)):\n",
    "        if not column.index[i].hasv:\n",
    "            doesnthavev.append(column.index[i])\n",
    "            \n",
    "    # Get rid of locations that dont have the desired variable\n",
    "    column.drop(doesnthavev,inplace = True)\n",
    "    # Get rid of the location we are predicting for if it exists\n",
    "    column.drop(location,inplace = True,errors=\"ignore\")\n",
    "    #print(type(column))\n",
    "    column.sort_values(inplace = True)\n",
    "    \n",
    "    return column.iloc[0:numclosest]\n",
    "\n",
    "# Key: Location Codes that need predicting\n",
    "# Value: List of tuples (locatcd,distance,value)\n",
    "def makeDict(DM,numclosest,testing):\n",
    "    # Loop through each location without a value for variable\n",
    "    closestDict = {}\n",
    "    for loc in DM.columns:\n",
    "        if not loc.hasv or testing:\n",
    "            # Get the closest locations to loc THAT ISN'T LOC\n",
    "            closest = getclosest(numclosest,DM,loc)\n",
    "            # The list of tuples that contain location id, the distance, and the value for variable\n",
    "            tuples = []\n",
    "            for i,dist in enumerate(closest):\n",
    "                SHEET = closest.index[i].SHEET\n",
    "                val = closest.index[i].value\n",
    "                tuples.append((SHEET,dist,val))\n",
    "            closestDict[loc.SHEET] = tuples\n",
    "    return closestDict\n",
    "\n",
    "def predict(tuples,numclosest = 2):\n",
    "    loc2 = tuples[0]\n",
    "    loc3 = tuples[1]\n",
    "    d12 = loc2[1]\n",
    "    val2 = loc2[2]\n",
    "    d13 = loc3[1]\n",
    "    val3 = loc3[2]\n",
    "    \n",
    "    if d12 == d13:\n",
    "        return 0.5*d12+0.5*d13\n",
    "    elif d12 == 0:\n",
    "        return val2\n",
    "    elif d13 == 0:\n",
    "        return val3\n",
    "    \n",
    "    else:\n",
    "        c2 = d12/(d12+d13)\n",
    "        c3 = d13/(d12+d13)\n",
    "        \n",
    "        predicted = c2*val2+c3*val3\n",
    "    \n",
    "        return predicted\n",
    "\n",
    "      \n",
    "'''\n",
    "data - the pandas dataframe that is ready to interpolate missing values\n",
    "MUST HAVE \"LATITUDE\", \"LONGITUDE\",\"YEAR\", \"TIME CODE\", \"SHEETBAR\" columns\n",
    "\n",
    "missing_vars - the list of column names (as strings) of the dataframe that we should attempt to fill in\n",
    "\n",
    "numlocations - the number of locations used to predict the new value, default is 2 (currently the only option implemented)\n",
    "\n",
    "RETURN - a dataframe with extra columns saying the predicted values of the missing_vars\n",
    "'''\n",
    "def linear_interpolate(data,missing_vars,numlocations = 2,testing = False,verbosity = 0):\n",
    "    \n",
    "    print(\"Building a new dataframe with predicted values\")\n",
    "    start_time = time.time()\n",
    "    # Testing for duplicated locations if needed\n",
    "    #s = qualdata_noprediction[\"LOCATCD\"].duplicated(keep=False)\n",
    "    # get the years and timecodes for this dataset\n",
    "    # predictions can only be made if the point is in the same year and time code (what if we don't need to do this)\n",
    "    years = data[\"YEAR\"].unique()\n",
    "    seasons = data[\"SEASON\"].unique()\n",
    "    pools = data[\"FLDNUM\"].unique()\n",
    "    data_prediction = pd.DataFrame()\n",
    "    for pool in pools:\n",
    "        for year in years:\n",
    "            for season in seasons:\n",
    "                if verbosity > 0:\n",
    "                    print(f\"Appending predicted data for {year}  {season}  FLDNUM {pool}\")\n",
    "                # curset is the current set of rows we are predicting for\n",
    "                curset = data[(data[\"YEAR\"]==year) & (data[\"SEASON\"]==season) & (data[\"FLDNUM\"]==pool)].copy()\n",
    "                \n",
    "                if verbosity > 1:\n",
    "                    print(\"Size of this year and season:\", curset.shape)\n",
    "                \n",
    "                # Boolean to indicate if variable in Distance matrix needs updating\n",
    "                first = True\n",
    "                for var in missing_vars:\n",
    "                    newcolumn = \"PREDICTED_\"+var\n",
    "                    curset[newcolumn] = 0\n",
    "                \n",
    "                    #check to see if there are enough valid locations\n",
    "                    # that can be used to predict\n",
    "                    if not testing:\n",
    "                        bad = bool((curset[var].notnull().sum()<numlocations))\n",
    "                    else:\n",
    "                        bad = bool((curset[var].notnull().sum()<numlocations+1))\n",
    "                    \n",
    "    \n",
    "                    if(bad):\n",
    "                        if verbosity > 2:\n",
    "                            print(\"Less than \"+str(numlocations)+\" locations have \"+var+\" in this set, dropping rows without \"+var)\n",
    "                        curset = curset[curset[var].notnull()]\n",
    "                        curset[newcolumn] = curset[var]\n",
    "                        if verbosity > 2:\n",
    "                            print(\"Current set is now \",curset.shape)\n",
    "                    else:\n",
    "                        if first:\n",
    "                            if verbosity > 2:\n",
    "                                print(\"Creating DM with \",var)\n",
    "                            DM = DistanceMatrix(curset,var)\n",
    "                            first = False\n",
    "                        else:\n",
    "                            if verbosity > 2:\n",
    "                                print(\"Changing to \",var)\n",
    "                            changeVar(DM,curset,var)\n",
    "                            \n",
    "                        # Returns a dictionary mapping each location code to a tuple with prediction information\n",
    "                        Dict = makeDict(DM,numlocations,testing)\n",
    "                        \n",
    "                        #put in predicted variable\n",
    "                        for index,row in curset.iterrows():\n",
    "                            if pd.isnull(row[var]) or testing:\n",
    "                                try:\n",
    "                                    prediction = predict(Dict[row[\"SHEETBAR\"]])\n",
    "                                    #print(curset.loc[index,newcolumn],prediction)\n",
    "                                    curset.loc[index,newcolumn] = prediction\n",
    "                                except ZeroDivisionError:\n",
    "                                    print(\"Couldn't predict for \", str(row[\"SHEETBAR\"]))\n",
    "                                    print(Dict[row[\"SHEETBAR\"]])\n",
    "                                    curset.loc[index,newcolumn] = None\n",
    "                            else:\n",
    "                                curset.loc[index,newcolumn] = row[var]\n",
    "    \n",
    "                data_prediction = data_prediction.append(curset,ignore_index=True)  \n",
    "    \n",
    "    if verbosity > 0:\n",
    "        print(\"Final data set size is \",data_prediction.shape)\n",
    "    print(f\"Interpolating took {(time.time()-start_time)/60} minutes\")\n",
    "    return data_prediction"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "difficult-calculator",
   "metadata": {},
   "source": [
    "#### Interpolating missing data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "stainless-infrared",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building a new dataframe with predicted values\n",
      "Appending predicted data for 1993  FALL  FLDNUM 4\n",
      "Size of this year and season: (111, 21)\n",
      "Appending predicted data for 1993  SPRING  FLDNUM 4\n",
      "Size of this year and season: (0, 21)\n",
      "Appending predicted data for 1993  WINTER  FLDNUM 4\n",
      "Size of this year and season: (0, 21)\n",
      "Appending predicted data for 1993  SUMMER  FLDNUM 4\n",
      "Size of this year and season: (0, 21)\n",
      "Appending predicted data for 1994  FALL  FLDNUM 4\n",
      "Size of this year and season: (121, 21)\n",
      "Appending predicted data for 1994  SPRING  FLDNUM 4\n",
      "Size of this year and season: (120, 21)\n",
      "Appending predicted data for 1994  WINTER  FLDNUM 4\n",
      "Size of this year and season: (71, 21)\n",
      "Appending predicted data for 1994  SUMMER  FLDNUM 4\n",
      "Size of this year and season: (106, 21)\n",
      "Appending predicted data for 1995  FALL  FLDNUM 4\n",
      "Size of this year and season: (121, 21)\n",
      "Appending predicted data for 1995  SPRING  FLDNUM 4\n",
      "Size of this year and season: (121, 21)\n",
      "Appending predicted data for 1995  WINTER  FLDNUM 4\n",
      "Size of this year and season: (99, 21)\n",
      "Appending predicted data for 1995  SUMMER  FLDNUM 4\n",
      "Size of this year and season: (120, 21)\n",
      "Appending predicted data for 1996  FALL  FLDNUM 4\n",
      "Size of this year and season: (106, 21)\n",
      "Appending predicted data for 1996  SPRING  FLDNUM 4\n",
      "Size of this year and season: (121, 21)\n",
      "Appending predicted data for 1996  WINTER  FLDNUM 4\n",
      "Size of this year and season: (106, 21)\n",
      "Appending predicted data for 1996  SUMMER  FLDNUM 4\n",
      "Size of this year and season: (121, 21)\n",
      "Appending predicted data for 1997  FALL  FLDNUM 4\n",
      "Size of this year and season: (120, 21)\n",
      "Appending predicted data for 1997  SPRING  FLDNUM 4\n",
      "Size of this year and season: (121, 21)\n",
      "Appending predicted data for 1997  WINTER  FLDNUM 4\n",
      "Size of this year and season: (87, 21)\n",
      "Appending predicted data for 1997  SUMMER  FLDNUM 4\n",
      "Size of this year and season: (121, 21)\n",
      "Appending predicted data for 1998  FALL  FLDNUM 4\n",
      "Size of this year and season: (121, 21)\n",
      "Appending predicted data for 1998  SPRING  FLDNUM 4\n",
      "Size of this year and season: (121, 21)\n",
      "Appending predicted data for 1998  WINTER  FLDNUM 4\n",
      "Size of this year and season: (121, 21)\n",
      "Appending predicted data for 1998  SUMMER  FLDNUM 4\n",
      "Size of this year and season: (120, 21)\n",
      "Appending predicted data for 1999  FALL  FLDNUM 4\n",
      "Size of this year and season: (121, 21)\n",
      "Appending predicted data for 1999  SPRING  FLDNUM 4\n",
      "Size of this year and season: (121, 21)\n",
      "Appending predicted data for 1999  WINTER  FLDNUM 4\n",
      "Size of this year and season: (105, 21)\n",
      "Appending predicted data for 1999  SUMMER  FLDNUM 4\n",
      "Size of this year and season: (103, 21)\n",
      "Appending predicted data for 2000  FALL  FLDNUM 4\n",
      "Size of this year and season: (116, 21)\n",
      "Appending predicted data for 2000  SPRING  FLDNUM 4\n",
      "Size of this year and season: (121, 21)\n",
      "Appending predicted data for 2000  WINTER  FLDNUM 4\n",
      "Size of this year and season: (68, 21)\n",
      "Appending predicted data for 2000  SUMMER  FLDNUM 4\n",
      "Size of this year and season: (119, 21)\n",
      "Appending predicted data for 2001  FALL  FLDNUM 4\n",
      "Size of this year and season: (120, 21)\n",
      "Appending predicted data for 2001  SPRING  FLDNUM 4\n",
      "Size of this year and season: (121, 21)\n",
      "Appending predicted data for 2001  WINTER  FLDNUM 4\n",
      "Size of this year and season: (102, 21)\n",
      "Appending predicted data for 2001  SUMMER  FLDNUM 4\n",
      "Size of this year and season: (121, 21)\n",
      "Appending predicted data for 2002  FALL  FLDNUM 4\n",
      "Size of this year and season: (0, 21)\n",
      "Appending predicted data for 2002  SPRING  FLDNUM 4\n",
      "Size of this year and season: (117, 21)\n",
      "Appending predicted data for 2002  WINTER  FLDNUM 4\n",
      "Size of this year and season: (114, 21)\n",
      "Appending predicted data for 2002  SUMMER  FLDNUM 4\n",
      "Size of this year and season: (108, 21)\n",
      "Appending predicted data for 2004  FALL  FLDNUM 4\n",
      "Size of this year and season: (118, 21)\n",
      "Appending predicted data for 2004  SPRING  FLDNUM 4\n",
      "Size of this year and season: (117, 21)\n",
      "Appending predicted data for 2004  WINTER  FLDNUM 4\n",
      "Size of this year and season: (45, 21)\n",
      "Appending predicted data for 2004  SUMMER  FLDNUM 4\n",
      "Size of this year and season: (118, 21)\n",
      "Appending predicted data for 2005  FALL  FLDNUM 4\n",
      "Size of this year and season: (121, 21)\n",
      "Appending predicted data for 2005  SPRING  FLDNUM 4\n",
      "Size of this year and season: (119, 21)\n",
      "Appending predicted data for 2005  WINTER  FLDNUM 4\n",
      "Size of this year and season: (112, 21)\n",
      "Appending predicted data for 2005  SUMMER  FLDNUM 4\n",
      "Size of this year and season: (121, 21)\n",
      "Appending predicted data for 2006  FALL  FLDNUM 4\n",
      "Size of this year and season: (121, 21)\n",
      "Appending predicted data for 2006  SPRING  FLDNUM 4\n",
      "Size of this year and season: (113, 21)\n",
      "Appending predicted data for 2006  WINTER  FLDNUM 4\n",
      "Size of this year and season: (121, 21)\n",
      "Appending predicted data for 2006  SUMMER  FLDNUM 4\n",
      "Size of this year and season: (113, 21)\n",
      "Appending predicted data for 2007  FALL  FLDNUM 4\n",
      "Size of this year and season: (118, 21)\n",
      "Appending predicted data for 2007  SPRING  FLDNUM 4\n",
      "Size of this year and season: (121, 21)\n",
      "Appending predicted data for 2007  WINTER  FLDNUM 4\n",
      "Size of this year and season: (108, 21)\n",
      "Appending predicted data for 2007  SUMMER  FLDNUM 4\n",
      "Size of this year and season: (117, 21)\n",
      "Appending predicted data for 2008  FALL  FLDNUM 4\n",
      "Size of this year and season: (121, 21)\n",
      "Appending predicted data for 2008  SPRING  FLDNUM 4\n",
      "Size of this year and season: (121, 21)\n",
      "Appending predicted data for 2008  WINTER  FLDNUM 4\n",
      "Size of this year and season: (117, 21)\n",
      "Appending predicted data for 2008  SUMMER  FLDNUM 4\n",
      "Size of this year and season: (120, 21)\n",
      "Appending predicted data for 2009  FALL  FLDNUM 4\n",
      "Size of this year and season: (118, 21)\n",
      "Appending predicted data for 2009  SPRING  FLDNUM 4\n",
      "Size of this year and season: (121, 21)\n",
      "Appending predicted data for 2009  WINTER  FLDNUM 4\n",
      "Size of this year and season: (116, 21)\n",
      "Appending predicted data for 2009  SUMMER  FLDNUM 4\n",
      "Size of this year and season: (106, 21)\n",
      "Appending predicted data for 2010  FALL  FLDNUM 4\n",
      "Size of this year and season: (121, 21)\n",
      "Appending predicted data for 2010  SPRING  FLDNUM 4\n",
      "Size of this year and season: (121, 21)\n",
      "Appending predicted data for 2010  WINTER  FLDNUM 4\n",
      "Size of this year and season: (117, 21)\n",
      "Appending predicted data for 2010  SUMMER  FLDNUM 4\n",
      "Size of this year and season: (121, 21)\n",
      "Appending predicted data for 2011  FALL  FLDNUM 4\n",
      "Size of this year and season: (106, 21)\n",
      "Appending predicted data for 2011  SPRING  FLDNUM 4\n",
      "Size of this year and season: (121, 21)\n",
      "Appending predicted data for 2011  WINTER  FLDNUM 4\n",
      "Size of this year and season: (117, 21)\n",
      "Appending predicted data for 2011  SUMMER  FLDNUM 4\n",
      "Size of this year and season: (117, 21)\n",
      "Appending predicted data for 2012  FALL  FLDNUM 4\n",
      "Size of this year and season: (106, 21)\n",
      "Appending predicted data for 2012  SPRING  FLDNUM 4\n",
      "Size of this year and season: (117, 21)\n",
      "Appending predicted data for 2012  WINTER  FLDNUM 4\n",
      "Size of this year and season: (121, 21)\n",
      "Appending predicted data for 2012  SUMMER  FLDNUM 4\n",
      "Size of this year and season: (98, 21)\n",
      "Appending predicted data for 2013  FALL  FLDNUM 4\n",
      "Size of this year and season: (106, 21)\n",
      "Appending predicted data for 2013  SPRING  FLDNUM 4\n",
      "Size of this year and season: (121, 21)\n",
      "Appending predicted data for 2013  WINTER  FLDNUM 4\n",
      "Size of this year and season: (121, 21)\n",
      "Appending predicted data for 2013  SUMMER  FLDNUM 4\n",
      "Size of this year and season: (121, 21)\n",
      "Appending predicted data for 2014  FALL  FLDNUM 4\n",
      "Size of this year and season: (121, 21)\n",
      "Appending predicted data for 2014  SPRING  FLDNUM 4\n",
      "Size of this year and season: (103, 21)\n",
      "Appending predicted data for 2014  WINTER  FLDNUM 4\n",
      "Size of this year and season: (58, 21)\n",
      "Appending predicted data for 2014  SUMMER  FLDNUM 4\n",
      "Size of this year and season: (106, 21)\n",
      "Appending predicted data for 2015  FALL  FLDNUM 4\n",
      "Size of this year and season: (121, 21)\n",
      "Appending predicted data for 2015  SPRING  FLDNUM 4\n",
      "Size of this year and season: (117, 21)\n",
      "Appending predicted data for 2015  WINTER  FLDNUM 4\n",
      "Size of this year and season: (122, 21)\n",
      "Appending predicted data for 2015  SUMMER  FLDNUM 4\n",
      "Size of this year and season: (121, 21)\n",
      "Appending predicted data for 2016  FALL  FLDNUM 4\n",
      "Size of this year and season: (121, 21)\n",
      "Appending predicted data for 2016  SPRING  FLDNUM 4\n",
      "Size of this year and season: (113, 21)\n",
      "Appending predicted data for 2016  WINTER  FLDNUM 4\n",
      "Size of this year and season: (121, 21)\n",
      "Appending predicted data for 2016  SUMMER  FLDNUM 4\n",
      "Size of this year and season: (80, 21)\n",
      "Appending predicted data for 2017  FALL  FLDNUM 4\n",
      "Size of this year and season: (106, 21)\n",
      "Appending predicted data for 2017  SPRING  FLDNUM 4\n",
      "Size of this year and season: (121, 21)\n",
      "Appending predicted data for 2017  WINTER  FLDNUM 4\n",
      "Size of this year and season: (121, 21)\n",
      "Appending predicted data for 2017  SUMMER  FLDNUM 4\n",
      "Size of this year and season: (98, 21)\n",
      "Appending predicted data for 2018  FALL  FLDNUM 4\n",
      "Size of this year and season: (121, 21)\n",
      "Appending predicted data for 2018  SPRING  FLDNUM 4\n",
      "Size of this year and season: (111, 21)\n",
      "Appending predicted data for 2018  WINTER  FLDNUM 4\n",
      "Size of this year and season: (86, 21)\n",
      "Appending predicted data for 2018  SUMMER  FLDNUM 4\n",
      "Size of this year and season: (105, 21)\n",
      "Appending predicted data for 2019  FALL  FLDNUM 4\n",
      "Size of this year and season: (121, 21)\n",
      "Appending predicted data for 2019  SPRING  FLDNUM 4\n",
      "Size of this year and season: (58, 21)\n",
      "Appending predicted data for 2019  WINTER  FLDNUM 4\n",
      "Size of this year and season: (57, 21)\n",
      "Appending predicted data for 2019  SUMMER  FLDNUM 4\n",
      "Size of this year and season: (106, 21)\n",
      "Appending predicted data for 2020  FALL  FLDNUM 4\n",
      "Size of this year and season: (92, 21)\n",
      "Appending predicted data for 2020  SPRING  FLDNUM 4\n",
      "Size of this year and season: (0, 21)\n",
      "Appending predicted data for 2020  WINTER  FLDNUM 4\n",
      "Size of this year and season: (100, 21)\n",
      "Appending predicted data for 2020  SUMMER  FLDNUM 4\n",
      "Size of this year and season: (94, 21)\n",
      "Final data set size is  (11116, 32)\n",
      "Interpolating took 12.700153462092082 minutes\n"
     ]
    }
   ],
   "source": [
    "missing_vars = ['TN','TP','TEMP','DO','TURB','COND','VEL','SS','WDP','CHLcal','SECCHI']\n",
    "water_interpolated = linear_interpolate(df,missing_vars, verbosity = 2)\n",
    "water_interpolated.to_csv(r\"C:\\Users\\forre\\Desktop\\REU\\TDA\\Data\\caseyInterpolatePool26.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "existing-marina",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n"
     ]
    }
   ],
   "source": [
    "for var in missing_vars:\n",
    "    print(water_interpolated[\"PREDICTED_\"+var].isna().sum())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dressed-rebate",
   "metadata": {},
   "source": [
    "# Multivariate Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "baking-notification",
   "metadata": {},
   "source": [
    "Filter for non missing data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "peaceful-feelings",
   "metadata": {},
   "outputs": [],
   "source": [
    "cols = ['TP','TN','CHLcal','SS','VEL','DO','COND','WDP','TURB','TEMP','SECCHI']\n",
    "print(\"Filtering out all rows with missing data\")\n",
    "qualdata = water_data.dropna(axis=0, how='any', thresh=None, subset=cols, inplace=False).copy()\n",
    "print(qualdata.shape)\n",
    "print(\"Filtering out colums that we dont need\")\n",
    "qualdata.drop(qualdata.columns.difference(cols), 1, inplace=True)\n",
    "print(qualdata.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "signal-creator",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model Hyperparameter: Set the degree of the polynomial to fit\n",
    "d = 5\n",
    "\n",
    "\n",
    "X = np.array(qualdata[['CHLcal','SS','VEL','DO','COND','WDP','TURB','TEMP','SECCHI']])\n",
    "TP = np.array(qualdata['TP'])\n",
    "TN = np.array(qualdata['TN'])\n",
    "\n",
    "# Good idea to standardize predictor attributes - assumes each variable has a decently normal distribution\n",
    "scaler = RobustScaler().fit(X)\n",
    "X_standard = scaler.transform(X)\n",
    "\n",
    "\n",
    "# The PolynomialFeatures class in sklearn.preprocessing can be used to transform a data matrix by\n",
    "# adding higher-order and interaction terms for the existing features. It also adds a \"zeroth\"\n",
    "# column consisting of all 1's that corresponds to the weight w_0 in a regression model.\n",
    "poly = PolynomialFeatures(d)\n",
    "\n",
    "# We \"fit\" the poly object to our data matrix to allow it to identify the structure of the data\n",
    "# (notably the number of attributes, or columns, in the data matrix).\n",
    "poly.fit(X_standard)\n",
    "#poly.fit(X)\n",
    "\n",
    "\n",
    "# Now we use poly.transform to add any higher-order terms to the data matrix.\n",
    "# This also adds a zeroth attribute which is set to all 1's.\n",
    "augmented_X = poly.transform(X_standard)\n",
    "#augmented_X = poly.transform(X)\n",
    "\n",
    "\n",
    "# Next we create a linear regression object (named lm for \"linear model\").\n",
    "# Because our augmented data matrix includes an all 1's column, we don't\n",
    "# need to fit the intercept (w_0) here.\n",
    "TP_lm = LinearRegression(fit_intercept=False)\n",
    "TN_lm = LinearRegression(fit_intercept=False)\n",
    "\n",
    "\n",
    "# Split data into train and test for each\n",
    "TPX_train, TPX_test, TPy_train, TPy_test = train_test_split(augmented_X, TP, train_size=0.7)\n",
    "TNX_train, TNX_test, TNy_train, TNy_test = train_test_split(augmented_X, TN, train_size=0.7)\n",
    "\n",
    "# Fit models using training data\n",
    "TP_lm.fit(TPX_train, TPy_train)\n",
    "TN_lm.fit(TNX_train, TNy_train)\n",
    "\n",
    "# After fitting the regression model, we can estimate the error\n",
    "# Get training errors\n",
    "TP_train_err = np.mean((TPy_train - TP_lm.predict(TPX_train)) ** 2)\n",
    "TN_train_err = np.mean((TNy_train - TN_lm.predict(TNX_train)) ** 2)\n",
    "\n",
    "# Get test errors\n",
    "TP_test_err = np.mean((TPy_test - TP_lm.predict(TPX_test)) ** 2)\n",
    "TN_test_err = np.mean((TNy_test - TN_lm.predict(TNX_test)) ** 2)\n",
    "\n",
    "# Report\n",
    "print(\"TP training set mean squared error: {:.6f}\".format(TP_train_err),\" on average off {:.6f}\".format(np.sqrt(TP_train_err)))\n",
    "print(\"TN training set mean squared error: {:.6f}\".format(TN_train_err),\" on average off {:.6f}\".format(np.sqrt(TN_train_err)),\"\\n\")\n",
    "print(\"TP test set mean squared error: {:.6f}\".format(TP_test_err),\" on average off {:.6f}\".format(np.sqrt(TP_test_err)))\n",
    "print(\"TN test set mean squared error: {:.6f}\".format(TN_test_err),\" on average off {:.6f}\".format(np.sqrt(TN_test_err)),\"\\n\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
