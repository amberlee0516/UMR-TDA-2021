{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "happy-intake",
   "metadata": {},
   "source": [
    "## Running regression again with median imputation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "searching-bradford",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import cross_val_score,train_test_split\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.preprocessing import RobustScaler,PolynomialFeatures\n",
    "import sklearn.metrics\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "informed-princess",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Now adding a year column\n",
      "Now adding a month column\n",
      "Now adding a season column\n",
      "Dropping TN outliers\n",
      "\n",
      " Water data\n",
      "Index(['SHEETBAR', 'DATE', 'LATITUDE', 'LONGITUDE', 'FLDNUM', 'STRATUM',\n",
      "       'LOCATCD', 'TN', 'TP', 'TEMP', 'DO', 'TURB', 'COND', 'VEL', 'SS', 'WDP',\n",
      "       'CHLcal', 'SECCHI', 'YEAR', 'MONTH', 'SEASON'],\n",
      "      dtype='object')\n",
      "(82478, 21)\n"
     ]
    }
   ],
   "source": [
    "# Load the cleaned data\n",
    "water_path = r\"..\\LTRM data\\water_data_qfneg.csv\"\n",
    "water_data = pd.read_csv(water_path, low_memory = False)\n",
    "\n",
    "continuous = ['TN','TP','TEMP','DO','TURB','COND','VEL','SS','WDP','CHLcal','SECCHI']\n",
    "seasons = {3:\"SPRING\",4:\"SPRING\",5:\"SPRING\",6:\"SUMMER\",7:\"SUMMER\",8:\"SUMMER\",9:\"FALL\",10:\"FALL\",11:\"FALL\",12:\"WINTER\",1:\"WINTER\",2:\"WINTER\"}\n",
    "\n",
    "print(\"Now adding a year column\")\n",
    "water_data[\"YEAR\"] = pd.DatetimeIndex(water_data[\"DATE\"]).year\n",
    "print(\"Now adding a month column\")\n",
    "water_data[\"MONTH\"] = pd.DatetimeIndex(water_data[\"DATE\"]).month\n",
    "print(\"Now adding a season column\")\n",
    "water_data[\"SEASON\"] = water_data[\"MONTH\"]\n",
    "water_data = water_data.replace({\"SEASON\":seasons})\n",
    "print(\"Dropping TN outliers\")\n",
    "water_data.drop([46795,46545,46727],axis=0,inplace=True)\n",
    "print(\"\\n Water data\")\n",
    "print(water_data.columns)\n",
    "print(water_data.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "optional-monitor",
   "metadata": {},
   "source": [
    "Looking at extreme outliers for model building, then removing them"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "convenient-purple",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "29136    46.989\n",
       "55650    32.965\n",
       "19178    22.939\n",
       "55611    22.677\n",
       "59504    22.576\n",
       "          ...  \n",
       "82475       NaN\n",
       "82477       NaN\n",
       "82478       NaN\n",
       "82479       NaN\n",
       "82480       NaN\n",
       "Name: TN, Length: 82478, dtype: float64"
      ]
     },
     "execution_count": 100,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "water_data[\"TN\"].sort_values(ascending=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "national-breast",
   "metadata": {},
   "source": [
    "Example for imputing medians into each column, not used after ambers suggesting of imputing separately"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "unlike-blond",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing multivariate polynomial interpolation, using every other variable as a predictor besides target variable\n",
      "Imputing medians\n",
      "Median for  TN  is  2.53\n",
      "Median for  TP  is  0.163\n",
      "Median for  TEMP  is  14.7\n",
      "Median for  DO  is  9.7\n",
      "Median for  TURB  is  21.0\n",
      "Median for  COND  is  462.0\n",
      "Median for  VEL  is  0.1\n",
      "Median for  SS  is  25.7\n",
      "Median for  WDP  is  2.24\n",
      "Median for  CHLcal  is  16.6519\n",
      "Median for  SECCHI  is  41.0\n",
      "(82478, 21)\n",
      "Filtering out colums that we dont need\n",
      "(82478, 11)\n"
     ]
    }
   ],
   "source": [
    "print(\"Testing multivariate polynomial interpolation, using every other variable as a predictor besides target variable\")\n",
    "print(\"Imputing medians\")\n",
    "imputed_water_data = water_data.copy()\n",
    "\n",
    "for col in continuous:\n",
    "    median = imputed_water_data[col].median()\n",
    "    print(\"Median for \",col,\" is \",round(median,4))\n",
    "    imputed_water_data[col].fillna(value=median,inplace=True)\n",
    "\n",
    "# Checking to see if we lost any values\n",
    "qualdata = imputed_water_data.dropna(axis=0, how='any', thresh=None, subset=continuous, inplace=False)\n",
    "print(qualdata.shape)\n",
    "print(\"Filtering out colums that we dont need\")\n",
    "qualdata.drop(qualdata.columns.difference(continuous), axis=1, inplace=True)\n",
    "print(qualdata.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "diagnostic-horizon",
   "metadata": {},
   "source": [
    "Water dataset has no missing variables in any variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "empty-literacy",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.mkdir(\"Regression Models\\\\Imputed_data\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "interesting-christian",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "9.7"
      ]
     },
     "execution_count": 102,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "var = \"TN\"\n",
    "predictors = continuous.copy()\n",
    "predictors.remove(var)\n",
    "\n",
    "# Define predictor and target data\n",
    "X = np.array(qualdata[predictors])\n",
    "y = np.array(qualdata[var])\n",
    "\n",
    "# Get median of first attribute\n",
    "np.nanmedian([item[2] for item in X])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "institutional-figure",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "9.7"
      ]
     },
     "execution_count": 103,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.nanmedian([item[2] for item in X])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "ordered-smoke",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Median for  TP  is  0.163\n",
      "Median for  TEMP  is  14.7\n",
      "Median for  DO  is  9.7\n",
      "Median for  TURB  is  21.0\n",
      "Median for  COND  is  462.0\n",
      "Median for  VEL  is  0.1\n",
      "Median for  SS  is  25.7\n",
      "Median for  WDP  is  2.24\n",
      "Median for  CHLcal  is  16.6519\n",
      "Median for  SECCHI  is  41.0\n"
     ]
    }
   ],
   "source": [
    "var = \"TN\"\n",
    "predictors = continuous.copy()\n",
    "predictors.remove(var)\n",
    "\n",
    "# Define predictor and target data\n",
    "X = np.array(qualdata[predictors])\n",
    "y = np.array(qualdata[var])\n",
    "\n",
    "# Impute medians for X_train\n",
    "for index,predictor in enumerate(predictors):\n",
    "    median = np.nanmedian([item[index] for item in X])\n",
    "    print(\"Median for \",predictor,\" is \",round(median,4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "mexican-television",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.shape[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "remarkable-healthcare",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0, 1, 2, 3, 4, 5, 6, 7, 8, 9]"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(range(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "id": "closing-hundred",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "-----------------------------------\n",
      "Building model for  TN\n",
      "Imputing medians for training set\n",
      "Imputing medians for test set\n",
      "Degree 1 polynomial has RMSE = 1.17639\n",
      "Degree 1 polynomial has MAE = 0.85433\n",
      "\n",
      "-----------------------------------\n",
      "Building model for  TN\n",
      "Imputing medians for training set\n",
      "Imputing medians for test set\n",
      "Degree 2 polynomial has RMSE = 1.12375\n",
      "Degree 2 polynomial has MAE = 0.81562\n",
      "\n",
      "-----------------------------------\n",
      "Building model for  TP\n",
      "Imputing medians for training set\n",
      "Imputing medians for test set\n",
      "Degree 1 polynomial has RMSE = 0.12826\n",
      "Degree 1 polynomial has MAE = 0.06691\n",
      "\n",
      "-----------------------------------\n",
      "Building model for  TP\n",
      "Imputing medians for training set\n",
      "Imputing medians for test set\n",
      "Degree 2 polynomial has RMSE = 0.12189\n",
      "Degree 2 polynomial has MAE = 0.05733\n",
      "\n",
      "-----------------------------------\n",
      "Building model for  VEL\n",
      "Imputing medians for training set\n",
      "Imputing medians for test set\n",
      "Degree 1 polynomial has RMSE = 0.29541\n",
      "Degree 1 polynomial has MAE = 0.17508\n",
      "\n",
      "-----------------------------------\n",
      "Building model for  VEL\n",
      "Imputing medians for training set\n",
      "Imputing medians for test set\n",
      "Degree 2 polynomial has RMSE = 0.27543\n",
      "Degree 2 polynomial has MAE = 0.16164\n"
     ]
    }
   ],
   "source": [
    "models = [\"TN\",\"TP\",\"VEL\"]\n",
    "degs = [1,2]\n",
    "\n",
    "for var in models:\n",
    "    #os.mkdir(\"Regression Models\\\\Imputed_data\\\\\"+var)\n",
    "    \n",
    "    # Filter data by non na var columns\n",
    "    qualdata = water_data.dropna(axis=0, how='any', thresh=None, subset=[var], inplace=False)\n",
    "    \n",
    "    for deg in degs:\n",
    "        print(\"\\n-----------------------------------\")\n",
    "        print(\"Building model for \",var)\n",
    "        \n",
    "        # Make path for outputs of this model\n",
    "        #os.mkdir(\"Regression Models\\\\Imputed_data\\\\\"+var+\"\\\\degree\"+str(deg))\n",
    "        #path = \"Regression Models\\\\Imputed_data\\\\\"+var+\"\\\\degree\"+str(deg)+\"\\\\\"\n",
    "        \n",
    "        predictors = continuous.copy()\n",
    "        predictors.remove(var)\n",
    "\n",
    "        # Define predictor and target data\n",
    "        X = np.array(qualdata[predictors])\n",
    "        y = np.array(qualdata[var])\n",
    "\n",
    "        # Split data into training and test sets\n",
    "        X_train, X_test, y_train, y_test = train_test_split(X, y, train_size=0.8)\n",
    "        \n",
    "        print(\"Imputing medians for training set\")\n",
    "        # Impute medians for X_train\n",
    "        for index,predictor in enumerate(predictors):\n",
    "            median = np.nanmedian([item[index] for item in X_train])\n",
    "            #print(\"Median for \",predictor,\" is \",round(median,4))\n",
    "            # Loop through the index of each indice looking for nan, replace with median\n",
    "            for item in X_train:\n",
    "                if np.isnan(item[index]):\n",
    "                    # Impute median\n",
    "                    item[index] = median\n",
    "                    \n",
    "            curvar = [item[index] for item in X_train]\n",
    "            assert not any(np.isnan(curvar)), \"Imputation didn't work\"\n",
    "        \n",
    "        print(\"Imputing medians for test set\")\n",
    "        # Impute medians for X_test\n",
    "        for index,predictor in enumerate(predictors):\n",
    "            median = np.nanmedian([item[index] for item in X_test])\n",
    "            #print(\"Median for \",predictor,\" is \",round(median,4))\n",
    "            # Loop through the index of each indice looking for nan, replace with median\n",
    "            for item in X_test:\n",
    "                if np.isnan(item[index]):\n",
    "                    # Impute median\n",
    "                    item[index] = median\n",
    "                    \n",
    "            curvar = [item[index] for item in X_test]\n",
    "            assert not any(np.isnan(curvar)), \"Imputation didn't work\"\n",
    "        \n",
    "\n",
    "        # Good idea to standardize predictor attributes on only the training set, but use it on both\n",
    "        scaler = RobustScaler().fit(X_train)\n",
    "        X_train_scaled = scaler.transform(X_train)\n",
    "        X_test_scaled = scaler.transform(X_test)\n",
    "        \n",
    "        pickle.dump(scaler,open(path+\"scaler.p\", \"wb\" ))\n",
    "\n",
    "        # Save train and test sets\n",
    "        pickle.dump(X_train,open(path+\"X_train.p\",\"wb\"))\n",
    "        pickle.dump(X_test,open(path+\"X_test.p\",\"wb\"))\n",
    "        pickle.dump(y_train,open(path+\"y_train.p\",\"wb\"))\n",
    "        pickle.dump(y_test,open(path+\"y_test.p\",\"wb\"))\n",
    "\n",
    "\n",
    "        # Finally, we build the model, fit it to the full training data, and\n",
    "        # estimate its out-of-sample performance by applying it to the test set\n",
    "        best_poly = PolynomialFeatures(deg)\n",
    "        best_lm = LinearRegression(fit_intercept=False)\n",
    "        best_lm.fit(best_poly.fit_transform(X_train_scaled), y_train)\n",
    "\n",
    "        # Save the best model and best polynomialfeatures\n",
    "        pickle.dump(best_lm,open(path+\"best_model_deg\"+str(deg)+\".p\",\"wb\"))\n",
    "        pickle.dump(best_poly,open(path+\"best_poly.p\",\"wb\"))\n",
    "\n",
    "\n",
    "        # Estimate performance on test set:\n",
    "        MSE = np.mean((y_test - best_lm.predict(best_poly.transform(X_test_scaled))) ** 2)\n",
    "        RMSE = np.sqrt(MSE)\n",
    "        MAE = np.mean(abs(y_test - best_lm.predict(best_poly.transform(X_test_scaled))))\n",
    "        print(f'Degree {deg} polynomial has RMSE = {RMSE:.5f}')\n",
    "        print(f'Degree {deg} polynomial has MAE = {MAE:.5f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "grand-miami",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
