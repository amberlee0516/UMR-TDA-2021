{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "intermediate-tonight",
   "metadata": {},
   "source": [
    "# Imports\n",
    "This the script for prepossing the data to create data ready for the TDA mapper algorithm. This has the implementation for the IDW weighting methods, both with a single season, as well as multiple yearsadjacent to it.\n",
    "\n",
    "Written by: Killian Davis & Frederick \"Forrest\" Miller (alphabetical order)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "literary-swaziland",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imports done\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import pandas as pd\n",
    "from geopy import distance\n",
    "import math\n",
    "pd.set_option('display.max_columns', None)\n",
    "import numpy as np\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.preprocessing import RobustScaler,StandardScaler\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "print(\"imports done\")\n",
    "pd.set_option(\"display.max_rows\", None)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "comprehensive-picnic",
   "metadata": {},
   "source": [
    "# File\n",
    "This uses the `water_data_qfneg.csv` file that is cleaned and ready to have missing values interpolated. It creates a pandas dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "regular-waters",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dataFrame Made\n"
     ]
    }
   ],
   "source": [
    "filePath = r\"..\\LTRM data\\water_data_qfneg.csv\"\n",
    "dataFrame = pd.read_csv(filePath, low_memory = False)\n",
    "print(\"dataFrame Made\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "massive-observer",
   "metadata": {},
   "source": [
    "# Filter for your pool\n",
    "Resets the index as well. This is done through the `FLDNUM` parameter, and can be found in accompanying documentation for the correct pool number. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "different-judges",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(11449, 18)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>SHEETBAR</th>\n",
       "      <th>DATE</th>\n",
       "      <th>LATITUDE</th>\n",
       "      <th>LONGITUDE</th>\n",
       "      <th>FLDNUM</th>\n",
       "      <th>STRATUM</th>\n",
       "      <th>LOCATCD</th>\n",
       "      <th>TN</th>\n",
       "      <th>TP</th>\n",
       "      <th>TEMP</th>\n",
       "      <th>DO</th>\n",
       "      <th>TURB</th>\n",
       "      <th>COND</th>\n",
       "      <th>VEL</th>\n",
       "      <th>SS</th>\n",
       "      <th>WDP</th>\n",
       "      <th>CHLcal</th>\n",
       "      <th>SECCHI</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>44000159</td>\n",
       "      <td>10/19/1993</td>\n",
       "      <td>38.873994</td>\n",
       "      <td>-90.189663</td>\n",
       "      <td>4</td>\n",
       "      <td>5</td>\n",
       "      <td>9343058</td>\n",
       "      <td>2.769</td>\n",
       "      <td>0.145</td>\n",
       "      <td>15.7</td>\n",
       "      <td>8.5</td>\n",
       "      <td>37.0</td>\n",
       "      <td>424.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>40.3</td>\n",
       "      <td>0.5</td>\n",
       "      <td>24.29702</td>\n",
       "      <td>27.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>44000160</td>\n",
       "      <td>10/19/1993</td>\n",
       "      <td>38.873772</td>\n",
       "      <td>-90.180452</td>\n",
       "      <td>4</td>\n",
       "      <td>5</td>\n",
       "      <td>9343059</td>\n",
       "      <td>3.049</td>\n",
       "      <td>0.146</td>\n",
       "      <td>15.7</td>\n",
       "      <td>8.3</td>\n",
       "      <td>43.0</td>\n",
       "      <td>424.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>48.6</td>\n",
       "      <td>1.1</td>\n",
       "      <td>26.64710</td>\n",
       "      <td>30.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>44000161</td>\n",
       "      <td>10/19/1993</td>\n",
       "      <td>38.869837</td>\n",
       "      <td>-90.166778</td>\n",
       "      <td>4</td>\n",
       "      <td>5</td>\n",
       "      <td>9343063</td>\n",
       "      <td>3.267</td>\n",
       "      <td>0.158</td>\n",
       "      <td>15.2</td>\n",
       "      <td>8.2</td>\n",
       "      <td>39.0</td>\n",
       "      <td>444.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>36.5</td>\n",
       "      <td>0.7</td>\n",
       "      <td>32.82694</td>\n",
       "      <td>32.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>44000162</td>\n",
       "      <td>10/19/1993</td>\n",
       "      <td>38.864269</td>\n",
       "      <td>-90.160085</td>\n",
       "      <td>4</td>\n",
       "      <td>5</td>\n",
       "      <td>9343065</td>\n",
       "      <td>3.345</td>\n",
       "      <td>0.161</td>\n",
       "      <td>15.2</td>\n",
       "      <td>8.4</td>\n",
       "      <td>37.0</td>\n",
       "      <td>456.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>43.3</td>\n",
       "      <td>1.5</td>\n",
       "      <td>42.57542</td>\n",
       "      <td>28.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>44000163</td>\n",
       "      <td>10/19/1993</td>\n",
       "      <td>38.877205</td>\n",
       "      <td>-90.173401</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>9343010</td>\n",
       "      <td>3.661</td>\n",
       "      <td>0.183</td>\n",
       "      <td>14.9</td>\n",
       "      <td>9.1</td>\n",
       "      <td>54.0</td>\n",
       "      <td>457.0</td>\n",
       "      <td>0.55</td>\n",
       "      <td>79.3</td>\n",
       "      <td>10.4</td>\n",
       "      <td>68.25222</td>\n",
       "      <td>26.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   SHEETBAR        DATE   LATITUDE  LONGITUDE  FLDNUM  STRATUM  LOCATCD  \\\n",
       "0  44000159  10/19/1993  38.873994 -90.189663       4        5  9343058   \n",
       "1  44000160  10/19/1993  38.873772 -90.180452       4        5  9343059   \n",
       "2  44000161  10/19/1993  38.869837 -90.166778       4        5  9343063   \n",
       "3  44000162  10/19/1993  38.864269 -90.160085       4        5  9343065   \n",
       "4  44000163  10/19/1993  38.877205 -90.173401       4        1  9343010   \n",
       "\n",
       "      TN     TP  TEMP   DO  TURB   COND   VEL    SS   WDP    CHLcal  SECCHI  \n",
       "0  2.769  0.145  15.7  8.5  37.0  424.0  0.00  40.3   0.5  24.29702    27.0  \n",
       "1  3.049  0.146  15.7  8.3  43.0  424.0  0.00  48.6   1.1  26.64710    30.0  \n",
       "2  3.267  0.158  15.2  8.2  39.0  444.0  0.00  36.5   0.7  32.82694    32.0  \n",
       "3  3.345  0.161  15.2  8.4  37.0  456.0  0.00  43.3   1.5  42.57542    28.0  \n",
       "4  3.661  0.183  14.9  9.1  54.0  457.0  0.55  79.3  10.4  68.25222    26.0  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataFrame = dataFrame[dataFrame['FLDNUM'] == 4]\n",
    "print(dataFrame.shape)\n",
    "dataFrame  = dataFrame.reset_index(drop = True)\n",
    "dataFrame.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "color-settle",
   "metadata": {},
   "source": [
    "# Functions\n",
    "Here, we interpolate for missing data values. These occur when the data set has a missing value. The way it is computed utilizes a $k$-nearest neighbors algorithm. A weighted average using the $k$ nearest points is used to compute the missing value, and it appends it to a new column in the data set called `\"PREDICTED_\" + variable`, where `variable` is what we wish to interpolate (`TN` or `TP`) for example."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "universal-reference",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Functions have been loaded\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Params:\n",
    "df = the dataframe filtered for the pool\n",
    "hashtable = the hash table of distances of each point for the data frame. (Created from construct_hashtable)\n",
    "naVar = the variable we wish to interpolate\n",
    "year = the year we wish to predict for\n",
    "k = the number of terms in the weighted average for interpolation\n",
    "\n",
    "NOTE: for now, set k = 2 due to potential bug for larger k\n",
    "\n",
    "This is one of two predict functions. \n",
    "Here, multiple years worth of data can put in, and only the specified year will be predicted\n",
    "and added to the dataframe. Note that this function will find the k nearnest neighbors using df,\n",
    "regardless of year.\n",
    "\"\"\"\n",
    "def predict_years(df, hashtable, naVar, year, k):\n",
    "    df[\"PREDICTED_\" + naVar] = df[naVar]\n",
    "    df_year = df.copy()\n",
    "    df_year = df_year[df_year[\"YEAR\"] == year]\n",
    "    naIndices = df_year[(df_year[naVar].isnull())]\n",
    "    print(\"For \" + naVar + \" we will interpolate \" + str(len(naIndices)) + \" points.\")\n",
    "    for index, row in naIndices.iterrows():\n",
    "        distances, neighbors = k_nearest_neighbors(df, index, naVar, hashtable, k)\n",
    "        df.loc[index, \"PREDICTED_\" + naVar] = interpolate(df, distances, neighbors, naVar)\n",
    "    print(naVar + \" interpolation success\")\n",
    "\"\"\"\n",
    "Params:\n",
    "df = the dataframe filtered for the pool\n",
    "hashtable = the hash table of distances of each point for the data frame.\n",
    "naVar = the variable we wish to interpolate\n",
    "k = the number of terms in the weighted average for interpolation\n",
    "\n",
    "NOTE: for now, keep k = 2 due to potential bug for k > 2\n",
    "\n",
    "This predict function is more crude than predict_years. It will predict using missing values of naVar for \n",
    "the entire dataframe, using the entire dataframe to locate the k nearnest neighbors.\n",
    "\"\"\"\n",
    "def predict(df, hashtable, naVar, k):\n",
    "    df[\"PREDICTED_\" + naVar] = df[naVar]\n",
    "    naIndices = df[(df[naVar].isnull())]\n",
    "    print(\"For \" + naVar + \" we will interpolate \" + str(len(naIndices)) + \" points.\")\n",
    "    for index, row in naIndices.iterrows():\n",
    "        distances, neighbors = k_nearest_neighbors(df, index, naVar, hashtable, k)\n",
    "        df.loc[index, \"PREDICTED_\" + naVar] = interpolate(df, distances, neighbors, naVar)\n",
    "    print(naVar + \" interpolation success\")\n",
    "\n",
    "    \n",
    "\"\"\"\n",
    "params:\n",
    "\n",
    "minimums = lower bound of data, typically latitude or longitude\n",
    "maximums = upper bound of data, typically latitude or longitude\n",
    "x = data we wish to transfrom in to [0,1], typically a latitude or a longitude\n",
    "\n",
    "This is a helper function for construct_hashtable and k_nearest_neighbors. It takes in a latitude or\n",
    "a longitude and maps to in to [0,1] so the point can be plotted properly in the hashtable\n",
    "\"\"\"    \n",
    "    \n",
    "def transform(minimum, maximum, x):\n",
    "    return (1 / (maximum - minimum) ) * (x - minimum)\n",
    "\"\"\"\n",
    "Params:\n",
    "point 1 = First point (latitude and longitude)\n",
    "point 2 = Second point (latitude and longitude)\n",
    "\n",
    "Returns the distance in kilometers between two points in space, using\n",
    "scipy distance function.\n",
    "\"\"\"\n",
    "def dist(point1, point2):\n",
    "    return distance.distance(point1, point2).km\n",
    "\"\"\"\n",
    "params:\n",
    "df = the dataframe\n",
    "\n",
    "returns: hashtable (list of lists of lists of tuples(index, latitude, longitude))\n",
    "\n",
    "Constructs a hash table of locations of points (the position where data is recorded)\n",
    "This is used in the k nearest neighbors algorithm. Locations that are near each other in space\n",
    "are near each other in the hashtable\n",
    "\n",
    "\"\"\"    \n",
    "def construct_hashtable(df):\n",
    "    #get hashtable information\n",
    "    data_length = math.sqrt(df.shape[0])\n",
    "    #print(\"data_length: \" + str(data_length))\n",
    "    interval_length = 1 / data_length\n",
    "    lat_minimum = df[[\"LATITUDE\"]].min()[0] - 1\n",
    "    lat_maximum = df[[\"LATITUDE\"]].max()[0] + 1\n",
    "    long_minimum = df[[\"LONGITUDE\"]].min()[0] - 1\n",
    "    long_maximum = df[[\"LONGITUDE\"]].max()[0] + 1\n",
    "    \n",
    "    #construct hashtable\n",
    "    hashtable = [[[] for x in range(int(data_length)+1)] for y in range(int(data_length)+1)]\n",
    "    \n",
    "    #populate hashtable\n",
    "    for index, row in df.iterrows():\n",
    "        r_lat = row['LATITUDE']\n",
    "        r_long = row['LONGITUDE']\n",
    "        lat = math.floor(transform(lat_minimum, lat_maximum, r_lat) / interval_length)\n",
    "        long = math.floor(transform(long_minimum, long_maximum, r_long) / interval_length)\n",
    "        #print(\"lat: \" + str(lat))\n",
    "        #print(\"long: \" + str(long))\n",
    "        hashtable[lat][long].append((index, r_lat, r_long))\n",
    "\n",
    "    return hashtable\n",
    "\"\"\"\n",
    "Params:\n",
    "df = dataframe\n",
    "index = index of variable we wish to find k nearest neighbors of\n",
    "naVar = variable to predict\n",
    "hashtable = data structure created from construct_hashtable\n",
    "k = number of nearest neighbors\n",
    "\n",
    "Returns: (distances, indices) of k nearnest neighbors\n",
    "\n",
    "This algorithm will find the k nearest neighbors of a desired point using the hashtable, if possible.\n",
    "If there are no valid points near the given point, then the algorithm will use brute force\n",
    "\"\"\"\n",
    "def k_nearest_neighbors(df, index, naVar, hashtable, k):\n",
    "\n",
    "    distances = []\n",
    "    neighbor_indices = []\n",
    "    neighbors = {}\n",
    "    \n",
    "    data_length = math.sqrt(df.shape[0])\n",
    "    interval_length = 1 / data_length\n",
    "    lat_minimum = df[[\"LATITUDE\"]].min()[0] - 1\n",
    "    lat_maximum = df[[\"LATITUDE\"]].max()[0] + 1\n",
    "    long_minimum = df[[\"LONGITUDE\"]].min()[0] - 1\n",
    "    long_maximum = df[[\"LONGITUDE\"]].max()[0] + 1\n",
    "    \n",
    "    \n",
    "    row_na = df.loc[index]\n",
    "    point_na = (row_na['LATITUDE'], row_na['LONGITUDE'])\n",
    "    lat = math.floor(transform(lat_minimum, lat_maximum, point_na[0]) / interval_length)\n",
    "    long = math.floor(transform(long_minimum, long_maximum, point_na[1]) / interval_length)\n",
    "    season = row_na['SEASON']\n",
    "    \n",
    "    for inx, latitude, longitude in hashtable[lat][long]:\n",
    "        distance_km = dist(point_na, (latitude, longitude))\n",
    "        if not np.isnan(df.loc[inx][naVar]) and distance_km != 0:\n",
    "            distances.append(distance_km)\n",
    "            if distance_km in neighbors.keys():\n",
    "                neighbors[distance_km].append(inx)\n",
    "            else:\n",
    "                neighbors[distance_km] = [inx]\n",
    "    \n",
    "    if lat != 0:\n",
    "        \n",
    "        for inx, latitude, longitude in hashtable[lat - 1][long]:\n",
    "                distance_km = dist(point_na, (latitude, longitude))\n",
    "                if not np.isnan(df.loc[inx][naVar]) and distance_km != 0:\n",
    "                    distances.append(distance_km)\n",
    "                    if distance_km in neighbors.keys():\n",
    "                        neighbors[distance_km].append(inx)\n",
    "                    else:\n",
    "                        neighbors[distance_km] = [inx]\n",
    "                    \n",
    "        if long != 0:\n",
    "            for inx, latitude, longitude in hashtable[lat - 1][long - 1]:\n",
    "                distance_km = dist(point_na, (latitude, longitude))\n",
    "                if not np.isnan(df.loc[inx][naVar]) and distance_km != 0:\n",
    "                    distances.append(distance_km)\n",
    "                    if distance_km in neighbors.keys():\n",
    "                        neighbors[distance_km].append(inx)\n",
    "                    else:\n",
    "                        neighbors[distance_km] = [inx]\n",
    "        \n",
    "        if long + 1 != len(hashtable):\n",
    "            for inx, latitude, longitude in hashtable[lat - 1][long + 1]:\n",
    "                distance_km = dist(point_na, (latitude, longitude))\n",
    "                if not np.isnan(df.loc[inx][naVar]) and distance_km != 0:\n",
    "                    distances.append(distance_km)\n",
    "                    if distance_km in neighbors.keys():\n",
    "                        neighbors[distance_km].append(inx)\n",
    "                    else:\n",
    "                        neighbors[distance_km] = [inx]\n",
    "        \n",
    "    if lat + 1 != len(hashtable):\n",
    "        \n",
    "        for inx, latitude, longitude in hashtable[lat + 1][long]:\n",
    "                distance_km = dist(point_na, (latitude, longitude))\n",
    "                if not np.isnan(df.loc[inx][naVar]) and distance_km != 0:\n",
    "                    distances.append(distance_km)\n",
    "                    if distance_km in neighbors.keys():\n",
    "                        neighbors[distance_km].append(inx)\n",
    "                    else:\n",
    "                        neighbors[distance_km] = [inx]\n",
    "                    \n",
    "        if long != 0:\n",
    "            for inx, latitude, longitude in hashtable[lat + 1][long - 1]:\n",
    "                distance_km = dist(point_na, (latitude, longitude))\n",
    "                if not np.isnan(df.loc[inx][naVar]) and distance_km != 0:\n",
    "                    distances.append(distance_km)\n",
    "                    if distance_km in neighbors.keys():\n",
    "                        neighbors[distance_km].append(inx)\n",
    "                    else:\n",
    "                        neighbors[distance_km] = [inx]\n",
    "        \n",
    "        if long + 1 != len(hashtable):\n",
    "            for inx, latitude, longitude in hashtable[lat + 1][long + 1]:\n",
    "                distance_km = dist(point_na, (latitude, longitude))\n",
    "                if not np.isnan(df.loc[inx][naVar]) and distance_km != 0:\n",
    "                    distances.append(distance_km)\n",
    "                    if distance_km in neighbors.keys():\n",
    "                        neighbors[distance_km].append(inx)\n",
    "                    else:\n",
    "                        neighbors[distance_km] = [inx]\n",
    "        \n",
    "    if long != 0:\n",
    "        for inx, latitude, longitude in hashtable[lat][long - 1]:\n",
    "            distance_km = dist(point_na, (latitude, longitude))\n",
    "            if not np.isnan(df.loc[inx][naVar]) and distance_km != 0:\n",
    "                distances.append(distance_km)\n",
    "                if distance_km in neighbors.keys():\n",
    "                    neighbors[distance_km].append(inx)\n",
    "                else:\n",
    "                    neighbors[distance_km] = [inx]\n",
    "        \n",
    "    if long + 1 != len(hashtable):\n",
    "        for inx, latitude, longitude in hashtable[lat][long + 1]:\n",
    "            distance_km = dist(point_na, (latitude, longitude))\n",
    "            if not np.isnan(df.loc[inx][naVar]) and distance_km != 0:\n",
    "                distances.append(distance_km)\n",
    "                if distance_km in neighbors.keys():\n",
    "                    neighbors[distance_km].append(inx)\n",
    "                else:\n",
    "                    neighbors[distance_km] = [inx]\n",
    "                    \n",
    "    #Possible bug with neighbor dictionary for k > 2\n",
    "    distances.sort()\n",
    "    distances = distances[0:k]\n",
    "    for distance_km in distances:\n",
    "        for inx in neighbors[distance_km]:\n",
    "            neighbor_indices.append(inx)\n",
    "    neighbor_indices = neighbor_indices[0:k]\n",
    "    if len(neighbor_indices) < k and len(neighbor_indices) != 0:\n",
    "        print(\"INTERPOLATING WITH \" + str(len(neighbor_indices)) + \" POINTS INSTEAD OF \" + str(k) + \" POINTS\")\n",
    "    if len(neighbor_indices) >= 2:\n",
    "        return (distances, neighbor_indices)\n",
    "    \n",
    "    #Possible bug with neighbor dictionary for k > 2\n",
    "    distances = []\n",
    "    neighbors = {}\n",
    "    neighbor_indices = []\n",
    "    for inx, row in df.iterrows():\n",
    "        distance_km = dist(point_na, (row['LATITUDE'], row['LONGITUDE']))\n",
    "        if not np.isnan(df.loc[inx][naVar]) and distance_km != 0:\n",
    "            distances.append(distance_km)\n",
    "            if distance_km in neighbors.keys():\n",
    "                neighbors[distance_km].append(inx)\n",
    "            else:\n",
    "                neighbors[distance_km] = [inx]\n",
    "    distances.sort()\n",
    "    distances = distances[0:k]\n",
    "    for distance_km in distances:\n",
    "        for inx in neighbors[distance_km]:\n",
    "            neighbor_indices.append(inx)\n",
    "    neighbor_indices = neighbor_indices[0:k]\n",
    "    return (distances, neighbor_indices)            \n",
    "    \n",
    "\"\"\"\n",
    "Params:\n",
    "df = dataframe\n",
    "distances = distances of the points being used to interpolate for the missing value \n",
    "neighbors  = the points for prediction\n",
    "naVar = variable to predict\n",
    "\n",
    "Returns the interpolated data value for a missing datapoint.\n",
    "\"\"\"\n",
    "def interpolate(df, distances, neighbors, naVar):\n",
    "    result = 0\n",
    "    denominator = [1 / x for x in distances]\n",
    "    denominator = sum(denominator)\n",
    "    for i in range(len(distances)):\n",
    "        result += ((1/distances[i]) / denominator) * df.loc[neighbors[i]][naVar]\n",
    "    return result\n",
    "print(\"Functions have been loaded\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ahead-traveler",
   "metadata": {},
   "source": [
    "# Season by Season Interpolation\n",
    "Here, we begin the process of interpolating for missing data based upon the season. To do this, the dataframe we input needs a season column. To obtain this, we create a copy of the dataframe, and use this copy throughout the rest of the work. To obtain the season, we utilize the date recorded. We create a column for the particular month, and then use a dictionary to replace that value with the appropriate season."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "commercial-snapshot",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>SHEETBAR</th>\n",
       "      <th>DATE</th>\n",
       "      <th>LATITUDE</th>\n",
       "      <th>LONGITUDE</th>\n",
       "      <th>FLDNUM</th>\n",
       "      <th>STRATUM</th>\n",
       "      <th>LOCATCD</th>\n",
       "      <th>TN</th>\n",
       "      <th>TP</th>\n",
       "      <th>TEMP</th>\n",
       "      <th>DO</th>\n",
       "      <th>TURB</th>\n",
       "      <th>COND</th>\n",
       "      <th>VEL</th>\n",
       "      <th>SS</th>\n",
       "      <th>WDP</th>\n",
       "      <th>CHLcal</th>\n",
       "      <th>SECCHI</th>\n",
       "      <th>MONTH</th>\n",
       "      <th>YEAR</th>\n",
       "      <th>SEASON</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>44000159</td>\n",
       "      <td>10/19/1993</td>\n",
       "      <td>38.873994</td>\n",
       "      <td>-90.189663</td>\n",
       "      <td>4</td>\n",
       "      <td>5</td>\n",
       "      <td>9343058</td>\n",
       "      <td>2.769</td>\n",
       "      <td>0.145</td>\n",
       "      <td>15.7</td>\n",
       "      <td>8.5</td>\n",
       "      <td>37.0</td>\n",
       "      <td>424.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>40.3</td>\n",
       "      <td>0.5</td>\n",
       "      <td>24.29702</td>\n",
       "      <td>27.0</td>\n",
       "      <td>10</td>\n",
       "      <td>1993</td>\n",
       "      <td>FALL</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>44000160</td>\n",
       "      <td>10/19/1993</td>\n",
       "      <td>38.873772</td>\n",
       "      <td>-90.180452</td>\n",
       "      <td>4</td>\n",
       "      <td>5</td>\n",
       "      <td>9343059</td>\n",
       "      <td>3.049</td>\n",
       "      <td>0.146</td>\n",
       "      <td>15.7</td>\n",
       "      <td>8.3</td>\n",
       "      <td>43.0</td>\n",
       "      <td>424.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>48.6</td>\n",
       "      <td>1.1</td>\n",
       "      <td>26.64710</td>\n",
       "      <td>30.0</td>\n",
       "      <td>10</td>\n",
       "      <td>1993</td>\n",
       "      <td>FALL</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>44000161</td>\n",
       "      <td>10/19/1993</td>\n",
       "      <td>38.869837</td>\n",
       "      <td>-90.166778</td>\n",
       "      <td>4</td>\n",
       "      <td>5</td>\n",
       "      <td>9343063</td>\n",
       "      <td>3.267</td>\n",
       "      <td>0.158</td>\n",
       "      <td>15.2</td>\n",
       "      <td>8.2</td>\n",
       "      <td>39.0</td>\n",
       "      <td>444.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>36.5</td>\n",
       "      <td>0.7</td>\n",
       "      <td>32.82694</td>\n",
       "      <td>32.0</td>\n",
       "      <td>10</td>\n",
       "      <td>1993</td>\n",
       "      <td>FALL</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>44000162</td>\n",
       "      <td>10/19/1993</td>\n",
       "      <td>38.864269</td>\n",
       "      <td>-90.160085</td>\n",
       "      <td>4</td>\n",
       "      <td>5</td>\n",
       "      <td>9343065</td>\n",
       "      <td>3.345</td>\n",
       "      <td>0.161</td>\n",
       "      <td>15.2</td>\n",
       "      <td>8.4</td>\n",
       "      <td>37.0</td>\n",
       "      <td>456.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>43.3</td>\n",
       "      <td>1.5</td>\n",
       "      <td>42.57542</td>\n",
       "      <td>28.0</td>\n",
       "      <td>10</td>\n",
       "      <td>1993</td>\n",
       "      <td>FALL</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>44000163</td>\n",
       "      <td>10/19/1993</td>\n",
       "      <td>38.877205</td>\n",
       "      <td>-90.173401</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>9343010</td>\n",
       "      <td>3.661</td>\n",
       "      <td>0.183</td>\n",
       "      <td>14.9</td>\n",
       "      <td>9.1</td>\n",
       "      <td>54.0</td>\n",
       "      <td>457.0</td>\n",
       "      <td>0.55</td>\n",
       "      <td>79.3</td>\n",
       "      <td>10.4</td>\n",
       "      <td>68.25222</td>\n",
       "      <td>26.0</td>\n",
       "      <td>10</td>\n",
       "      <td>1993</td>\n",
       "      <td>FALL</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   SHEETBAR        DATE   LATITUDE  LONGITUDE  FLDNUM  STRATUM  LOCATCD  \\\n",
       "0  44000159  10/19/1993  38.873994 -90.189663       4        5  9343058   \n",
       "1  44000160  10/19/1993  38.873772 -90.180452       4        5  9343059   \n",
       "2  44000161  10/19/1993  38.869837 -90.166778       4        5  9343063   \n",
       "3  44000162  10/19/1993  38.864269 -90.160085       4        5  9343065   \n",
       "4  44000163  10/19/1993  38.877205 -90.173401       4        1  9343010   \n",
       "\n",
       "      TN     TP  TEMP   DO  TURB   COND   VEL    SS   WDP    CHLcal  SECCHI  \\\n",
       "0  2.769  0.145  15.7  8.5  37.0  424.0  0.00  40.3   0.5  24.29702    27.0   \n",
       "1  3.049  0.146  15.7  8.3  43.0  424.0  0.00  48.6   1.1  26.64710    30.0   \n",
       "2  3.267  0.158  15.2  8.2  39.0  444.0  0.00  36.5   0.7  32.82694    32.0   \n",
       "3  3.345  0.161  15.2  8.4  37.0  456.0  0.00  43.3   1.5  42.57542    28.0   \n",
       "4  3.661  0.183  14.9  9.1  54.0  457.0  0.55  79.3  10.4  68.25222    26.0   \n",
       "\n",
       "   MONTH  YEAR SEASON  \n",
       "0     10  1993   FALL  \n",
       "1     10  1993   FALL  \n",
       "2     10  1993   FALL  \n",
       "3     10  1993   FALL  \n",
       "4     10  1993   FALL  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = dataFrame.copy()\n",
    "df[\"MONTH\"] = pd.DatetimeIndex(dataFrame[\"DATE\"]).month\n",
    "df[\"YEAR\"] = pd.DatetimeIndex(dataFrame[\"DATE\"]).year\n",
    "df[\"SEASON\"] = df[\"MONTH\"]\n",
    "seasons = {3 : 'SPRING',\n",
    "           4 : 'SPRING',\n",
    "           5 : 'SPRING',\n",
    "           6 : 'SUMMER',\n",
    "           7 : 'SUMMER',\n",
    "           8 : 'SUMMER',\n",
    "           9 : 'FALL',\n",
    "           10 : 'FALL',\n",
    "           11: 'FALL',\n",
    "           12: 'WINTER',\n",
    "           1: 'WINTER',\n",
    "           2: 'WINTER'}\n",
    "df = df.replace({\"SEASON\" : seasons})\n",
    "# for index, row in newData.iterrows():\n",
    "#     newData.loc[index, 'SEASON'] = seasons\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "equivalent-cylinder",
   "metadata": {},
   "source": [
    "# Prediction Group by Season and several years\n",
    "\n",
    "Here, we interpolate missing data values for a certain year, grouping data by season. For example, we can use spring data from 2001, 2002, and 2003 to predict spring data for 2002. In the list below, `x` represents the year we predict. Modify the lower bound to be the earliest year of data that you have.\n",
    "\n",
    "Then, we create a year column on the data frame to allow the predict function to get the correct year for prediction purposes. Then, we interpolate data values year by year, season by season. After it is done, it sends the output to a `.csv` file, so modify the path as necessary. The `if (seasonFrame.shape[0] > 1)` is a check to make sure there is enough data present to do any interpolating, since we use `2` points to predict a missing third here.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "subsequent-giving",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "continuous = ['TN','TP','TEMP','DO','TURB','COND','VEL','SS','WDP','CHLcal','SECCHI'] # variables to interpolate\n",
    "years = [[x-1, x, x+1] for x in range(1995, 2021)] # start and end years from LTRM dataset\n",
    "newData[\"YEAR\"] = pd.DatetimeIndex(dataFrame[\"DATE\"]).year\n",
    "result = pd.DataFrame()\n",
    "for setOfYears in years:\n",
    "    print(\"Set of years: \" + str(setOfYears))\n",
    "    print(\"Year to interpolate missing data: \" + str(setOfYears[1]))\n",
    "    threeYearFrame = newData[newData['YEAR'].isin(setOfYears)]\n",
    "    seasons = ['SPRING','SUMMER','FALL','WINTER']\n",
    "    for season in seasons:\n",
    "        print(season)\n",
    "        seasonalFrame = threeYearFrame[threeYearFrame['SEASON'] == season]\n",
    "        if (seasonalFrame.shape[0] > 1):\n",
    "            seasonalHash = construct_hashtable(seasonalFrame)\n",
    "            for var in continuous:\n",
    "                predict_years(seasonalFrame, seasonalHash, var,setOfYears[1],2)\n",
    "            \n",
    "            yearToAdd = seasonalFrame[seasonalFrame['YEAR'] == setOfYears[1]]\n",
    "            result = result.append(yearToAdd, ignore_index = True)\n",
    "            result = result.reset_index(drop = True)\n",
    "            \n",
    "            \n",
    "    print(\"Predicted for \" + str(setOfYears[1]))\n",
    "    \n",
    "result.to_csv(r\"..\\pools_specific_EDA\\Open River\\allvars_interpolated_3yearsxseason.csv\") # can change outputs\n",
    "print(\"Done\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fifth-pixel",
   "metadata": {},
   "source": [
    "# Prediction group year by year\n",
    "\n",
    "Here, we do predictions for missing data for a given year using the entire year's worth of data. As such, we use the `predict` function instead of the `predict_years` function. Here as well, we create a year column from the date in the data frame, which we then use to generate a list of years to predict data for. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "automated-province",
   "metadata": {},
   "outputs": [],
   "source": [
    "newData = dataFrame.copy()\n",
    "newData[\"YEAR\"] = pd.DatetimeIndex(dataFrame[\"DATE\"]).year\n",
    "years = newData[\"YEAR\"].unique()\n",
    "result = pd.DataFrame()\n",
    "for year in years:\n",
    "    currentSet = newData[(newData[\"YEAR\"] == year)]\n",
    "    currentSet = currentSet.reset_index(drop = True)\n",
    "    hashTable = construct_hashtable(currentSet)\n",
    "    predict(currentSet, hashTable, \"TN\",2)\n",
    "    predict(currentSet, hashTable, \"TP\",2)\n",
    "    print(\"Predicted for \" + str(year))\n",
    "    result = result.append(currentSet)\n",
    "\n",
    "result = result.reset_index(drop = True)\n",
    "result.to_csv(r\"C:\\Users\\forre\\Desktop\\REU\\TDA\\Data\\predicted_tn_tp_years.csv\") # can change\n",
    "print(\"Done\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
