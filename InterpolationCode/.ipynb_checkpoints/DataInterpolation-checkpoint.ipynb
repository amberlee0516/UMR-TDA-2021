{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "intermediate-tonight",
   "metadata": {},
   "source": [
    "# Imports\n",
    "This the script for prepossing the data to create data ready for the TDA mapper algorithm."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "literary-swaziland",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import pandas as pd\n",
    "from geopy import distance\n",
    "import math\n",
    "pd.set_option('display.max_columns', None)\n",
    "import numpy as np\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.preprocessing import RobustScaler,StandardScaler\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "print(\"imports done\")\n",
    "pd.set_option(\"display.max_rows\", None)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "comprehensive-picnic",
   "metadata": {},
   "source": [
    "# File\n",
    "From the resulting `R` script written by Amber and `python` script written by Alaina, grab the cleaned data file. The `R` file can be found in the github repository and is called `water cleaning data.Rmd` and is in the WaterCleaning folder. Secondly, run it through the `python` script called `Data_Collapse.ipynb`. From here, this will result in the `cleaned_data.csv` file. This can found in the github repo as well. Download it, and edit the file path if necessary to read in the dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "regular-waters",
   "metadata": {},
   "outputs": [],
   "source": [
    "filePath = r\"..\\LTRM data\\water_data_qfneg.csv\"\n",
    "dataFrame = pd.read_csv(filePath, low_memory = False)\n",
    "print(\"dataFrame Made\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "massive-observer",
   "metadata": {},
   "source": [
    "# Filter for your pool\n",
    "Resets the index as well. This is done through the `FLDNUM` parameter, and can be found in accompanying documentation for the correct pool number. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "different-judges",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataFrame = dataFrame[dataFrame['FLDNUM'] == 4]\n",
    "print(dataFrame.shape)\n",
    "dataFrame  = dataFrame.reset_index(drop = True)\n",
    "dataFrame.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "color-settle",
   "metadata": {},
   "source": [
    "# Functions\n",
    "Here, we interpolate for missing data values. These occur when the data set has a missing value. The way it is computed utilizes a $k$-nearest neighbors algorithm. A weighted average using the $k$ nearest points is used to compute the missing value, and it appends it to a new column in the data set called `\"PREDICTED_\" + variable`, where `variable` is what we wish to interpolate (`TN` or `TP`) for example."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "universal-reference",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Params:\n",
    "df = the dataframe filtered for the pool\n",
    "hashtable = the hash table of distances of each point for the data frame. (Created from construct_hashtable)\n",
    "naVar = the variable we wish to interpolate\n",
    "year = the year we wish to predict for\n",
    "k = the number of terms in the weighted average for interpolation\n",
    "\n",
    "NOTE: for now, set k = 2 due to potential bug for larger k\n",
    "\n",
    "This is one of two predict functions. \n",
    "Here, multiple years worth of data can put in, and only the specified year will be predicted\n",
    "and added to the dataframe. Note that this function will find the k nearnest neighbors using df,\n",
    "regardless of year.\n",
    "\"\"\"\n",
    "def predict_years(df, hashtable, naVar, year, k):\n",
    "    df[\"PREDICTED_\" + naVar] = df[naVar]\n",
    "    df_year = df.copy()\n",
    "    df_year = df_year[df_year[\"YEAR\"] == year]\n",
    "    naIndices = df_year[(df_year[naVar].isnull())]\n",
    "    print(\"For \" + naVar + \" we will interpolate \" + str(len(naIndices)) + \" points.\")\n",
    "    for index, row in naIndices.iterrows():\n",
    "        distances, neighbors = k_nearest_neighbors(df, index, naVar, hashtable, k)\n",
    "        df.loc[index, \"PREDICTED_\" + naVar] = interpolate(df, distances, neighbors, naVar)\n",
    "    print(naVar + \" interpolation success\")\n",
    "\"\"\"\n",
    "Params:\n",
    "df = the dataframe filtered for the pool\n",
    "hashtable = the hash table of distances of each point for the data frame.\n",
    "naVar = the variable we wish to interpolate\n",
    "k = the number of terms in the weighted average for interpolation\n",
    "\n",
    "NOTE: for now, keep k = 2 due to potential bug for k > 2\n",
    "\n",
    "This predict function is more crude than predict_years. It will predict using missing values of naVar for \n",
    "the entire dataframe, using the entire dataframe to locate the k nearnest neighbors.\n",
    "\"\"\"\n",
    "def predict(df, hashtable, naVar, k):\n",
    "    df[\"PREDICTED_\" + naVar] = df[naVar]\n",
    "    naIndices = df[(df[naVar].isnull())]\n",
    "    print(\"For \" + naVar + \" we will interpolate \" + str(len(naIndices)) + \" points.\")\n",
    "    for index, row in naIndices.iterrows():\n",
    "        distances, neighbors = k_nearest_neighbors(df, index, naVar, hashtable, k)\n",
    "        df.loc[index, \"PREDICTED_\" + naVar] = interpolate(df, distances, neighbors, naVar)\n",
    "    print(naVar + \" interpolation success\")\n",
    "\n",
    "    \n",
    "\"\"\"\n",
    "params:\n",
    "\n",
    "minimums = lower bound of data, typically latitude or longitude\n",
    "maximums = upper bound of data, typically latitude or longitude\n",
    "x = data we wish to transfrom in to [0,1], typically a latitude or a longitude\n",
    "\n",
    "This is a helper function for construct_hashtable and k_nearest_neighbors. It takes in a latitude or\n",
    "a longitude and maps to in to [0,1] so the point can be plotted properly in the hashtable\n",
    "\"\"\"    \n",
    "    \n",
    "def transform(minimum, maximum, x):\n",
    "    return (1 / (maximum - minimum) ) * (x - minimum)\n",
    "\"\"\"\n",
    "Params:\n",
    "point 1 = First point (latitude and longitude)\n",
    "point 2 = Second point (latitude and longitude)\n",
    "\n",
    "Returns the distance in kilometers between two points in space, using\n",
    "scipy distance function.\n",
    "\"\"\"\n",
    "def dist(point1, point2):\n",
    "    return distance.distance(point1, point2).km\n",
    "\"\"\"\n",
    "params:\n",
    "df = the dataframe\n",
    "\n",
    "returns: hashtable (list of lists of lists of tuples(index, latitude, longitude))\n",
    "\n",
    "Constructs a hash table of locations of points (the position where data is recorded)\n",
    "This is used in the k nearest neighbors algorithm. Locations that are near each other in space\n",
    "are near each other in the hashtable\n",
    "\n",
    "\"\"\"    \n",
    "def construct_hashtable(df):\n",
    "    #get hashtable information\n",
    "    data_length = math.sqrt(df.shape[0])\n",
    "    #print(\"data_length: \" + str(data_length))\n",
    "    interval_length = 1 / data_length\n",
    "    lat_minimum = df[[\"LATITUDE\"]].min()[0] - 1\n",
    "    lat_maximum = df[[\"LATITUDE\"]].max()[0] + 1\n",
    "    long_minimum = df[[\"LONGITUDE\"]].min()[0] - 1\n",
    "    long_maximum = df[[\"LONGITUDE\"]].max()[0] + 1\n",
    "    \n",
    "    #construct hashtable\n",
    "    hashtable = [[[] for x in range(int(data_length)+1)] for y in range(int(data_length)+1)]\n",
    "    \n",
    "    #populate hashtable\n",
    "    for index, row in df.iterrows():\n",
    "        r_lat = row['LATITUDE']\n",
    "        r_long = row['LONGITUDE']\n",
    "        lat = math.floor(transform(lat_minimum, lat_maximum, r_lat) / interval_length)\n",
    "        long = math.floor(transform(long_minimum, long_maximum, r_long) / interval_length)\n",
    "        #print(\"lat: \" + str(lat))\n",
    "        #print(\"long: \" + str(long))\n",
    "        hashtable[lat][long].append((index, r_lat, r_long))\n",
    "\n",
    "    return hashtable\n",
    "\"\"\"\n",
    "Params:\n",
    "df = dataframe\n",
    "index = index of variable we wish to find k nearest neighbors of\n",
    "naVar = variable to predict\n",
    "hashtable = data structure created from construct_hashtable\n",
    "k = number of nearest neighbors\n",
    "\n",
    "Returns: (distances, indices) of k nearnest neighbors\n",
    "\n",
    "This algorithm will find the k nearest neighbors of a desired point using the hashtable, if possible.\n",
    "If there are no valid points near the given point, then the algorithm will use brute force\n",
    "\"\"\"\n",
    "def k_nearest_neighbors(df, index, naVar, hashtable, k):\n",
    "\n",
    "    distances = []\n",
    "    neighbor_indices = []\n",
    "    neighbors = {}\n",
    "    \n",
    "    data_length = math.sqrt(df.shape[0])\n",
    "    interval_length = 1 / data_length\n",
    "    lat_minimum = df[[\"LATITUDE\"]].min()[0] - 1\n",
    "    lat_maximum = df[[\"LATITUDE\"]].max()[0] + 1\n",
    "    long_minimum = df[[\"LONGITUDE\"]].min()[0] - 1\n",
    "    long_maximum = df[[\"LONGITUDE\"]].max()[0] + 1\n",
    "    \n",
    "    \n",
    "    row_na = df.loc[index]\n",
    "    point_na = (row_na['LATITUDE'], row_na['LONGITUDE'])\n",
    "    lat = math.floor(transform(lat_minimum, lat_maximum, point_na[0]) / interval_length)\n",
    "    long = math.floor(transform(long_minimum, long_maximum, point_na[1]) / interval_length)\n",
    "    season = row_na['SEASON']\n",
    "    \n",
    "    for inx, latitude, longitude in hashtable[lat][long]:\n",
    "        distance_km = dist(point_na, (latitude, longitude))\n",
    "        if not np.isnan(df.loc[inx][naVar]) and distance_km != 0:\n",
    "            distances.append(distance_km)\n",
    "            if distance_km in neighbors.keys():\n",
    "                neighbors[distance_km].append(inx)\n",
    "            else:\n",
    "                neighbors[distance_km] = [inx]\n",
    "    \n",
    "    if lat != 0:\n",
    "        \n",
    "        for inx, latitude, longitude in hashtable[lat - 1][long]:\n",
    "                distance_km = dist(point_na, (latitude, longitude))\n",
    "                if not np.isnan(df.loc[inx][naVar]) and distance_km != 0:\n",
    "                    distances.append(distance_km)\n",
    "                    if distance_km in neighbors.keys():\n",
    "                        neighbors[distance_km].append(inx)\n",
    "                    else:\n",
    "                        neighbors[distance_km] = [inx]\n",
    "                    \n",
    "        if long != 0:\n",
    "            for inx, latitude, longitude in hashtable[lat - 1][long - 1]:\n",
    "                distance_km = dist(point_na, (latitude, longitude))\n",
    "                if not np.isnan(df.loc[inx][naVar]) and distance_km != 0:\n",
    "                    distances.append(distance_km)\n",
    "                    if distance_km in neighbors.keys():\n",
    "                        neighbors[distance_km].append(inx)\n",
    "                    else:\n",
    "                        neighbors[distance_km] = [inx]\n",
    "        \n",
    "        if long + 1 != len(hashtable):\n",
    "            for inx, latitude, longitude in hashtable[lat - 1][long + 1]:\n",
    "                distance_km = dist(point_na, (latitude, longitude))\n",
    "                if not np.isnan(df.loc[inx][naVar]) and distance_km != 0:\n",
    "                    distances.append(distance_km)\n",
    "                    if distance_km in neighbors.keys():\n",
    "                        neighbors[distance_km].append(inx)\n",
    "                    else:\n",
    "                        neighbors[distance_km] = [inx]\n",
    "        \n",
    "    if lat + 1 != len(hashtable):\n",
    "        \n",
    "        for inx, latitude, longitude in hashtable[lat + 1][long]:\n",
    "                distance_km = dist(point_na, (latitude, longitude))\n",
    "                if not np.isnan(df.loc[inx][naVar]) and distance_km != 0:\n",
    "                    distances.append(distance_km)\n",
    "                    if distance_km in neighbors.keys():\n",
    "                        neighbors[distance_km].append(inx)\n",
    "                    else:\n",
    "                        neighbors[distance_km] = [inx]\n",
    "                    \n",
    "        if long != 0:\n",
    "            for inx, latitude, longitude in hashtable[lat + 1][long - 1]:\n",
    "                distance_km = dist(point_na, (latitude, longitude))\n",
    "                if not np.isnan(df.loc[inx][naVar]) and distance_km != 0:\n",
    "                    distances.append(distance_km)\n",
    "                    if distance_km in neighbors.keys():\n",
    "                        neighbors[distance_km].append(inx)\n",
    "                    else:\n",
    "                        neighbors[distance_km] = [inx]\n",
    "        \n",
    "        if long + 1 != len(hashtable):\n",
    "            for inx, latitude, longitude in hashtable[lat + 1][long + 1]:\n",
    "                distance_km = dist(point_na, (latitude, longitude))\n",
    "                if not np.isnan(df.loc[inx][naVar]) and distance_km != 0:\n",
    "                    distances.append(distance_km)\n",
    "                    if distance_km in neighbors.keys():\n",
    "                        neighbors[distance_km].append(inx)\n",
    "                    else:\n",
    "                        neighbors[distance_km] = [inx]\n",
    "        \n",
    "    if long != 0:\n",
    "        for inx, latitude, longitude in hashtable[lat][long - 1]:\n",
    "            distance_km = dist(point_na, (latitude, longitude))\n",
    "            if not np.isnan(df.loc[inx][naVar]) and distance_km != 0:\n",
    "                distances.append(distance_km)\n",
    "                if distance_km in neighbors.keys():\n",
    "                    neighbors[distance_km].append(inx)\n",
    "                else:\n",
    "                    neighbors[distance_km] = [inx]\n",
    "        \n",
    "    if long + 1 != len(hashtable):\n",
    "        for inx, latitude, longitude in hashtable[lat][long + 1]:\n",
    "            distance_km = dist(point_na, (latitude, longitude))\n",
    "            if not np.isnan(df.loc[inx][naVar]) and distance_km != 0:\n",
    "                distances.append(distance_km)\n",
    "                if distance_km in neighbors.keys():\n",
    "                    neighbors[distance_km].append(inx)\n",
    "                else:\n",
    "                    neighbors[distance_km] = [inx]\n",
    "                    \n",
    "    #Possible bug with neighbor dictionary for k > 2\n",
    "    distances.sort()\n",
    "    distances = distances[0:k]\n",
    "    for distance_km in distances:\n",
    "        for inx in neighbors[distance_km]:\n",
    "            neighbor_indices.append(inx)\n",
    "    neighbor_indices = neighbor_indices[0:k]\n",
    "    if len(neighbor_indices) < k and len(neighbor_indices) != 0:\n",
    "        print(\"INTERPOLATING WITH \" + str(len(neighbor_indices)) + \" POINTS INSTEAD OF \" + str(k) + \" POINTS\")\n",
    "    if len(neighbor_indices) >= 2:\n",
    "        return (distances, neighbor_indices)\n",
    "    \n",
    "    #Possible bug with neighbor dictionary for k > 2\n",
    "    distances = []\n",
    "    neighbors = {}\n",
    "    neighbor_indices = []\n",
    "    for inx, row in df.iterrows():\n",
    "        distance_km = dist(point_na, (row['LATITUDE'], row['LONGITUDE']))\n",
    "        if not np.isnan(df.loc[inx][naVar]) and distance_km != 0:\n",
    "            distances.append(distance_km)\n",
    "            if distance_km in neighbors.keys():\n",
    "                neighbors[distance_km].append(inx)\n",
    "            else:\n",
    "                neighbors[distance_km] = [inx]\n",
    "    distances.sort()\n",
    "    distances = distances[0:k]\n",
    "    for distance_km in distances:\n",
    "        for inx in neighbors[distance_km]:\n",
    "            neighbor_indices.append(inx)\n",
    "    neighbor_indices = neighbor_indices[0:k]\n",
    "    return (distances, neighbor_indices)            \n",
    "    \n",
    "\"\"\"\n",
    "Params:\n",
    "df = dataframe\n",
    "distances = distances of the points being used to interpolate for the missing value \n",
    "neighbors  = the points for prediction\n",
    "naVar = variable to predict\n",
    "\n",
    "Returns the interpolated data value for a missing datapoint.\n",
    "\"\"\"\n",
    "def interpolate(df, distances, neighbors, naVar):\n",
    "    result = 0\n",
    "    denominator = [1 / x for x in distances]\n",
    "    denominator = sum(denominator)\n",
    "    for i in range(len(distances)):\n",
    "        result += ((1/distances[i]) / denominator) * df.loc[neighbors[i]][naVar]\n",
    "    return result\n",
    "print(\"Functions have been loaded\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ahead-traveler",
   "metadata": {},
   "source": [
    "# Season by Season Interpolation\n",
    "Here, we begin the process of interpolating for missing data based upon the season. To do this, the dataframe we input needs a season column. To obtain this, we create a copy of the dataframe, and use this copy throughout the rest of the work. To obtain the season, we utilize the date recorded to get the season. We create a column for the particular month, and then use a dictionary to replace that value with the appropriate season."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "commercial-snapshot",
   "metadata": {},
   "outputs": [],
   "source": [
    "newData = dataFrame.copy()\n",
    "newData[\"MONTH\"] = pd.DatetimeIndex(dataFrame[\"DATE\"]).month\n",
    "newData[\"SEASON\"] = newData[\"MONTH\"]\n",
    "seasons = {3 : 'SPRING',\n",
    "           4 : 'SPRING',\n",
    "           5 : 'SPRING',\n",
    "           6 : 'SUMMER',\n",
    "           7 : 'SUMMER',\n",
    "           8 : 'SUMMER',\n",
    "           9 : 'FALL',\n",
    "           10 : 'FALL',\n",
    "           11: 'FALL',\n",
    "           12: 'WINTER',\n",
    "           1: 'WINTER',\n",
    "           2: 'WINTER'}\n",
    "newData = newData.replace({\"SEASON\" : seasons})\n",
    "# for index, row in newData.iterrows():\n",
    "#     newData.loc[index, 'SEASON'] = seasons\n",
    "newData.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "equivalent-cylinder",
   "metadata": {},
   "source": [
    "# Prediction Group by Season and several years\n",
    "\n",
    "Here, we interpolate missing data values for a certain year, grouping data by season. For example, we can use spring data from 2001, 2002, and 2003 to predict spring data for 2002. In the list below, `x` represents the year we predict. Modify the lower bound to be the earliest year of data that you have.\n",
    "\n",
    "Then, we create a year column on the data frame to allow the predict function to get the correct year for prediction purposes. Then, we interpolate data values year by year, season by season. After it is done, it sends the output to a `.csv` file, so modify the path as necessary. The `if (seasonFrame.shape[0] > 1)` is a check to make sure there is enough data present to do any interpolating, since we use `2` points to predict a missing third here.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "subsequent-giving",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "continuous = ['TN','TP','TEMP','DO','TURB','COND','VEL','SS','WDP','CHLcal','SECCHI']\n",
    "years = [[x-1, x, x+1] for x in range(1995, 2021)]\n",
    "newData[\"YEAR\"] = pd.DatetimeIndex(dataFrame[\"DATE\"]).year\n",
    "result = pd.DataFrame()\n",
    "for setOfYears in years:\n",
    "    print(\"Set of years: \" + str(setOfYears))\n",
    "    print(\"Year to interpolate missing data: \" + str(setOfYears[1]))\n",
    "    threeYearFrame = newData[newData['YEAR'].isin(setOfYears)]\n",
    "    seasons = ['SPRING','SUMMER','FALL','WINTER']\n",
    "    for season in seasons:\n",
    "        print(season)\n",
    "        seasonalFrame = threeYearFrame[threeYearFrame['SEASON'] == season]\n",
    "        if (seasonalFrame.shape[0] > 1):\n",
    "            seasonalHash = construct_hashtable(seasonalFrame)\n",
    "            for var in continuous:\n",
    "                predict_years(seasonalFrame, seasonalHash, var,setOfYears[1],2)\n",
    "            \n",
    "            yearToAdd = seasonalFrame[seasonalFrame['YEAR'] == setOfYears[1]]\n",
    "            result = result.append(yearToAdd, ignore_index = True)\n",
    "            result = result.reset_index(drop = True)\n",
    "            \n",
    "            \n",
    "    print(\"Predicted for \" + str(setOfYears[1]))\n",
    "    \n",
    "result.to_csv(r\"..\\pools_specific_EDA\\Open River\\allvars_interpolated_3yearsxseason.csv\")\n",
    "print(\"Done\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fifth-pixel",
   "metadata": {},
   "source": [
    "# Prediction group year by year\n",
    "\n",
    "Here, we do predictions for missing data for a given year using the entire year's worth of data. As such, we use the `predict` function instead of the `predict_years` function. Here as well, we create a year column from the date in the data frame, which we then use to generate a list of years to predict data for. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "automated-province",
   "metadata": {},
   "outputs": [],
   "source": [
    "newData = dataFrame.copy()\n",
    "newData[\"YEAR\"] = pd.DatetimeIndex(dataFrame[\"DATE\"]).year\n",
    "years = newData[\"YEAR\"].unique()\n",
    "result = pd.DataFrame()\n",
    "for year in years:\n",
    "    currentSet = newData[(newData[\"YEAR\"] == year)]\n",
    "    currentSet = currentSet.reset_index(drop = True)\n",
    "    hashTable = construct_hashtable(currentSet)\n",
    "    predict(currentSet, hashTable, \"TN\",2)\n",
    "    predict(currentSet, hashTable, \"TP\",2)\n",
    "    print(\"Predicted for \" + str(year))\n",
    "    result = result.append(currentSet)\n",
    "\n",
    "result = result.reset_index(drop = True)\n",
    "result.to_csv(r\"C:\\Users\\forre\\Desktop\\REU\\TDA\\Data\\predicted_tn_tp_years.csv\")\n",
    "print(\"Done\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "formal-rendering",
   "metadata": {},
   "source": [
    "# Spacial interpolation by year, by season (Casey)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "undefined-subcommittee",
   "metadata": {},
   "source": [
    "Load functions - parameter definitons and correct documentation still needs to be completed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "australian-rating",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "from geopy import distance\n",
    "import time\n",
    "import pickle\n",
    "from sklearn.model_selection import cross_val_score,train_test_split\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.preprocessing import RobustScaler,PolynomialFeatures\n",
    "import sklearn.metrics\n",
    "pd.set_option('display.max_columns', None)\n",
    "\n",
    "# useful functions and classes\n",
    "\n",
    "# This class stores the latitude and longitude of a sample, and indicates \n",
    "# if this location has the desired variable we are estimating\n",
    "class Location:\n",
    "    def __init__(self,latitude,longitude,hasv,ID,value,SHEET):\n",
    "        self.ID = ID\n",
    "        self.SHEET = SHEET\n",
    "        self.latitude = latitude\n",
    "        self.longitude = longitude\n",
    "        self.hasv = hasv\n",
    "        self.value = value\n",
    "        \n",
    "    def __str__(self):\n",
    "        return str(self.ID)\n",
    "\n",
    "# Calculates the distance between 2 samples in km\n",
    "def getdist(S1,S2):\n",
    "    # radius of earth in km\n",
    "    coords_1 = (S1.latitude, S1.longitude)\n",
    "    coords_2 = (S2.latitude, S2.longitude)\n",
    "    dist = distance.distance(coords_1, coords_2).km\n",
    "    return dist\n",
    "\n",
    "\n",
    "# PRE: all locations in the dataframe are\n",
    "# unique\n",
    "def DistanceMatrix(dataframe,variable):\n",
    "    # numunique = len(dataframe[\"LOCATCD\"].unique())\n",
    "    # numlocations = dataframe.shape[0]\n",
    "    # try:\n",
    "    #     assert(numunique == numlocations), f\"{numunique} unique locations but {numlocations} number of locations\"\n",
    "    \n",
    "    # except AssertionError as msg:\n",
    "    #     print(dataframe[dataframe[\"LOCATCD\"].duplicated(keep=False)])\n",
    "    #     print(msg)\n",
    "        \n",
    "    # the list of location objects\n",
    "    locations = []\n",
    "    # the list of indexes where the the row is located in the dataframe\n",
    "    #indexes = []\n",
    "    for index,row in dataframe.iterrows():\n",
    "        # make a location object on this row\n",
    "        if pd.isnull(row[variable]):\n",
    "            hasv = False\n",
    "        else:\n",
    "            hasv = True\n",
    "        locations.append(Location(row[\"LATITUDE\"],row[\"LONGITUDE\"],hasv,row[\"LOCATCD\"],row[variable],row[\"SHEETBAR\"]))\n",
    "        #indexes.append(index)\n",
    "        \n",
    "    matrix = pd.DataFrame(0,index=locations,columns=locations)\n",
    "    for ci,column in enumerate(locations):\n",
    "        for ri,row in enumerate(locations):\n",
    "            if ri>ci:\n",
    "                # compute distance between column and row\n",
    "                dist = getdist(row,column)\n",
    "            elif ci>ri:\n",
    "                dist = matrix.iloc[ci,ri]\n",
    "            # put this distance in the dataframe\n",
    "            else:\n",
    "                continue\n",
    "            matrix.iloc[ri,ci] = dist\n",
    "    return matrix\n",
    "\n",
    "def changeVar(DM,dataframe,variable):\n",
    "    locations = DM.index\n",
    "    # loop through each location\n",
    "    for i,loc in enumerate(locations):\n",
    "        SHEET = loc.SHEET\n",
    "        row = dataframe.loc[dataframe[\"SHEETBAR\"]==SHEET]\n",
    "        #print(row)\n",
    "        #print(row.shape)\n",
    "        #print(row.loc[row.index[0],variable])\n",
    "        #print(type(row[variable]))\n",
    "        try:\n",
    "            assert(row.shape[0]==1), \"Multiple rows with same SHEETBAR\"\n",
    "        except AssertionError as msg:\n",
    "            print(dataframe[dataframe[\"SHEETBAR\"].duplicated(keep=False)])\n",
    "            print(msg)\n",
    "            \n",
    "        # Pull value of desired variable\n",
    "        val = row.loc[row.index[0],variable]\n",
    "        if pd.isnull(val):\n",
    "            locations[i].hasv = False\n",
    "            locations[i].value = None\n",
    "        else:\n",
    "            locations[i].hasv = True\n",
    "            locations[i].value = val\n",
    "            \n",
    "    DM.index = locations\n",
    "    DM.columns = locations\n",
    "        \n",
    "def getclosest(numclosest,distancematrix,location):\n",
    "    column = distancematrix.loc[:,location].copy()\n",
    "    #print(type(distancematrix.index[0]))\n",
    "    # Filter the locations that dont have the desired variable\n",
    "    doesnthavev = []\n",
    "    for i in range(len(column)):\n",
    "        if not column.index[i].hasv:\n",
    "            doesnthavev.append(column.index[i])\n",
    "            \n",
    "    # Get rid of locations that dont have the desired variable\n",
    "    column.drop(doesnthavev,inplace = True)\n",
    "    # Get rid of the location we are predicting for if it exists\n",
    "    column.drop(location,inplace = True,errors=\"ignore\")\n",
    "    #print(type(column))\n",
    "    column.sort_values(inplace = True)\n",
    "    \n",
    "    return column.iloc[0:numclosest]\n",
    "\n",
    "# Key: Location Codes that need predicting\n",
    "# Value: List of tuples (locatcd,distance,value)\n",
    "def makeDict(DM,numclosest,testing):\n",
    "    # Loop through each location without a value for variable\n",
    "    closestDict = {}\n",
    "    for loc in DM.columns:\n",
    "        if not loc.hasv or testing:\n",
    "            # Get the closest locations to loc THAT ISN'T LOC\n",
    "            closest = getclosest(numclosest,DM,loc)\n",
    "            # The list of tuples that contain location id, the distance, and the value for variable\n",
    "            tuples = []\n",
    "            for i,dist in enumerate(closest):\n",
    "                SHEET = closest.index[i].SHEET\n",
    "                val = closest.index[i].value\n",
    "                tuples.append((SHEET,dist,val))\n",
    "            closestDict[loc.SHEET] = tuples\n",
    "    return closestDict\n",
    "\n",
    "def predict(tuples,numclosest = 2):\n",
    "    loc2 = tuples[0]\n",
    "    loc3 = tuples[1]\n",
    "    d12 = loc2[1]\n",
    "    val2 = loc2[2]\n",
    "    d13 = loc3[1]\n",
    "    val3 = loc3[2]\n",
    "    \n",
    "    if d12 == d13:\n",
    "        return 0.5*d12+0.5*d13\n",
    "    elif d12 == 0:\n",
    "        return val2\n",
    "    elif d13 == 0:\n",
    "        return val3\n",
    "    \n",
    "    else:\n",
    "        c2 = d12/(d12+d13)\n",
    "        c3 = d13/(d12+d13)\n",
    "        \n",
    "        predicted = c2*val2+c3*val3\n",
    "    \n",
    "        return predicted\n",
    "\n",
    "      \n",
    "'''\n",
    "data - the pandas dataframe that is ready to interpolate missing values\n",
    "MUST HAVE \"LATITUDE\", \"LONGITUDE\",\"YEAR\", \"TIME CODE\", \"LOCATCD\" columns\n",
    "\n",
    "missing_vars - the list of column names (as strings) of the dataframe that we should attempt to fill in\n",
    "\n",
    "numlocations - the number of locations used to predict the new value, default is 2 (currently the only option implemented)\n",
    "\n",
    "RETURN - a dataframe with extra columns saying the predicted values of the missing_vars\n",
    "'''\n",
    "def linear_interpolate(data,missing_vars,numlocations = 2,testing = False,verbosity = 0):\n",
    "    \n",
    "    print(\"Building a new dataframe with predicted values\")\n",
    "    start_time = time.time()\n",
    "    # Testing for duplicated locations if needed\n",
    "    #s = qualdata_noprediction[\"LOCATCD\"].duplicated(keep=False)\n",
    "    # get the years and timecodes for this dataset\n",
    "    # predictions can only be made if the point is in the same year and time code (what if we don't need to do this)\n",
    "    years = data[\"YEAR\"].unique()\n",
    "    seasons = data[\"SEASON\"].unique()\n",
    "    pools = data[\"FLDNUM\"].unique()\n",
    "    data_prediction = pd.DataFrame()\n",
    "    for pool in pools:\n",
    "        for year in years:\n",
    "            for season in seasons:\n",
    "                if verbosity > 0:\n",
    "                    print(f\"Appending predicted data for {year}  {season}  FLDNUM {pool}\")\n",
    "                # curset is the current set of rows we are predicting for\n",
    "                curset = data[(data[\"YEAR\"]==year) & (data[\"SEASON\"]==season) & (data[\"FLDNUM\"]==pool)].copy()\n",
    "                \n",
    "                if verbosity > 1:\n",
    "                    print(\"Size of this year and season:\", curset.shape)\n",
    "                \n",
    "                # Boolean to indicate if variable in Distance matrix needs updating\n",
    "                first = True\n",
    "                for var in missing_vars:\n",
    "                    newcolumn = \"PREDICTED_\"+var\n",
    "                    curset[newcolumn] = 0\n",
    "                \n",
    "                    #check to see if there are enough valid locations\n",
    "                    # that can be used to predict\n",
    "                    if not testing:\n",
    "                        bad = bool((curset[var].notnull().sum()<numlocations))\n",
    "                    else:\n",
    "                        bad = bool((curset[var].notnull().sum()<numlocations+1))\n",
    "                    \n",
    "    \n",
    "                    if(bad):\n",
    "                        if verbosity > 2:\n",
    "                            print(\"Less than \"+str(numlocations)+\" locations have \"+var+\" in this set, dropping rows without \"+var)\n",
    "                        curset = curset[curset[var].notnull()]\n",
    "                        curset[newcolumn] = curset[var]\n",
    "                        if verbosity > 2:\n",
    "                            print(\"Current set is now \",curset.shape)\n",
    "                    else:\n",
    "                        if first:\n",
    "                            if verbosity > 2:\n",
    "                                print(\"Creating DM with \",var)\n",
    "                            DM = DistanceMatrix(curset,var)\n",
    "                            first = False\n",
    "                        else:\n",
    "                            if verbosity > 2:\n",
    "                                print(\"Changing to \",var)\n",
    "                            changeVar(DM,curset,var)\n",
    "                            \n",
    "                        # Returns a dictionary mapping each location code to a tuple with prediction information\n",
    "                        Dict = makeDict(DM,numlocations,testing)\n",
    "                        \n",
    "                        #put in predicted variable\n",
    "                        for index,row in curset.iterrows():\n",
    "                            if pd.isnull(row[var]) or testing:\n",
    "                                try:\n",
    "                                    prediction = predict(Dict[row[\"SHEETBAR\"]])\n",
    "                                    #print(curset.loc[index,newcolumn],prediction)\n",
    "                                    curset.loc[index,newcolumn] = prediction\n",
    "                                except ZeroDivisionError:\n",
    "                                    print(\"Couldn't predict for \", str(row[\"SHEETBAR\"]))\n",
    "                                    print(Dict[row[\"SHEETBAR\"]])\n",
    "                                    curset.loc[index,newcolumn] = None\n",
    "                            else:\n",
    "                                curset.loc[index,newcolumn] = row[var]\n",
    "    \n",
    "                data_prediction = data_prediction.append(curset,ignore_index=True)  \n",
    "    \n",
    "    if verbosity > 0:\n",
    "        print(\"Final data set size is \",data_prediction.shape)\n",
    "    print(f\"Interpolating took {(time.time()-start_time)/60} minutes\")\n",
    "    return data_prediction"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "protected-dallas",
   "metadata": {},
   "source": [
    "#### Interpolating missing data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "breeding-fiber",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "missing_vars = ['TN','TP','TEMP','DO','TURB','COND','VEL','SS','WDP','CHLcal','SECCHI']\n",
    "water_interpolated = linear_interpolate(newData,missing_vars)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "conscious-jaguar",
   "metadata": {},
   "source": [
    "# Multivariate Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "sustainable-comparison",
   "metadata": {},
   "source": [
    "Filter for non missing data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "acceptable-moment",
   "metadata": {},
   "outputs": [],
   "source": [
    "cols = ['TP','TN','CHLcal','SS','VEL','DO','COND','WDP','TURB','TEMP','SECCHI']\n",
    "print(\"Filtering out all rows with missing data\")\n",
    "qualdata = water_data.dropna(axis=0, how='any', thresh=None, subset=cols, inplace=False).copy()\n",
    "print(qualdata.shape)\n",
    "print(\"Filtering out colums that we dont need\")\n",
    "qualdata.drop(qualdata.columns.difference(cols), 1, inplace=True)\n",
    "print(qualdata.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "thick-elephant",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model Hyperparameter: Set the degree of the polynomial to fit\n",
    "d = 5\n",
    "\n",
    "\n",
    "X = np.array(qualdata[['CHLcal','SS','VEL','DO','COND','WDP','TURB','TEMP','SECCHI']])\n",
    "TP = np.array(qualdata['TP'])\n",
    "TN = np.array(qualdata['TN'])\n",
    "\n",
    "# Good idea to standardize predictor attributes - assumes each variable has a decently normal distribution\n",
    "scaler = RobustScaler().fit(X)\n",
    "X_standard = scaler.transform(X)\n",
    "\n",
    "\n",
    "# The PolynomialFeatures class in sklearn.preprocessing can be used to transform a data matrix by\n",
    "# adding higher-order and interaction terms for the existing features. It also adds a \"zeroth\"\n",
    "# column consisting of all 1's that corresponds to the weight w_0 in a regression model.\n",
    "poly = PolynomialFeatures(d)\n",
    "\n",
    "# We \"fit\" the poly object to our data matrix to allow it to identify the structure of the data\n",
    "# (notably the number of attributes, or columns, in the data matrix).\n",
    "poly.fit(X_standard)\n",
    "#poly.fit(X)\n",
    "\n",
    "\n",
    "# Now we use poly.transform to add any higher-order terms to the data matrix.\n",
    "# This also adds a zeroth attribute which is set to all 1's.\n",
    "augmented_X = poly.transform(X_standard)\n",
    "#augmented_X = poly.transform(X)\n",
    "\n",
    "\n",
    "# Next we create a linear regression object (named lm for \"linear model\").\n",
    "# Because our augmented data matrix includes an all 1's column, we don't\n",
    "# need to fit the intercept (w_0) here.\n",
    "TP_lm = LinearRegression(fit_intercept=False)\n",
    "TN_lm = LinearRegression(fit_intercept=False)\n",
    "\n",
    "\n",
    "# Split data into train and test for each\n",
    "TPX_train, TPX_test, TPy_train, TPy_test = train_test_split(augmented_X, TP, train_size=0.7)\n",
    "TNX_train, TNX_test, TNy_train, TNy_test = train_test_split(augmented_X, TN, train_size=0.7)\n",
    "\n",
    "# Fit models using training data\n",
    "TP_lm.fit(TPX_train, TPy_train)\n",
    "TN_lm.fit(TNX_train, TNy_train)\n",
    "\n",
    "# After fitting the regression model, we can estimate the error\n",
    "# Get training errors\n",
    "TP_train_err = np.mean((TPy_train - TP_lm.predict(TPX_train)) ** 2)\n",
    "TN_train_err = np.mean((TNy_train - TN_lm.predict(TNX_train)) ** 2)\n",
    "\n",
    "# Get test errors\n",
    "TP_test_err = np.mean((TPy_test - TP_lm.predict(TPX_test)) ** 2)\n",
    "TN_test_err = np.mean((TNy_test - TN_lm.predict(TNX_test)) ** 2)\n",
    "\n",
    "# Report\n",
    "print(\"TP training set mean squared error: {:.6f}\".format(TP_train_err),\" on average off {:.6f}\".format(np.sqrt(TP_train_err)))\n",
    "print(\"TN training set mean squared error: {:.6f}\".format(TN_train_err),\" on average off {:.6f}\".format(np.sqrt(TN_train_err)),\"\\n\")\n",
    "print(\"TP test set mean squared error: {:.6f}\".format(TP_test_err),\" on average off {:.6f}\".format(np.sqrt(TP_test_err)))\n",
    "print(\"TN test set mean squared error: {:.6f}\".format(TN_test_err),\" on average off {:.6f}\".format(np.sqrt(TN_test_err)),\"\\n\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
