{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4ada49ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "import kmapper as km\n",
    "from kmapper.plotlyviz import *\n",
    "import sklearn\n",
    "#from sklearn import ensemble\n",
    "# DBSCAN from sklearn for clustering algorithms\n",
    "from sklearn.cluster import DBSCAN\n",
    "# PCA from sklearn for projection/lens creation\n",
    "from sklearn.decomposition import PCA\n",
    "# from sklearn.manifold import MDS\n",
    "\n",
    "# scipy for interpolation\n",
    "# import scipy \n",
    "# from scipy.interpolate import *\n",
    "# Plotly and Dash\n",
    "import plotly.graph_objs as go\n",
    "from kmapper.plotlyviz import plotlyviz\n",
    "from kmapper.plotlyviz import *\n",
    "from ipywidgets import (HBox, VBox)\n",
    "import dash_html_components as html\n",
    "import dash_core_components as dcc\n",
    "import dash\n",
    "from ipywidgets import interactive, HBox, VBox, widgets, interact\n",
    "import warnings\n",
    "import pandas as pd\n",
    "import json\n",
    "import queue\n",
    "import networkx as nx\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.neighbors import NearestNeighbors\n",
    "from sklearn.preprocessing import StandardScaler, RobustScaler\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c130a8c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_scomplex(df, epsilon = 10, min_samples = 10, n_cubes = [125,125], perc_overlap = [.4,.4]):\n",
    "\n",
    "    X = RobustScaler().fit_transform(df)\n",
    "    cluster_alg = sklearn.cluster.DBSCAN(eps = epsilon, min_samples = min_samples, metric = 'euclidean')\n",
    "    pca = PCA(n_components = 2)\n",
    "    lens = pca.fit_transform(X)\n",
    "\n",
    "    mapper = km.KeplerMapper(verbose = 0)\n",
    "    scomplex = mapper.map(lens, X, cover = km.Cover(n_cubes = n_cubes, perc_overlap = perc_overlap), \n",
    "                                                clusterer = cluster_alg, remove_duplicate_nodes = True)  \n",
    "    return scomplex\n",
    "\n",
    "\n",
    "def visualize_by_state(scomplex, df, filepath):\n",
    "    mapper = km.KeplerMapper(verbose = 0)\n",
    "    continuous_variables =  [\"WDP\", \"SECCHI\", \"TEMP\", \"DO\", \"TURB\", \"VEL\", \"TP\", \"TN\", \"SS\", \"CHLcal\"]\n",
    "    var_to_index = {continuous_variables[i] : i for i in range(len(continuous_variables))}\n",
    "    projected_var_indices = [var_to_index[var] for var in continuous_variables]\n",
    "    summary_variable = mapper.project(np.array(df), projection = projected_var_indices, scaler = None)\n",
    "    \n",
    "    # Make HTML with color values as the states\n",
    "    color_vals = df[scomplex['maxima']]\n",
    "    color_names = scomplex['maxima']\n",
    "    kmgraph,  mapper_summary, colorf_distribution = get_mapper_graph(scomplex, \n",
    "                                                                        color_values = color_vals,\n",
    "                                                                        color_function_name = color_names)\n",
    "\n",
    "    #for node in kmgraph['nodes']:\n",
    "    #    node['custom_tooltips'] = np.array(water_df[variable])[scomplex['nodes'][node['name']]]\n",
    "\n",
    "    plotly_graph_data = plotly_graph(kmgraph, graph_layout = 'fr', factor_size = 2.5, edge_linewidth = 0.5)\n",
    "\n",
    "    layout = plot_layout(title = 'LTRM DATA',  \n",
    "                        width = 620, height = 570,\n",
    "                        annotation_text = get_kmgraph_meta(mapper_summary))\n",
    "\n",
    "#     # FigureWidget is responsible for event listeners\n",
    "#     fw_graph = go.FigureWidget(data = plotly_graph_data, layout = layout)\n",
    "#     fw_hist = node_hist_fig(colorf_distribution)\n",
    "#     fw_summary = summary_fig(mapper_summary, height = 300)\n",
    "#     dashboard = hovering_widgets(kmgraph, \n",
    "#                                 fw_graph, \n",
    "#                                 ctooltips = True,\n",
    "#                                 member_textbox_width = 600)\n",
    "    \n",
    "    #Update the fw_graph colorbar, setting its title:\n",
    "    #fw_graph.data[1].marker.colorbar.title = 'hi'\n",
    "\n",
    "    html_output_path = filepath\n",
    "\n",
    "    mapper.visualize(scomplex, color_values = color_vals, color_function_name = color_names, \n",
    "                    path_html = html_output_path, lens = summary_variable, lens_names = continuous_variables)\n",
    "\n",
    "\n",
    "def k_nearest_neighbors(df, neigh, point, k):\n",
    "    return neigh.kneighbors([list(df.loc[point])], k)[0].flatten()\n",
    "    \n",
    "def calculate_density(scomplex, node, df, k):\n",
    "    neigh = NearestNeighbors(n_neighbors=k)\n",
    "    neigh.fit(df)\n",
    "    knn = 0\n",
    "    n = len(scomplex['nodes'][node])\n",
    "    for point in scomplex['nodes'][node]:\n",
    "        distances = k_nearest_neighbors(df, neigh, point, k)\n",
    "        knn += (sum(distances) / k)\n",
    "    density = knn / (n*n)\n",
    "    return (1.0 / density)\n",
    "\n",
    "def generate_graph(scomplex, shape):\n",
    "    dg = generate_nodes(scomplex, shape)\n",
    "    dg = generate_edges(scomplex, shape, dg)\n",
    "    return dg\n",
    "\n",
    "def generate_nodes(scomplex, shape):\n",
    "    dg = nx.DiGraph()\n",
    "    dg.add_nodes_from(shape)\n",
    "    return dg\n",
    "\n",
    "def generate_edges(scomplex, shape, dg):\n",
    "    for node in shape:\n",
    "        if node in scomplex['links']:\n",
    "            for adjacent_node in scomplex['links'][node]:\n",
    "                if scomplex['density'][node] < scomplex['density'][adjacent_node]:\n",
    "                    dg.add_edge(node, adjacent_node)\n",
    "                else:\n",
    "                    dg.add_edge(adjacent_node, node)\n",
    "    return dg\n",
    "\n",
    "def get_local_maxima(dg):\n",
    "    maxima = []\n",
    "    for node in list(dg.nodes):\n",
    "        succ = dict(nx.bfs_successors(dg, source=node))\n",
    "        if not succ[node]:\n",
    "            maxima.append(node)\n",
    "    return maxima\n",
    "\n",
    "def draw_graph(scomplex, filepath, with_labels=False):\n",
    "    colors = ['#ff0000', '#3366ff', '#0BB30B', '#A312A3', '#669999']\n",
    "    colors_dict = {scomplex['maxima'][i] : colors[i] for i in range(len(scomplex['maxima']))}\n",
    "    transition_color = '#D8D135'\n",
    "    color_map = []\n",
    "    scomplex['states'] = {scomplex['maxima'][i] : [] for i in range(len(scomplex['maxima']))}\n",
    "    \n",
    "    fig = plt.figure(figsize=(8,8))\n",
    "    ax = plt.subplot(111)\n",
    "    #ax.set_title('La Grange Ecological States', fontsize=40)\n",
    "    \n",
    "    for node in scomplex['graph']:\n",
    "        distDict = {scomplex['maxima'][i] : graph_distance(scomplex['graph'], node, scomplex['maxima'][i])\n",
    "                    for i in range(len(scomplex['maxima']))}\n",
    "        minDist = min(distDict.values())\n",
    "        states = [maxima if distDict[maxima] == minDist else None for maxima in scomplex['maxima']]\n",
    "        states = list(filter(None, states))\n",
    "        \n",
    "        for state in states:\n",
    "            scomplex['states'][state].append(node)\n",
    "        \n",
    "        if len(states) > 1:\n",
    "            color_map.append(transition_color)\n",
    "        else:\n",
    "            color_map.append(colors_dict[states[0]])\n",
    "\n",
    "    nx.draw_kamada_kawai(scomplex['graph'], with_labels=with_labels, node_color=color_map, node_size = 1000)\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(filepath, format=\"PNG\")\n",
    "    plt.show()\n",
    "\n",
    "def graph_distance(dg, source, target):\n",
    "    if nx.has_path(dg, source, target):\n",
    "        return len(nx.shortest_path(dg, source, target))\n",
    "    return float('inf')\n",
    "\n",
    "\n",
    "def getSubdf(scomplex, shape, df):\n",
    "    \"\"\"\n",
    "    Returns the part of the data frame from the particular shape in the simplicial complex.\n",
    "    params:\n",
    "    scomplex: the entire simplicial complex\n",
    "    shape: the particular shape being inspected (within the simplicial complex)\n",
    "    df: the entire data frame\n",
    "    \n",
    "    Description:\n",
    "    1. Get all the nodes from the particular simplicial complex. \n",
    "    2. Generate the indices we care about from the particular shape. To do this, we read each node and append it's \n",
    "    indices to a list. Then, we convert the list to a set and then back to a list to eliminate duplicates.\n",
    "    3. Return the dataframe with only those indices.\n",
    "    \"\"\"\n",
    "    nodes = scomplex.get('nodes')\n",
    "    indices = []\n",
    "    npShape = np.array(shape).flatten()\n",
    "    for node in npShape:\n",
    "        indices.append(nodes.get(node))\n",
    "    indices = list(set([item for sublist in indices for item in sublist]))\n",
    "    subdf = df.loc[indices]\n",
    "    return subdf\n",
    "\n",
    "\n",
    "def adjacent(v, scomplex):\n",
    "    \"\"\"\n",
    "    Determines the nodes adjacent to a given vertex\n",
    "    \n",
    "    params:\n",
    "    v: vertex\n",
    "    scomlex: the entire simplicial complex\n",
    "    \n",
    "    Description:\n",
    "    Determines the nodes that are adjacent to a given vertex.\n",
    "    \"\"\"\n",
    "    \n",
    "    simplices = scomplex.get('simplices')\n",
    "    edges = [item for item in simplices if len(item) == 2]\n",
    "    result = []\n",
    "    for edge in edges:\n",
    "        if v in edge:\n",
    "            for item in edge:\n",
    "                if item != v:\n",
    "                    result.append(item)\n",
    "    return result\n",
    "\n",
    "\n",
    "def bfs(node, scomplex):\n",
    "    \"\"\"\n",
    "    Conducts a breadth first search to obtain the entire shape from a given node\n",
    "    params:\n",
    "    node: the start node\n",
    "    scomplex: the entire simplicial complex\n",
    "    \n",
    "    Description:\n",
    "    Preforms a breadth first search to obtain the entire shape for a given start node.\n",
    "    \"\"\"\n",
    "    Q = queue.Queue()\n",
    "    result = []\n",
    "    result.append(node)\n",
    "    Q.put(node)\n",
    "    while not Q.empty():\n",
    "        v = Q.get()\n",
    "        adjacentEdges = adjacent(v, scomplex)\n",
    "        for edge in adjacentEdges:\n",
    "            if edge not in result:\n",
    "                result.append(edge)\n",
    "                Q.put(edge)\n",
    "    return result\n",
    "\n",
    "\n",
    "def condenseShape(shape, scomplex):\n",
    "    \"\"\"\n",
    "    \n",
    "    params:\n",
    "    shape: a shape of two nodes. must be 2\n",
    "    scomplex: the entire simplicial complex\n",
    "    \n",
    "    description:\n",
    "    gets the two nodes a and b\n",
    "    gets the indices for a and b (what is inside the nodes)\n",
    "    if a \\subseteq b, return b\n",
    "    elif b \\subseteq a, return a \n",
    "    else return shape \n",
    "    \n",
    "    \"\"\"\n",
    "    nodes = scomplex.get('nodes')\n",
    "    a = shape[0]\n",
    "    b = shape[1]\n",
    "    aIndices = set(nodes.get(a))\n",
    "    bIndices = set(nodes.get(b))\n",
    "    \n",
    "    if aIndices.issubset(bIndices):\n",
    "        return b\n",
    "    elif bIndices.issubset(aIndices):\n",
    "        return a\n",
    "    else:\n",
    "        return shape\n",
    "\n",
    "\n",
    "def getShapes(scomplex):\n",
    "    \"\"\"\n",
    "    Gets all of the shapes from a given simplicial complex.\n",
    "    \n",
    "    params:\n",
    "    scomplex: the entire simplicial complex\n",
    "    \n",
    "    Description:\n",
    "    1. Obtain all the nodes for the entire complex\n",
    "    2. For each node, preform a breadth first search to obtain everything in that particular shape. \n",
    "    If this entire shape has not already been discovered, add it to the set of results. \n",
    "    The result item is a set as the order of the shapes does not matter. The resulting shape is a frozenset\n",
    "    which means items cannot be added or removed once created, and is needed to allow the set object to have other sets within it.\n",
    "    3. Convert each shape to a list and the result to a list for easier navigation outside of the function.\n",
    "    4. Return the result\n",
    "    \n",
    "    \"\"\"\n",
    "    \n",
    "    nodes = list(scomplex.get('nodes').keys())\n",
    "    result = set()\n",
    "    for node in nodes: # currently does more computations than necessary due to going through every node without considering it is already in a shape\n",
    "        bfsResult = frozenset(bfs(node, scomplex))\n",
    "        result.add(bfsResult)\n",
    "    result = [list(x) for x in result]\n",
    "    # Sort the list depending on what is decided: nodes or indices. Currently doing it by number of nodes\n",
    "    result.sort(key = len, reverse = True)\n",
    "    \n",
    "    return result\n",
    "\n",
    "def clean_getShapes(scomplex):\n",
    "    \"\"\"\n",
    "    Condenses 1-simplices down to 0-simplices when each node \n",
    "    is a subset of the other \n",
    "    \n",
    "    params:\n",
    "    scomplex: the entire simplicial complex\n",
    "    \n",
    "    Description:\n",
    "    1. Get all the shapes from the original getShapes function\n",
    "    2. For shapes that of length 2, if one is a subset of the other, return the larger of the two\n",
    "        Otherwise, do nothing\n",
    "    3. return the clean Shapes list \n",
    "    \n",
    "    \"\"\"\n",
    "    shapes = getShapes(scomplex)\n",
    "    cleanShapes = []\n",
    "    for shape in shapes:\n",
    "        if len(shape) == 2:\n",
    "            shape = condenseShape(shape, scomplex)\n",
    "            cleanShapes.append([shape])\n",
    "        else:\n",
    "            cleanShapes.append(shape)\n",
    "    return cleanShapes\n",
    "\n",
    "\n",
    "def append_states(scomplex, shape, df):\n",
    "    for maximum in scomplex['maxima']:\n",
    "        entries = [0 for x in range(df.shape[0])]\n",
    "        for node in scomplex['states'][maximum]:\n",
    "            for entry in scomplex['nodes'][node]:\n",
    "                entries[entry] = 1\n",
    "        df[maximum] = entries\n",
    "        #water_df = pd.concat([water_df, temp], axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "68c4be4e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-3-c81fa4fdc0f6>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[0mk\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m/\u001b[0m \u001b[1;36m10\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 7\u001b[1;33m \u001b[0mscomplex\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mgenerate_scomplex\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdf\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      8\u001b[0m \u001b[0mscomplex\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'density'\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m{\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      9\u001b[0m \u001b[0mlargestShape\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mclean_getShapes\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mscomplex\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-2-21d133100e38>\u001b[0m in \u001b[0;36mgenerate_scomplex\u001b[1;34m(df, epsilon, min_samples, n_cubes, perc_overlap)\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      8\u001b[0m     \u001b[0mmapper\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mkm\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mKeplerMapper\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mverbose\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 9\u001b[1;33m     scomplex = mapper.map(lens, X, cover = km.Cover(n_cubes = n_cubes, perc_overlap = perc_overlap), \n\u001b[0m\u001b[0;32m     10\u001b[0m                                                 clusterer = cluster_alg, remove_duplicate_nodes = True)  \n\u001b[0;32m     11\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mscomplex\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\kmapper\\kmapper.py\u001b[0m in \u001b[0;36mmap\u001b[1;34m(self, lens, X, clusterer, cover, nerve, precomputed, remove_duplicate_nodes)\u001b[0m\n\u001b[0;32m    539\u001b[0m                     \u001b[0mfit_data\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mfit_data\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mids\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    540\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 541\u001b[1;33m                 \u001b[0mcluster_predictions\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mclusterer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit_predict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfit_data\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    542\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    543\u001b[0m                 \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mverbose\u001b[0m \u001b[1;33m>\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\sklearn\\cluster\\_dbscan.py\u001b[0m in \u001b[0;36mfit_predict\u001b[1;34m(self, X, y, sample_weight)\u001b[0m\n\u001b[0;32m    387\u001b[0m             \u001b[0mCluster\u001b[0m \u001b[0mlabels\u001b[0m\u001b[1;33m.\u001b[0m \u001b[0mNoisy\u001b[0m \u001b[0msamples\u001b[0m \u001b[0mare\u001b[0m \u001b[0mgiven\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mlabel\u001b[0m \u001b[1;33m-\u001b[0m\u001b[1;36m1.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    388\u001b[0m         \"\"\"\n\u001b[1;32m--> 389\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msample_weight\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0msample_weight\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    390\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlabels_\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\sklearn\\cluster\\_dbscan.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, X, y, sample_weight)\u001b[0m\n\u001b[0;32m    350\u001b[0m         \u001b[0mdbscan_inner\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcore_samples\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mneighborhoods\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    351\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 352\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcore_sample_indices_\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwhere\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcore_samples\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    353\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlabels_\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlabels\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    354\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "df = pd.read_csv(r\"C:\\Users\\killiad\\Documents\\Senior\\REU\\Data\\water_full.csv\")\n",
    "continuous_variables = [\"WDP\", \"SECCHI\", \"TEMP\", \"DO\", \"TURB\", \"VEL\", \"TP\", \"TN\", \"SS\", \"CHLcal\"]\n",
    "df = df[continuous_variables]\n",
    "df.reset_index(drop = True, inplace = True)\n",
    "k = int(df.shape[0] / 10) + 1\n",
    "\n",
    "scomplex = generate_scomplex(df)\n",
    "scomplex['density'] = {}\n",
    "largestShape = clean_getShapes(scomplex)[0]\n",
    "print(\"Number of Nodes in Largest Shape: \", len(largestShape))\n",
    "i = 1\n",
    "for node in largestShape:\n",
    "    print(i)\n",
    "    i += 1\n",
    "    scomplex['density'][node] = calculate_density(scomplex, node, df, k)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b928283",
   "metadata": {},
   "outputs": [],
   "source": [
    "scomplex['graph'] = generate_graph(scomplex, largestShape)\n",
    "scomplex['maxima'] = get_local_maxima(scomplex['graph'])\n",
    "print(\"Number of Maxima: \", len(scomplex['maxima']))\n",
    "filepath =  r\"C:\\Users\\killiad\\Documents\\Senior\\REU\\Mapper\\Whole_River\\States.png\"\n",
    "draw_graph(scomplex, filepath, with_labels=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6a9e86e",
   "metadata": {},
   "outputs": [],
   "source": [
    "filepath = r\"C:\\Users\\killiad\\Documents\\Senior\\REU\\Mapper\\Whole_River\\States.html\"\n",
    "append_states(scomplex, largestShape, df)\n",
    "visualize_by_state(scomplex, df, filepath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aab0e979",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
