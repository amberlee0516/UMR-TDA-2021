---
title: "Filtering water quality dataset"
author: "Amber Lee"
date: "6/15/2021"
output: pdf_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE, message = FALSE, warning = FALSE)
```

## Unfinished

Line 177 in the code is where we should filter for the appropriate QF values. Note how `waterQF_var` is defined (line 71). Use the tutorial (very last link) to use `filter_at()`. Based on how I did the filtering, which was to remove *ALL ROWS* for which the QF code is 8, I only get 53,000 rows.

## Summary

One important goal before we interpolate missing values is to filter the data, to decide on the scope of our analysis. Filtering the data to what is necessary will also reduce the amount of interpolation needed. We will see that removing duplicate rows will greatly reduce the missing values (that aren't actually missing), thus making the interpolation step significantly easier. 

In cleaning the LTRM Water Quality dataset, we encounter the following questions:

* Which variables (columns) are the most important for us to keep (out of the 133 total variables)?

* Why do duplicate rows happen, and how do we deal with them?

* Which samples (rows) are high enough quality for us to keep? (This involves the QF codes.)


The LTRM Water Quality dataset has duplicate rows for the same `SHEETBAR`, which is problematic because `SHEETBAR` is a unique identifier for a water data sheet (sample at a date, time, and location). 


```{r}
library(tidyverse)
library(ggplot2)
library(lubridate)
library(corrplot)
library(RColorBrewer)
```

```{r}
water20 <- read.csv(file = "C:\\Users\\forre\\Desktop\\REU\\TDA\\Data\\ltrm_water_data_lat_long_06072021.csv")
```

## Important variables

```{r}

water_var <- c('TN','TP','TEMP','DO','TURB',
               'COND','VEL','SS','WDP','CHLcal','SECCHI')

waterQF_var <- paste(water_var, "QF", sep = "")

identifier_var <- c('SHEETBAR', 'DATE', 'LATITUDE', 'LONGITUDE', 'FLDNUM')

```

We decided that the 11 continuous variables of importance were: total nitrogen, total phosphorous, temperature, dissolved oxygen, turbidity, water condition, velocity, suspended solids, water depth, chlorophyll-a, and Secchi distance. 

In addition, we will want to include the QA/QC codes, along with identifier variables like `SHEETBAR` and date. 

Lastly, we manually edit these variable strings because:

* The water depth variable is `WDP`, but the corresponding quality factor is `ZMAXQF` rather than `WDPQF`.

* `CHLcal`, calibrated fluorometric chlorophyll a, does not have a corresponding quality factor code. According to the metadata: "`CHLcal` is generated by calibration of fluorometric chlorophyll readings (`CHLF`) 
to season and year specific measurements of spectrophotometric chlorophyll  (`CHLS`). Data from sites where CHLS and CHLF are both collected 
are used to build river-specific calibration curves for these data. Values are corrected for pheophytin. Units are micrograms per liter.``

```{r}

waterQF_var <- waterQF_var[waterQF_var != "WDPQF" & 
                             waterQF_var != "CHLcalQF"]

waterQF_var <- c(waterQF_var, "ZMAXQF")

```


## Duplicate rows

There are `r dim(water20)[1]` total rows in the LTRM water quality dataset. Of these rows, there are `r dim(water20 %>% distinct(SHEETBAR))[1]` distinct `SHEETBAR` codes. 

We visualize duplicates as follows. To identify the duplicate rows, we count the number of occurences of each unique `SHEETBAR` value in the dataset. Then, we can calculate and plot the distribution of `SHEETBAR` duplicates. 

```{r}

duplicates <- water20 %>% 
  select(SHEETBAR) %>%
  group_by(SHEETBAR) %>%
  summarize(count = n()) 

duplicates %>% head()

count_n_duplicates <- function(n, df) {
  return((df %>% filter(count == n) %>% dim())[1]/156474) #156k distinct sheetbars
}

count_duplicates <- data.frame(proportion = sapply(1:8, count_n_duplicates,
                                                   duplicates),
                               number_duplicates = 1:8)

ggplot(count_duplicates, aes(x = number_duplicates, y = proportion)) +
  geom_bar(stat = "identity") +
  geom_vline(xintercept = 7) 

count_duplicates # use kable to make nice table output

```

The proportion of `SHEETBAR`s with at least one duplicated row is`r 1 - count_duplicates$proportion[1]` (representing about 47,000 rows). When do duplicated rows occur?

We look at two `SHEETBAR`s with duplicated rows. 

```{r}

water20 %>% 
  filter(SHEETBAR == -4604347) %>% 
  select(SHEETBAR, Z, CALCZCD, DO, TP, TN)

```

```{r}

water20 %>% 
  filter(SHEETBAR == 41015929	) %>% 
  select(SHEETBAR, Z, CALCZCD, DO, TP, TN)

```

Here, we see that `TP` and `TN`, total phosphorous and total nitrogen, are measured only at the surface level (when `CALCZCD == "SF"`). The variable `CALCZCD` is a categorical variable with levels surface, middle, bottom, and other. It is calculated with the sample depth and the total water depth (of the river site). 

In contrast, dissolved oxygen `DO` is measured at various depths (denoted by `Z`) because different parts of the water column have different levels of `DO`. It would be inappropriate to average the dissolved oxygen levels because they were taken at different sample depths. 

Thus, this missing values of `TP` and `TN` are occuring at different sample depths at the same sampling site. These missing values aren't *really* missing values; they would be redundant to interpolate.

## We can reasonably keep only the samples taken at the surface level

We decided to filter for rows that were labelled as surface level,`CALCZCD == "SF"`. Implicitly, this filtering step removes samples for which the sample depth is missing.

```{r}
table(water20$CALCZCD)
```

More than 70% of the samples were taken on the surface level. Of these measurements taken at the surface level, the eleven important continuous variables (in `water_var`) were recorded with a recording rate of at least 50%. This is a good sanity check because `TN` and `TP` are never recorded in the middle and bottom water depths. we checked to see the recording rate of the eleven important variables and found recording rates greater than 50%. 

```{r}

filterwater <- water20 %>%
  filter(CALCZCD == "SF") %>%
  select(all_of(water_var))

sapply(filterwater, function(x) sum(is.na(x)/147171))

```
### What if CALCZCD is missing?

```{r}
(water20 %>%
  filter(CALCZCD == "") %>%
  dim())[1]
```

There are are about 8000 samples with missing `CALCZCD`. We are 

## Water column variables

WDP, 

## Filter for QA/QC

## Combine

```{r}

filterwater <- water20 %>%
  filter(CALCZCD == "SF") %>%
  select(all_of(c(identifier_var, water_var, waterQF_var))) %>%
  filter_at(vars(c("TURBQF","TEMPQF","DOQF","VELQF","ZMAXQF","SECCHIQF","CONDQF")), any_vars(. != "A" | . != 0)) %>%
  # forrest, edit this
  filter_at(vars(c("TNQF","TPQF","SSQF")), all_vars(. != 8 | . != 64))
filterwater %>% head()
```

```{r}
dim(filterwater)
```

use this tutorial about `filter_at`
https://suzan.rbind.io/2018/02/dplyr-tutorial-3/

```{r}
write.csv(filterwater, "C:\\Users\\forre\\Desktop\\REU\\TDA\\Data\\water_data_filtered.csv", row.names = FALSE)
```
